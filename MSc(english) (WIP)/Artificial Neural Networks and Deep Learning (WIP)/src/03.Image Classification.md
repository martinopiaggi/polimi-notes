# Image Classification

Computer vision  is an interdisciplinary scientific field that can be summarize to everything that makes computer able to understand the  high level understanding of digits from images or videos.

In recent years, there has been a significant shift towards using deep learning for understanding the content of digital images and videos. **Deep learning** has become the go-to approach for solving visual recognition problems, such as image classification.

We will focus on the first visual recognition problem ever introduced, which is **image classification**: by addressing this problem, we aim to gain a deeper understanding of its complexity without the use of deep learning techniques.


## Traditional classifiers

### Local (Spatial) Filter

Local (Spatial) transformations mix all the pixels locally "around" a given pixel.
These transformations can be written as:

$$
G(r, c)=T_{U}[I](r, c)
$$

Where:

- $I$ is the input image to be transformed
- $U$ identifies the region, "a neighborhood
- $T_u$ is a function which returns an image that for each output pixel $(r, c)$ is defined by all neighbors input pixels $U$ to input pixel $(r,c)$

The simplest possible transformation $T$ is a **linear** combination:

$$
T[I](r, c)=\sum_{(u, v) \in U} w(u, v) * I(r+u, c+v)
$$

Weights can be thought of as a filter $h$ which completely determines the operation.

### Linear Classifier

To feed images to a neural network (NN), we use column-wise unfolding to convert the image into a vector shape. The length of this vector is determined by the image dimensions, where $R$ represents the number of rows, $C$ represents the number of columns, and we multiply this by 3 to account for the three color channels (RGB).

In this vector representation, each element corresponds to the intensity of a specific color in a pixel of the image. This means that each color is represented by $R \times C$ pixels, with values ranging from 0 to 255.

![](images/cf8bb9b9b4c65696c8e73890a775803a.png) 


For classification tasks, the output of the NN is typically a vector with a length equal to the number of classes, where each element in this vector represents the score associated with a particular class in the input image.
The high dimensionality of the problem poses a challenge when using traditional NN: even with smaller images, the number of parameters can still be in the billions. 


### The problem with traditional classifiers 

Image Classification is a challenging problem and actually "traditional" classifiers are not helping at all.
The main challenges of a image classification problem are:

- **Dimensionality**: Images are very high-dimensional image data
- **Label ambiguity**: Images with a huge amount of content can be ambiguous (more than one label can correctly be assigned)
- **Transformations**: images of a the same subject can be very different because deformations, lightning changes, occlusion, background clutter, scale differences, a lot of variability in the same class 
- **Perceptual similarity**: the most important problem is that perceptual similarity in images is not related to pixel-similarity. Differences between pixels is not the same difference between "concepts". This is way simple classifier like Nearest Neighborhood Classifiers and linear classifier are not fit to classify images. What really happens is the perceptual similarity which is not related to Euclidean distance between pixels (which is what actually traditional classifiers do).  


## Convolutional Neural Networks 

Convolutional neural networks (CNNs) are specifically optimized for processing image data. They use convolution operations to **extract features** from the input. 
The CNN is a multi-layered architecture that turns the input image into a vector shape which is inputted into a traditional NN for classification purposes.

![](images/a3fb0fbd2c1672a28b2f149aa0367218.png)

The output size of a convolutional layer is called a **feature map**. The size of the feature map depends on three hyperparameters:

- **Depth**: corresponds to the **number of filters** in the layer (filters represented as the weights over the connection to another neuron), each of which looking for different characteristics in the input. The set of neurons belonging to different filters and connected to the same input region is the same depth column.
- **Stride**: number of pixel of which perform translations over the input image. This parameter allows to set the domain of the depth columns, because each depth column is linked to only one fraction of the input.
- **Zero-Padding**: often used to match input and output shapes.


![](images/c2ef3a1911c73be91e5401bed0ad2e6c.png)


![](images/7e5f25dac91ca0dd689a6115e4e15fcb.png)

[Animated AI](https://animatedai.github.io/)

The convolutional layers "merge" the filters with are extracting features. In particular similarities between the filters. 

Activation layers introduce non-linearities in the network to differentiate CNN from linear classifiers. They are scalar functions that operate on each value of the volume, and do not change its size.

![](images/840a1b68e7afcf2c5ea0aee34b19f405.png)

![](images/43605eb97d842d275e59dc0bb604e48d.png)

The ReLu performs a tresholding on the feature maps but may suffer from vanishing gradient and dying neurons, so the Leaky variant is preferred. It acts separately on each layer. Other variants can be the Tanh and Sigmoid, but are preferred in MLP architectures.

Pooling layers 

These layers reduce the spatial size of the volume, leaving the depth untouched. They operate on each layer of the input volume and resizes it. most popular one is the MAX one which using the max operation:

![](images/a47d211607c990cbf56ccec95e5a371e.png)




![](images/22dd68f76ad5994991ddfb2af54298ae.png)


And at the end we have **dense layers** 

The spatial dimension is lost and the CNN stacks the hidden layers. It's called dense because each output neuron is connected to each input neuron. This contains the CNN network found features through which we perform the final classification. In fact, the last layer (FC layer) has the same size as the number of classes, and provides a score for the input image to belong to each class. They're useful because they can summarize the feature extraction results using single numeric values that will be used by successive layers. 

![](images/61fa598eee3791ddeae921f944287bb6.png)

![](images/52bf3d43e213353f07ece2c7f7e895e7.png)

At the end there is a multi layer perceptron which operates on feature. 
Latent representation


The CNN structure is particularly fitting for image processing because of 3 main characteristics:

1. **Sparse interaction**: not every neuron is connected to the others. Each one is only connected to a region of the input. Each output value in a layer only depends on a specific region in the input which is called the receptive field for that output. As we go deeper into the network, the receptive field becomes wider. This increased width is achieved through operations like maxpooling, convolutions, and stride greater than 1. 
2. **Weight sharing**: all the neurons in a given slice of a feature map share the same weights and bias. This sharing of parameters reduces the total number of parameters in the network. The assumption is made that if a particular feature is useful to compute at a certain spatial position (x, y), it should also be useful to compute at a different position (x2, y2). However, note that this assumption may not always hold true, especially when trying to identify localized features.
3. **Translation invariance**: by design (multiple filters), a CNN is **insensitive** to small transformations of the input


## Extras 

### Data augmentation

Deep learning models are data-hungry. Data augmentation involves applying various transformations to the dataset to artificially increase its size. Geometric transformations (such as shift, rotation, affine/perspective distortion, shear ...) and photometric transformations (such as adding noise, modifying average intensity, image contrast ...) are used. 
It is important to choose transformations that do not impact key features and preserve the input label.
In general, transformations can be also class-specific: to address class imbalance by creating more examples for the minority class for example. 
To further improve prediction accuracy, test time augmentation (TTA) can be performed:

1. Randomly augment each test image to create augmented images $A_l(I)_l$.
2. Make predictions for each augmented image using the CNN: $p_i = CNN(A_l(I))$.
3. Take the average predictions of each augmented image: $p = Avg(\{p_l\}_l)$.

### Transfer learning

A CNN consists of a feature extraction section, which is powerful and general purpose, and a classifier, which is task-specific.

We can use a pre-trained model's convolutional layers as a **feature extractor**, as they are effective and general. The FC layers, on the other hand, are specific and tailored to solve a particular problem.

In transfer learning, only the FC layers are trained. This is a good option when there is limited training data and the pre-trained model aligns with the problem. 

**Fine tuning** involves retraining the entire CNN while initializing the convolutional layers with the pre-trained model. This is useful when there is enough training data or when the task differs. Lower learning rates are used in fine tuning.