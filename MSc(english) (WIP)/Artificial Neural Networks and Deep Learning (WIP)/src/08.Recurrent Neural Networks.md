
# Recurrent Neural Networks

We've only discussed static datasets, unchanging over time till now: 
- **Static** datasets are unchanging over time.
- **Dynamic** datasets change over time and require special handling.
    
To handle **Dynamic Datasets** two main approaches exists: Memoryless models and Models with memory.

- **Memoryless Models**
    - These models take only the current input into account, without memory or retention of past inputs or outputs. Example of this: 
	    - **Autoregressive Models**:
		    - **Function**: Predict the next value in a sequence using past values.
		    - **Mechanism**: Utilizes "delay traps" to capture information from previous time steps.
		    - **Lag Order**: Refers to the number of previous time steps used for prediction. For instance, a lag order of 3 uses the past three values.
		    - **Training**: Parameters are estimated using methods like least squares on training data.
		    - **Application**: Useful for making predictions in time-series data.
	    - Feedforward Neural Networks (FFNNs): 
		    - **Dynamic Data Handling**: Can model dynamic data by incorporating nonlinear hidden layers.
		    - **Nonlinear Layers**: These layers learn complex relationships, allowing the network to predict future values more accurately.
		    - **Advantages**: FFNNs with nonlinear hidden layers can capture more intricate patterns in data than linear models.
- **Models with Memory**
    - These models retain past input information, which subsequently influences the current output. Models with memory:
	    - **Linear dynamic systems**:
		    - **Characteristics**: Governed by linear differential equations linking current and past states.
		    - **State Estimation**: Involves continuous states with Gaussian uncertainty, linear transformations, and can utilize Kalman filtering for state estimation.
	    - **Hidden Markov Models (HMMs)**:
		    - **Basics**: Probabilistic models with hidden states and observable outputs.
		    - **State Dynamics**: States are discrete with stochastic transitions and outputs dependent on these hidden states.
		    - **State Estimation**: Viterbi algorithm is commonly used for estimating states in HMMs.



## RNNs

- RNNs are a type of neural network designed for sequential data processing.
- They can deal with time series, natural language, and audio due to their "recurrent" component, which allows for operations consistency across the input sequence.




- Feedback is vital for memory simulation and retention.
- RNNs can potentially compute any process a computer can if given enough neurons and time.
- Their function resembles Turing machines - completely computational models with memory.


A "context" network which acts as a memory. In principle this recurrent part of the network acts as an infinite memory. This is achieved through hidden states that effectively 'remember' past inputs. 


How to implement this in a neural network? 

**"Context" Network and Memory:**

- Function as a memory, referred to as a "context" network.
- The hidden state serves as an 'infinite' memory which recalls past inputs.


**Structure of RNNs consist of input, hidden, and output layers:**
- The input layer receives the sequence.
- Hidden layer stores recurrent units or neurons that recall past inputs, essential for sequential data processing.
- Output layer produces the final result.


## BPTT

Backpropagation Through Time (BPTT) is an adaptation of the normal backpropagation algorithm, is designed for sequential data. In usual neural networks, backpropagation calculates the gradient of loss function for each weight, determining how a change in weight influences the total error, and adjusts the weights to minimize this error.

- **RNNs (Recurrent Neural Networks)**: RNNs possess a unique structure that allows for the sharing of weights uniformally across sequential time steps, enabling the processing of data series. Not just the current input but also previous ones influence the output. The temporal dependencies hence introduced complicate the training process.
- **BPTT (Backpropagation Through Time)**: BPTT is designed to handle the training complexity by "unrolling" the RNN over the sequence's time. Each time step is treated as a distinct layer in a deep network, transforming the network into a standard deep neural framework with each layer representing a time step in the initial RNN.





Here's a step-by-step overview of BPTT:

1. **Forward Pass**: Just like in standard backpropagation, BPTT begins with a forward pass through the network. For an RNN, this means processing the entire sequence from the first time step to the last, updating the hidden states based on the recurrent connections at each step.
2. **Compute Loss**: At the end of the sequence, a loss is computed. This loss measures the discrepancy between the predicted output of the RNN and the actual target values.
3. **Backward Pass**: Next, the gradients of this loss are propagated backward through the network. However, unlike standard backpropagation, this involves going backward through each time step, accumulating gradients as we move back in time. Since the same weights are used at each time step, the gradients from all time steps are summed up.
4. **Update Weights**: Finally, the weights are updated based on these accumulated gradients. Typically, optimization algorithms like Stochastic Gradient Descent (SGD) are used for this purpose.
    



In Recurrent Neural Networks, the processing of each new input involves a sophisticated interplay between the current data and information carried over from past inputs. This process is fundamentally driven by the network's hidden state, which serves as a dynamic form of memory, continuously updated throughout the sequence. As each new input is received, it doesn't stand alone; instead, it's integrated with the accumulated knowledge from previous inputs. This integration occurs through the recurrent connections and their associated weights, which are pivotal in determining the influence of past information on the current state. During sequential processing, the RNN's calculations at any given step incorporate both the fresh input and the data stored in the hidden state, ensuring that the output at each step is shaped by the entire sequence of prior inputs. This mechanism, often referred to as the 'recurrent context', is not about repeatedly sending old inputs but rather about retaining their effects in the hidden state and bringing this contextual background into current computations.




RNNs process sequences by considering both current and past inputs, thanks to their recurrent structure. This allows them to capture dependencies that span across time steps.



The hidden states (`h1_A`, `h1_B`, etc.) keep evolving with each input in the sequence. They get 'updated' to reflect the information they've accumulated from all past inputs up to that point.

In summary, the essence of RNNs lies in this mechanism of combining current inputs with information from past inputs (via hidden states) and updating these hidden states continuously. This process enables RNNs to 'remember' and leverage the sequential context of the data they are processing, which is fundamental to their ability to handle tasks involving sequential or time-dependent data effectively.



"When you have a loop, you can always unroll the iterations of them" 

You can always "cut/truncate" the roll. 


![](images/e28a04dca244d4509fcb23a9f26f0964.png)

Actually you don't unroll, you just "sum/average" the sequence of all the hidden states (past). 

BPTT allows RNNs to learn from long sequences and capture long-term dependencies, but it also has challenges. One significant issue is the vanishing and exploding gradient problem, where gradients become too small or too large as they are propagated back through many time steps. This issue can make training RNNs with BPTT challenging, especially for very long sequences. Techniques like gradient clipping, gated recurrent units (GRUs), and long short-term memory (LSTM) units are often used to mitigate these problems.


# Time series 

stuff from jupyter 

he session you described provides a comprehensive overview of using Recurrent Neural Networks (RNNs), particularly LSTM networks, for time series classification and forecasting. To integrate and explain the most important concepts from this session, we need to focus on the fundamental aspects of LSTM networks, their significance in handling time series data, and the key preprocessing steps necessary for their effective application.

### Understanding LSTM Networks

1. **Long Short-Term Memory (LSTM) Networks**: A crucial concept in RNNs. LSTMs are designed to address the limitations of traditional RNNs, particularly the problem of vanishing gradients, which makes it challenging for RNNs to learn and retain information over long sequences. LSTMs achieve this through a specialized architecture that includes memory cells and gates (input, output, and forget gatse), allowing them to maintain a balance between long-term memory and short-term inputs. This feature is vital for tasks like time series analysis where dependencies span over long sequences of data.
    
2. **Application in Time Series Data**: LSTMs are particularly well-suited for time series data due to their ability to remember previous inputs over long periods. This capability is crucial for predicting future values in a series based on historical data. In your session, the focus on activity datasets with accelerometer data demonstrates LSTM's effectiveness in capturing temporal dynamics within three-dimensional data (X, Y, Z axes).
    

### Key Preprocessing Steps

1. **Data Sorting and Normalization**: For time series data, ensuring the data is correctly sorted in time is critical. Normalizing the data, which involves scaling the input features to a standard range, is essential for efficient training of neural networks, including LSTMs. These steps help in reducing model complexity and improving the learning process.
    
2. **Handling Imbalanced Data**: In cases where there is an imbalance in user data points, techniques such as resampling or weighted loss functions can be employed to ensure the model does not become biased towards the majority class.
    
3. **Data Transformation and Label Encoding**: Converting activity labels from strings to integers and transforming data into a suitable format for LSTM input are crucial steps. This transformation enables the model to interpret the data correctly and learn the underlying patterns effectively.
    

### Training and Model Evaluation

1. **Neural Network Configuration**: Selecting appropriate input sizes, strides, and creating training/testing splits are pivotal in defining how the LSTM will process the data. The configuration directly impacts the model's ability to learn from the training data and generalize to unseen data.
    
2. **Advanced LSTM Features**: The session's focus on bidirectional LSTMs, which process data in both forward and backward directions, adds a layer of complexity and effectiveness in understanding time series data.
    
3. **Overfitting and Performance Metrics**: Addressing overfitting through techniques like dropout or early stopping is essential in LSTM models. Monitoring training/validation loss and accuracy helps in understanding the model's learning curve and making necessary adjustments.
    
4. **Forecasting Challenges**: The session's conclusion on forecasting emphasizes the challenges in predicting future events based on past data. This requires careful data preparation, model tuning, and understanding the limitations of predictive modeling.
    

In summary, LSTMs represent a significant advancement in dealing with sequence data, particularly in time series analysis. Their ability to capture long-term dependencies makes them an ideal choice for complex tasks like activity recognition and forecasting. The preprocessing steps, network configuration, and evaluation techniques discussed in your session are crucial in leveraging the full potential of LSTM networks for real-world applications.


#### Vanilla Long Short Term Memory (LSTM) Neural Network



Long Short-Term Memory (LSTM) networks are a special kind of Recurrent Neural Network (RNN), specifically designed to overcome the limitations of traditional RNNs. The core issue LSTMs address is the problem of long-term dependencies, often referred to as the vanishing gradient problem, which hinders RNNs' ability to remember and process inputs from longer sequences. Let's break down the key components and functionalities of LSTMs to understand them better.



![](images/Pasted%20image%2020231206154406.png)





### Basic Structure of LSTM

1. **Memory Cells**: The fundamental building block of an LSTM network is the memory cell. These cells are designed to store information for long periods. Each cell has the capability to add or remove information, controlled through structures called gates.
2. **Gates in LSTM**: There are three types of gates in an LSTM cell:
    - **Input Gate**: Determines how much of the new information should be added to the cell state.
    - **Forget Gate**: Decides what information should be discarded from the cell state.
    - **Output Gate**: Determines the next hidden state, which is a filtered version of the cell state and will be used for predictions and transferring information to the next time step.

### How LSTM Works

1. **Processing Input Sequence**: In tasks like time series prediction, language modeling, or activity recognition, input data comes in sequences. LSTMs process these sequences one element at a time, maintaining a memory (cell state) of previous inputs.
    
2. **Updating Cell State**:
    
    - The forget gate first decides what information to discard from the previous cell state.
    - Simultaneously, the input gate and a creation of a new candidate values layer decide what new information to add to the cell state.
    - The cell state is then updated by combining the old state (after discarding information deemed unnecessary) and adding the new candidate values.
3. **Generating Output**:
    
    - The output gate decides what part of the current cell state will make it to the output.
    - This output will be based on the filtered version of the cell state and will be used for predictions or as an input to the next LSTM cell in the sequence.

### Advantages of LSTM

- **Handling Long-Term Dependencies**: Traditional RNNs struggle to carry information across many time steps during training, leading to the vanishing gradient problem. LSTMs can maintain information in their memory cells for long durations, making them effective for applications where the context from much earlier inputs is essential for understanding the current input.
    
- **Flexibility in Sequence Length**: Unlike feedforward neural networks, LSTMs can handle inputs of variable lengths, which is crucial for many real-world applications like speech and language processing.
    
- **Improved Learning Capability**: Due to their architecture, LSTMs can learn patterns and dependencies in sequence data more effectively than standard RNNs, making them suitable for complex tasks like language translation, sentiment analysis, and more.
    

### Applications of LSTM

LSTMs are widely used in various applications, such as:

- **Natural Language Processing (NLP)**: For tasks like machine translation, text generation, and sentiment analysis.
- **Time Series Analysis**: In financial forecasting, weather prediction, and any domain where patterns over time are crucial.
- **Speech Recognition**: LSTMs can be used to process and transcribe spoken language in real-time.

In conclusion, LSTM networks are a powerful tool for modeling sequential data, particularly when dealing with long sequences and the need to capture long-term dependencies. Their unique architecture allows them to retain information over long periods, making them a go-to choice for many applications in time series analysis and natural language processing.





#### Bidirectional Long Short Term Memory (BiLSTM) Neural Network


![](images/Pasted%20image%2020231206154527.png)


BiLSTM, or Bidirectional Long Short-Term Memory, is an extension of the traditional LSTM model that enhances its ability to understand context and sequence information in data. Let's delve into what BiLSTM is and how it differs from a standard LSTM.

### What is BiLSTM?

1. **Bidirectional Processing**: Unlike standard LSTMs which process data in a forward direction (from past to future), BiLSTMs process data in both forward and backward directions. This means that for any given point in the input sequence, the BiLSTM has contextual information from both the past and the future relative to that point.
    
2. **Dual LSTM Layers**: A BiLSTM consists of two LSTM layers that are run in parallel: one processes the data from start to end (forward pass), and the other processes the data from end to start (backward pass). The outputs of these two layers are then combined at each time step.
    

### Advantages of BiLSTM

1. **Enhanced Contextual Understanding**: By incorporating information from both past and future contexts, BiLSTMs can capture nuances and dependencies in sequential data more comprehensively than standard LSTMs. This is particularly beneficial in tasks where understanding the entire sequence is crucial for accurate predictions or classifications.
    
2. **Application in Various Domains**: BiLSTMs are highly effective in many natural language processing tasks like sentiment analysis, text classification, and named entity recognition. They are also used in other sequence data tasks like speech recognition.
    
3. **Improved Model Performance**: For many tasks, especially those involving language data, BiLSTMs often outperform traditional LSTMs because they provide a more complete understanding of the input sequence.
    

### How BiLSTMs Work in Practice

- **Data Flow**: In a BiLSTM, input data flows through two LSTM layers simultaneously. One layer captures forward dependencies (what comes next), and the other captures backward dependencies (what came before).
- **Output Combination**: The outputs from the two layers are combined at each time step, which can be done in various ways, such as concatenation or summing.

### Applications and Limitations

- **NLP and Beyond**: BiLSTMs have shown significant success in complex NLP tasks. They are especially useful where the context in both directions is important for understanding the meaning or intent of a given segment in the text.
- **Limitations**: While powerful, BiLSTMs are also more computationally intensive than standard LSTMs due to the dual processing paths. Additionally, they may not be the best choice for real-time applications as the future context is required at each step, meaning the entire sequence must be available before processing.

### Summary

BiLSTMs represent an advancement in handling sequential data by providing a framework that takes into account the entire context of a sequence, both past, and future. This makes them a powerful tool in many domains, particularly in complex sequence modeling tasks in natural language processing. However, their increased computational demand and the requirement for complete sequence availability can be limiting factors depending on the application.


#### Extra approach:  1D Convolutional Neural Network

![](images/Pasted%20image%2020231206155653.png)





Time forecasting and time serios

"It's useful if there are patterns. If you have patterns and the model are learning to recognize patterns". 


Regarding cross validation, multiple approaches are possible. 





----


Your paragraph on time series analysis using neural networks is largely accurate but can be enhanced with additional details and corrections, particularly regarding LSTM networks and their applications. Here’s an improved version of the paragraph:

In the comprehensive notes on time series analysis using neural networks, a variety of key concepts and models are examined, with a notable emphasis on Long Short-Term Memory (LSTM) networks. As a specialized form of Recurrent Neural Network (RNN), LSTMs effectively address the limitations of traditional RNNs, such as the vanishing gradient problem, which is pivotal in processing long sequences of data. Their unique architecture, which includes memory cells and multiple gates (input, output, and forget gates), allows them to adeptly balance long-term memory with short-term inputs, making them ideal for a range of tasks that involve analyzing and forecasting sequential data, such as interpreting time series data from accelerometers.

The LSTM's architecture is designed to store information over extended periods through its memory cells, and it employs three distinct types of gates to regulate information flow. These networks sequentially process input data, updating the cell state with new inputs while filtering out irrelevant information. The resultant output is then utilized for predictions or as input to subsequent LSTM cells in the sequence. LSTMs are particularly valued for their capacity to handle long-term dependencies, accommodate variable sequence lengths, and learn intricate data patterns, thus finding extensive applications in fields like natural language processing, time series forecasting, and speech recognition.

An advancement of the LSTM is the Bidirectional Long Short-Term Memory (BiLSTM) network, which enhances context comprehension by processing data in both forward and backward directions. This approach employs two layers - one processes data from start to end, while the other operates in reverse - providing a more comprehensive understanding of the input sequence. BiLSTMs are particularly effective in natural language processing tasks, offering improved performance due to their enhanced contextual grasp. However, they are more computationally demanding and necessitate access to the entire data sequence for processing, which can be a constraint in scenarios requiring real-time analysis.

In addition to these, 1D Convolutional Neural Networks (CNNs) are recognized as another valuable approach for time series forecasting and analysis. These models are especially effective when identifiable patterns are present in the data, enabling the network to efficiently learn and recognize these patterns.

Cross-validation remains a critical aspect of model evaluation in time series analysis. It provides a framework for assessing the robustness and reliability of predictions by employing multiple validation approaches. These neural network models, including LSTMs, BiLSTMs, and 1D CNNs, signify significant advancements in processing sequential data. They offer potent tools for handling complex tasks across diverse domains, extending from natural language processing to intricate time series analysis.


---



# Seq2seq


Sequence-to-sequence (seq2seq) learning is used to convert a sequence of input data into a sequence of output data. It's frequently used in NLP context.


![](images/Pasted%20image%2020231214170642.png)


The seq2seq process trains an encoder-decoder model. The encoder takes the input sequence and turns it into a fixed-length representation called the "context." The decoder then takes the context and forms the output sequence. Both the encoder and decoder usually use either recurrent neural networks (RNNs) or long short-term memory (LSTM) networks, as they're able to handle time-based dependencies in the input and the output sequences


Given an input sequence $x_1,x_2,...,x_m$ andatarget output sequence $y_1,y_2,...,y_n$ we aim the sequence which maximizes the conditional probability : 

$$
\begin{aligned}y^*&=\underset{y}{\operatorname{argmax}}P(y_1,y_2,...,y_n|x_1,x_2,...,x_m)\end{aligned}
$$

in sequence-to-sequence modeling, we learn from data a model $\mathrm{P( y|x, \theta) }$ and our prediction now becomes

$$
y'=\underset{y}{\operatorname*{argmax}}P(y_1,y_2,...,y_n|x_1,x_2,...,x_m,\theta)
$$


![](images/Pasted%20image%2020231214171207.png)


The target sentence is generated in the target language, word by word, until the end-of-sequence token (`<EOS>`) is produced.
The source sentence is given to an encoder network with each word changed into a word embedding: a vector that can express the word's meaning. The encoder network uses these vectors and communicates information from one unit to the next.

The first unit starts with a "zero vector": an initial state that is updated with each new input. After going through the whole source sentence, the final state represents this sentence. That state is used as the starting point for the decoder.

The decoder starts with a start-of-sequence token and uses the state from the encoder. It then creates the target sentence, one word at a time. To predict the next word, it makes an output vector. This output is changed into a word embedding representing the predicted word. The process continues until an end-of-sequence token is produced, completing the sentence in the target language.


`<PAD>`: During training, examples are fed to the network in batches. The inputs in these batches need
to be the same width. This is used to pad shorter inputs to the same width of the batch
`<EOS>`: Needed for batching on the decoder side. It tells the decoder where a sentence ends, and it
allows the decoder to indicate the same thing in its outputs as well.
`<UNK>`: On real data, it can vastly improve the resource efficiency to ignore words that do not show
up often enough in your vocabulary by replace those with this character.
`<SOS>/<GO>`: This is the input to the first time step of the decoder to let the decoder know when to
start generating output.


Approaches:


- **greedy decoding**: at each step, pick the most probable token. This greedy approach doesn't guarantee to find the best sequence. Formally: $y^{\prime}=\mathop{\mathrm{argmax}}_{y}\prod_{t=1}^{n}P(y_{t}|y_{<t},x_{1},x_{2},...,x_{m},\theta)\approx\prod_{t=1}^{n}\mathop{\mathrm{argmax}}P(y_{t}|y_{<t},x_{1},x_{2},...,x_{m},\theta)$
- **beam search**: it keeps track of several most probably hypotheses and

## Word Embedding


Words lives in an high dimensional space but **very sparse** -> curse of dimensionality. 
Words live in a complex space and this can often lead to difficulties known as the "curse of dimensionality". This is evident when a system has to assign probability to many different word combinations, needing more and more data as the number of unique words increase.

In reality, a set of text may contain up to a million unique words, but the context might only need two words at a time.

That's why we used word embedding, which helps simplify the process and improve performance. Word embedding turns a word into a simpler number format. It does this by moving the word from a space where all words are mixed together, to a simpler, digital space. 

In this space, similar words are nearby, meaning they have similar meanings and can be grouped together. For example, "cat", "dog", and "pet" might be grouped together, while "car", "bus", and "vehicle" form a different group because they're related to transportation. 


Fighting the curse of dimensionality with:

- Compression (dimensionality reduction)
- Smoothing (discrete to continuous)
- Densification (sparse to dense)




Different approaches:

1. Local representations examples:
	- **N-Grams**: A sequence of 'n' items from a given text. In n-gram models, one-hot vectors can be used to represent the words in a corpus. In this representation, each unique word in the corpus is represented by a vector with a single element set to 1 , and all other elements set to 0 . For example, if the corpus contains the words "cat", "dog", and "bird", the one-hot vectors for these words might be $[1,0,0],[0,1,0]$, and $[0,0,1]$, respectively. These vectors can be used as inputs to an n-gram model, which uses the context of the surrounding words to predict the next word in a sequence. In N-gram language models, the goal is predict the probability of a sentence given the previous words, called "context". Determine $P\left(s=w_{1}, \ldots, w_{k}\right)$ in some domain of interest. 
	- **Bag-of-words**: A representation of text that describes the occurrence of words within a document.
	- **1-of-N-coding**: Also called 'one-hot' coding, is a group of bits among which the legal combinations of values are only those with a single high (or '1') bit and all the others low (or '0').

2. Continuous representations examples:
	- **Latent semantic analysis**: A technique in natural language processing of analyzing relationships between a set of documents and the terms they contain.
	- **Latent Dirichet allocation**: A way of automatically discovering topics that these sentences contain.
	- **Distributed representations**: The capacity of a neural network to represent (or encode) a wide array of inputs using fewer units or nodes.



## Word2Vec


Word2Vec is a model that helps computers understand language. It's simpler and quicker than other models because it doesn't have a hidden layer, which speeds up its training process. It uses the same weights for all words and considers both past and future words to understand the context better. 

Training involves tweaking word vectors of surrounding words to increase the chance of the exact word appearing. It results in a set of vectors for each word, which can be used for tasks like language modeling, determining word similarity, and classifying text. 

The output predicts the likelihood of a word based on its surrounding words, aiming to refine the vectors to increase the chance of the correct word appearing. 

The output is a probability distribution over the vocabulary, which is used to predict the target word given the context words. The probability distribution is generated using the softmax function, which takes as input the dot product of the context and target word vectors, and outputs a value between 0 and 1 for each word in the vocabulary. The goal of training is to adjust the word vectors in a way that maximizes the probability of the correct target word given the context words.

Word2Vec learns word placements based on their surrounding words. It groups similar words close together. Then it predicts the likelihood of a word based on its placement relative to other words. 

This passage is about predicting a target word from the words surrounding it. It uses a 'probability distribution' - basically a scale of likelihood - for all possible words in its vocabulary. To create this scale, it uses a formula called the 'softmax function'. The 'softmax function' uses the measure of similarity between the surrounding words and each possible target word. This number is then converted into a likelihood between 0 and 1 for each potential target word. The aim is to keep tweaking the measure of similarity to get as close to the true target word as possible.


In the word2vec model, the complexity is

$$
n \times m+m \times \log |V|
$$

That's because for each context-target pair, the model updates only the context words. The complexity is determined by the number of context-target pairs $(n \times m)$ and the number of context words $(\mathrm{m})$ that need to be updated in each pair. The $\log |\mathrm{V}|$ term represents the complexity of updating the weights of the projection layer, which is shared among all context words.

The passage also talks about the complexity of the word2vec model. It explains that the complexity is a factor of the number of surrounding words and target words (n x m) plus the number of surrounding words (m), all adjusted by the complexity of updating the weights. The weights are simply the impact each surrounding word has on the probability of each potential target word.



The model is less complex because it doesn't have a hidden layer and shares a projection layer across all pairs, allowing for quicker training on more data. It doesn't need classifiers and uses cosine distances instead.









Word2Vec is a tool that helps computers understand human language. It's faster and simpler than other models because it doesn't use a hidden layer. This makes it faster to train. It uses the same weights for all words and looks at both previous and future words to understand the context.

Training modifies word vectors of nearby words to increase the likelihood of the specific word appearing. This results in a group of vectors for each word, which can be used for tasks like recognizing word similarities, organizing language, and categorizing text.

The result predicts the probability of a word based on nearby words. This is aimed at improving the vectors to increase the probability of the accurate word appearing.

The result is a probability distribution across the vocabulary, used to predict the target word from the context words. The probability distribution is produced with the softmax function. This function takes the dot product of the context and target word vectors and delivers a value between 0 and 1 for each vocabulary word. The aim of training is to adjust the word vectors to maximize the probability of the correct target word given the context words.

Word2Vec learns word positions based on their nearby words. It groups similar words closely. Then it predicts the probability of a word based on its position relative to other words.

This passage is about predicting a target word from the words near it. It uses a 'probability distribution' - a likelihood scale - for all words in its dictionary. To create this scale, it uses a formula called the 'softmax function'. The 'softmax function' uses the similarity of the surrounding words and each target word. This number is then turned into a likelihood between 0 and 1 for each potential target word. The aim is to keep adjusting the similarity measure to get as close to the true target word as possible.

The piece also covers the complexity of the word2vec model. The complexity is a factor of the number of surrounding words and target words (n x m) plus the number of surrounding words (m). These are all modified by the complexity of updating the weights. The weights are the effect each surrounding word has on the probability of each potential target word.

The model is simpler because it doesn't have a hidden layer and uses a shared projection layer for all pairs, which allow for faster training on more data. It doesn't require classifiers and uses cosine distances instead.





--- 


## Attention Mechanism in Seq2Seq Models

In sequence-to-sequence (Seq2Seq) models with attention, the decoder has an attention mechanism that allows it to focus on different parts of the input sequence while generating the output sequence. This can be particularly useful in situations where the input and output sequences are of different lengths, or when the input sequence is long and the decoder needs to selectively attend to certain parts of it in order to generate the correct output.

The attention mechanism works by first computing a set of attention weights, which are used to weight the different parts of the input sequence when generating the output. These weights are computed using a function that takes as input the current hidden state of the decoder, $h_{t}$, and the hidden states of the encoder, $h_{1}, h_{2}, \ldots, h_{n}$. The attention weights, $a_{1}, a_{2}, \ldots, a_{n}$, are then computed applying the softmax function on the scores, as:

$$
a_{i}=\frac{\exp \left(e_{i}\right)}{\sum_{j=1}^{n} \exp \left(e_{j}\right)}
$$

where $e_{i}$ is an attention score computed as:

$$
\begin{gathered}
\qquad e_{i}=\operatorname{score}\left(h_{t}, h_{i}\right) \\
\operatorname{score}\left(\boldsymbol{h}_{t}, \overline{\boldsymbol{h}}_{s}\right)= \begin{cases}\boldsymbol{h}_{t}^{\top} \boldsymbol{W} \overline{\boldsymbol{h}}_{s} & \text { [Luong's multiplicative style] } \\
\boldsymbol{v}_{a}^{\top} \tanh \left(\boldsymbol{W}_{1} \boldsymbol{h}_{t}+\boldsymbol{W}_{2} \overline{\boldsymbol{h}}_{s}\right) & \text { [Bahdanau's additive style] }\end{cases}
\end{gathered}
$$

The idea is to give the decoder more information about the source sequence (the input to the encoder) when generating each target word. The attention mechanism allows the decoder to "pay attention" to different parts of the source sequence at each time step, rather than only considering the final hidden state of the encoder.

The function $\operatorname{score}(\cdot, \cdot)$ can be any function that takes two hidden states as input and outputs a scalar attention score. A common choice is to use a dot product or a multi-layer perceptron (MLP) to compute the attention score.

Once the attention weights have been computed, they are used to weight the hidden states of the encoder when generating the output. This is done by computing the context vector as a weighted sum of the hidden states of the encoder, $c_{t}$ :

$$
c_{t}=\sum_{i=1}^{n} a_{i} h_{i}
$$

The weighted sum is then concatenated with the current hidden state of the decoder, $h_{t}$, and passed through an MLP to generate the output at time step $t$. The output at time step $t$ is computed as:

$$
o_{t}=\operatorname{MLP}\left(h_{t}, c_{t}\right)
$$

The attention mechanism can be thought of as a way to selectively weight different parts of the input sequence when generating the output, allowing the decoder to focus on the most relevant parts of the input when generating each output time step.

The output of a decoder layer at timestep $t$ is influenced not only by its own hidden state ht but also by an "attention context vector" ct, which is a weighted combination of the hidden states of the encoder hs. The attention mechanism works by first computing an "attention score" or "alignment" between each encoder hidden state and the current target hidden state, using a similarity function such as dot product or a feedforward neural network. These scores are then used to compute a "soft alignment" or "attention weights" alpha, which are used to weight the encoder hidden states when computing the attention context vector ct. The attention context vector ct is then concatenated with the current target hidden state ht and used as input to the next decoder layer or to compute the final output of the decoder




Attention models 



Bahdanau Attention Model 
Loung Attention Model 




how to handle very very long context?


Possible solution are generative hierarchical chatbots. 

· LSTM cells often fail to catch longterm dependencies within input sequences
that are longer than 100 tokens
. No explicit representation of turns can be exploited by attention mechanism

Xing et al., in 2017, extended attention mechanism from single-turn
response generation to a hierarchical attention mechanism
· Hierarchical attention networks (e.g., characters -> words -> sentences)
· Generate hidden representation of a sequence from contextualized words

NLP community believed LSTMs with attention could yield state-of-art
performance on any task. But some limits were preventing this ...

Because of using LSTMs (and any Recurrent Neural Network):

- Performing inference (and training) is sequential in nature
- Parallelization at sample level is precluded by recurrence sequential nature
- Parallelization can happen at level of batch only
- Memory constraints limit batching across to many examples
- This becomes critical at longer sequence lengths ...


# Transformers 

Google proposes to speed up training by replacing RNN (sequential in nature) with attention mechanism (parallel in nature)

At each level we look at the entire sequence

Look at other tokens -> Gather context -> update token representation. 


Everything is based on the **attention**

Attention defines the role of a word in a sentence. This, in turn, might be related to different aspects: 
- verb inflection wrt subject in terms of gender
- verb inflection wrt subjects in terms of number
- case Of Objects defines by verbs





This is implemented via:

- Query - asking for information;
- Key - saying that it has some information;
- Value - giving the information

The use of Query, Key and Value allows parallel execution and thus parallel training!

Attention computed as a dot product between of the query (linear transformation of the input) and the keys (linear transformation of the output) 


and then : 


$$Attention(q,k,v)=\text{softmax}\left(\frac{q\cdot k^T}{\sqrt{d_k}}v\right)$$




The Self-Attention mechanism is inherently permutation invariant; it does not rely on the position or order of words in a sequence. Changing the word order doesn't affect the attention values, just their sequence. 

To address this, **positional encoding** is used to make self-attention also consider the position of the input. 

In positional encoding, a token input representation includes both the embeddings for tokens (a regular practice) and for positions (essential for this particular model). 

Although positional embeddings can be learned, Transformers use fixed positional encodings. The input is the sum of two embeddings: one for the token, and another for the position.


The idea for example to use a sinusoidal functions it to permit the model to learn 

Nevertheless, state of the art Transformers (BERT/ RoBERTa, GPT-2, ...) learn the positional encoding instead of using a fixed one. 






Last exercitation 



