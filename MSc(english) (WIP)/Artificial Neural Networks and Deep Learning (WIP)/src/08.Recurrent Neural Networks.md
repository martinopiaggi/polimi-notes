
# Recurrent Neural Networks

We've only discussed static datasets, unchanging over time till now: 
- **Static** datasets are unchanging over time.
- **Dynamic** datasets change over time and require special handling.
    
To handle **Dynamic Datasets** two main approaches exists: Memoryless models and Models with memory.

- **Memoryless Models**
    - These models take only the current input into account, without memory or retention of past inputs or outputs. Example of this: 
	    - **Autoregressive Models**:
		    - **Function**: Predict the next value in a sequence using past values.
		    - **Mechanism**: Utilizes "delay traps" to capture information from previous time steps.
		    - **Lag Order**: Refers to the number of previous time steps used for prediction. For instance, a lag order of 3 uses the past three values.
		    - **Training**: Parameters are estimated using methods like least squares on training data.
		    - **Application**: Useful for making predictions in time-series data.
	    - Feedforward Neural Networks (FFNNs): 
		    - **Dynamic Data Handling**: Can model dynamic data by incorporating nonlinear hidden layers.
		    - **Nonlinear Layers**: These layers learn complex relationships, allowing the network to predict future values more accurately.
		    - **Advantages**: FFNNs with nonlinear hidden layers can capture more intricate patterns in data than linear models.
- **Models with Memory**
    - These models retain past input information, which subsequently influences the current output. Models with memory:
	    - **Linear dynamic systems**:
		    - **Characteristics**: Governed by linear differential equations linking current and past states.
		    - **State Estimation**: Involves continuous states with Gaussian uncertainty, linear transformations, and can utilize Kalman filtering for state estimation.
	    - **Hidden Markov Models (HMMs)**:
		    - **Basics**: Probabilistic models with hidden states and observable outputs.
		    - **State Dynamics**: States are discrete with stochastic transitions and outputs dependent on these hidden states.
		    - **State Estimation**: Viterbi algorithm is commonly used for estimating states in HMMs.



## RNNs

- RNNs are a type of neural network designed for sequential data processing.
- They can deal with time series, natural language, and audio due to their "recurrent" component, which allows for operations consistency across the input sequence.




- Feedback is vital for memory simulation and retention.
- RNNs can potentially compute any process a computer can if given enough neurons and time.
- Their function resembles Turing machines - completely computational models with memory.


A "context" network which acts as a memory. In principle this recurrent part of the network acts as an infinite memory. This is achieved through hidden states that effectively 'remember' past inputs. 


How to implement this in a neural network? 

**"Context" Network and Memory:**

- Function as a memory, referred to as a "context" network.
- The hidden state serves as an 'infinite' memory which recalls past inputs.


**Structure of RNNs consist of input, hidden, and output layers:**
- The input layer receives the sequence.
- Hidden layer stores recurrent units or neurons that recall past inputs, essential for sequential data processing.
- Output layer produces the final result.


## BPTT

Backpropagation Through Time (BPTT) is an adaptation of the normal backpropagation algorithm, is designed for sequential data. In usual neural networks, backpropagation calculates the gradient of loss function for each weight, determining how a change in weight influences the total error, and adjusts the weights to minimize this error.

- **RNNs (Recurrent Neural Networks)**: RNNs possess a unique structure that allows for the sharing of weights uniformally across sequential time steps, enabling the processing of data series. Not just the current input but also previous ones influence the output. The temporal dependencies hence introduced complicate the training process.
- **BPTT (Backpropagation Through Time)**: BPTT is designed to handle the training complexity by "unrolling" the RNN over the sequence's time. Each time step is treated as a distinct layer in a deep network, transforming the network into a standard deep neural framework with each layer representing a time step in the initial RNN.





Here's a step-by-step overview of BPTT:

1. **Forward Pass**: Just like in standard backpropagation, BPTT begins with a forward pass through the network. For an RNN, this means processing the entire sequence from the first time step to the last, updating the hidden states based on the recurrent connections at each step.
2. **Compute Loss**: At the end of the sequence, a loss is computed. This loss measures the discrepancy between the predicted output of the RNN and the actual target values.
3. **Backward Pass**: Next, the gradients of this loss are propagated backward through the network. However, unlike standard backpropagation, this involves going backward through each time step, accumulating gradients as we move back in time. Since the same weights are used at each time step, the gradients from all time steps are summed up.
4. **Update Weights**: Finally, the weights are updated based on these accumulated gradients. Typically, optimization algorithms like Stochastic Gradient Descent (SGD) are used for this purpose.
    



In Recurrent Neural Networks, the processing of each new input involves a sophisticated interplay between the current data and information carried over from past inputs. This process is fundamentally driven by the network's hidden state, which serves as a dynamic form of memory, continuously updated throughout the sequence. As each new input is received, it doesn't stand alone; instead, it's integrated with the accumulated knowledge from previous inputs. This integration occurs through the recurrent connections and their associated weights, which are pivotal in determining the influence of past information on the current state. During sequential processing, the RNN's calculations at any given step incorporate both the fresh input and the data stored in the hidden state, ensuring that the output at each step is shaped by the entire sequence of prior inputs. This mechanism, often referred to as the 'recurrent context', is not about repeatedly sending old inputs but rather about retaining their effects in the hidden state and bringing this contextual background into current computations.




RNNs process sequences by considering both current and past inputs, thanks to their recurrent structure. This allows them to capture dependencies that span across time steps.



The hidden states (`h1_A`, `h1_B`, etc.) keep evolving with each input in the sequence. They get 'updated' to reflect the information they've accumulated from all past inputs up to that point.

In summary, the essence of RNNs lies in this mechanism of combining current inputs with information from past inputs (via hidden states) and updating these hidden states continuously. This process enables RNNs to 'remember' and leverage the sequential context of the data they are processing, which is fundamental to their ability to handle tasks involving sequential or time-dependent data effectively.



"When you have a loop, you can always unroll the iterations of them" 

You can always "cut/truncate" the roll. 


![](images/e28a04dca244d4509fcb23a9f26f0964.png)

Actually you don't unroll, you just "sum/average" the sequence of all the hidden states (past). 

BPTT allows RNNs to learn from long sequences and capture long-term dependencies, but it also has challenges. One significant issue is the vanishing and exploding gradient problem, where gradients become too small or too large as they are propagated back through many time steps. This issue can make training RNNs with BPTT challenging, especially for very long sequences. Techniques like gradient clipping, gated recurrent units (GRUs), and long short-term memory (LSTM) units are often used to mitigate these problems.




