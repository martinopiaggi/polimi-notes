

## Training and overfitting 

We will discuss some refinements that will make feedforward neural networks more effective in practice. 


- Training dataset: the available data
- Training set: the data used to learn model parameters
- Test set: the data used to perform final model assessment
- Validation set: the data used to perform model selection
- Training data: used to train the model (fitting + selection)
- Validation data: used to assess the model quality (selection + assessment)



Regarding the issue of overfitting. 
Overfitting occurs when your model becomes too flexible and fits noise or irregularities present in training data instead of capturing underlying patterns or generalizing well to unseen data points during testing/validation phases.

It is important to strike a balance between simplicity and flexibility when designing neural networks to avoid both overfitting and underfitting.


Obviously it's huge the importance of evaluating models on separate test data, which can be provided by a company or split from the original dataset.

If the dataset is small, bootstrap sampling can be used to evaluate models without splitting the data. However, having enough data is crucial for effective learning.

It is common practice to shuffle and split the data into training and test sets. In fact, even in the first lab exercise, a pre-split dataset was provided.

Overall, it is important to consider model complexity, avoid overfitting through evaluation on separate test data, and ensure sufficient amounts of quality training data are available.


Cross-validation is the use of the training dataset to both train the model (parameter fitting + model selection) and estimate its error on new data
- When lots of data are available use a Hold Out set and perform validation
- When having few data available use Leave-One-Out Cross-Validation (LOOCV)
- K-fold Cross-Validation is a good trade-off (sometime better than LOOCV)




It is important to understand the data distribution and identify outliers and relevant features. This process involves using modeling techniques to analyze the data. One common technique is cross-validation, which uses a validation dataset to evaluate the model's performance.



## Hyperparameters

In summary,Â model parameters are estimated from data automatically and model hyperparameters are set manually and are used in processes to help estimate model parameters. 

Hyperparameters: 

- Number of neurons and layers
- Activation functions
- Error functions
- Gradient descent algorithm
- Batch dimension
- Validation set dimension


We will see 3 techniques:

- Early stopping
- Weight decay 
- Dropout 

Whose highlights are: 

- Early stopping wastes data but works out of the box. No hyperparameter. Early stopping is also possible to the trick to put "all together" epochs/number of neurons in one "last training". 
- Weigth decay you have to select for best gamma. We need to tune hyperparameter.
- Dropout you have to select best probability to dropout. We need to tune hyperparameter.


###  Early stopping  

The training can be stopped according to a set of hyperparameters. 

![](images/Pasted%20image%2020231003165335.png)


Online estimate of the generalization error

An online estimate is used to predict the behavior of your network on UD. It provides an estimate of the performance you should expect when you stop the network. By training multiple networks with early stopping, you can compare them by analyzing the validation error.

s
### Cross-validation and Hyperparameters Tuning


In a similar way we can "Try and error" to tune and select the complexity of a network. You use the early stopping to prevent overfitting and then you monitor validation error to check which is the best hyperparameters. 

![](images/Pasted%20image%2020231003165900.png)





### Weight Decay: Limiting Overfitting by Weights Regularization

Regularization is about constraining the model "freedom", based on a- priori assumption on the model, to reduce overfitting.

In the case of weight regularization, the weights in the model are constrained to be smaller. This is because smaller weights can help with better generalization. To achieve this, a penalty term proportional to the sum of the squares of the weights is added to the loss function. This forces the gradient to decrease the overall norm of the weights. To determine the best value for the regularization parameter, again cross-validation is possible:  training the model with different values and choose the one that yields the best results.


### Dropout: Limiting Overfitting by stochastic Regularization

The Dropout works by randomly selecting a subset of neurons and turning them off during each epoch of the training phase. This forces the network to learn redundant representations of features, which helps in reducing overfitting. 
Each epoch, a different set of neurons is turned off according to the **dropout probability**, denoted as $p$, which is a hyperparameter. 
The final network is an ensemble of different "subnets". The weights are scaled to average the output of these subnets.




## Vanishing and exploding Gradient

The vanishing gradient problem occurs when the gradients of the parameters with respect to the loss function become very small. This problem is particularly prominent when using activation functions such as **Sigmoid** or **Tanh**.

The vanishing gradient problem inhibits effective learning in **deep** networks because the backpropagation algorithm, which adjusts the parameters based on these gradients, requires gradient multiplications. When the gradient is close to zero or less than 1, these multiplications can cause the gradient to vanish, resulting in little to no learning.

A similar issue is the **exploding gradient**: if the gradient keeps getting multiplied at each layer, it can grow exponentially leading to unstable and unreliable training.

The **ReLU** activation function is a solution to both gradient explosion and vanishing, since it has a "efficient gradient propagation":
$$
g(a)=\operatorname{ReLu}(a)=\max (0, a)
$$

Or the **Leaky ReLU** which fix the"dying neurons" problem: by introducing a small slope for negative inputs, Leaky ReLU ensures that the gradient is non-zero even when the neuron is inactive. 
Other variations include the **ELU**, which is parametric, and the **Randomized ReLu**, with randomly chosen slope.

## Weight Initialization

There are different approaches:

- Zeros: it does not work! All gradient would be zero, no learning will happen
- Big Numbers: bad idea, if unlucky might take very long to converge
- $w \sim N\left(0, \sigma^2=0.01\right)$ : good for small networks, but it might be a problem for deeper neural networks
- $w_{i, j} \sim \mathcal{N}\left(0, \frac{1}{n_{i}}\right)$ where $n_i$ is the number of inputs to the network layer. This makes the total variance of activations equal to $1$.  It depends on the number of inputs of each layer, it's something "dynamic". 

Performing similar reasoning for the gradient Glorot \& Bengio found
$$
n_{\text {out }} \operatorname{Var}\left(w_j\right)=1
$$
To accommodate for this and Xavier propose $w \sim N\left(0, \frac{2}{n_{\text {in }}+n_{\text {out }}}\right)$ The default initialization of Tensorflow is this.  



More recently He proposed, for rectified linear units, $w \sim N\left(0, \frac{2}{n_{i n}}\right)$



Note that considerations over the variance are important to ensure stable learning:

- if all weights are initialized with large values there is a risk of exploding gradients. 
- On the other hand, if all weights are initialized with small values, there is a risk of vanishing gradients.


## Batch normalization


Input manipulation is commonly done to improve performance in neural networks. This typically involves operations such as normalization and standardization. Normalization is used to adjust values to a range of [0, 1], while standardization ensures a mean of 0 and a standard deviation of 1. These operations aid in the feature extraction process and can enhance training. However, as deep layers are involved, their impact is often reduced.

To further boost network convergence, input whitening is done. This involves ensuring inputs have a zero mean and unit variances. By making inputs uncorrelated, this accounts for covariate shift and accelerates convergence. It should be noted that not only the input, but also the hidden layers can benefit from normalization.

Batch normalization is a technique used to address internal covariate shift. It achieves this by ensuring activations have values following a unit Gaussian distribution at the start of training. A BatchNorm layer is introduced to standardize inputs and outputs of each layer. This layer consists of two weights per batch: a scale factor and a bias, which can be adjusted during training.

In practice:

- Each unit's pre-activation is normalized to $x_{i, j} \sim \mathcal{N}(0,1)$, also the input values;
- During training, mean and stddev are computed for each minibatch (output of a subset of the net);
- Backpropagation takes into account normalization;
- At test time, the global mean / stddev are used (global statistics are estimated using training running averages);


## Learning Rate Optimization

The learning rate **hyperparameter** of gradient descent algorithm can be set (to achieve better performance) using different approaches. 
It can be fixed, but actually the approaches where it's adaptive are more used. Examples of approaches:

- Resilient Propagation (Rprop - Riedmiller and Braun 1993)
- Adaptive Gradient (AdaGrad - Duchi et al. 2010)
- RMSprop (SGD + Rprop - Teileman and Hinton 2012)
- AdaDelta (Zeiler et at. 2012)
- **Adam** (Kingma and Ba, 2012)
- ... 




