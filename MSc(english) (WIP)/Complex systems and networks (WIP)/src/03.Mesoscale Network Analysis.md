# Mesoscale Network Analysis 



Meso means that we are analyzing properties that are not in micro or macro scale but something in the middle. 
There are structure in the middle zone?

Mesoscale network analysis encompasses both community detection and core-periphery analysis. Here are the key points for each:

## Community detection




### Modularity 



Ego chambers 


Modularity is a measure of the degree to which the links within a community are denser than what would be expected by chance. It quantifies the strength of connections within a community, surpassing random link placement based on node degree alone.

max modul: no parameters



Zachary's "karate club" social network

1. **Modularity Optimization**:
    
    - Communities are sets of nodes with denser internal connections compared to external ones.
    - Modularity $Q$ quantifies intra-/inter-community link densities against a null model.
    - High $Q$ values indicate significant community structure​​.
2. **Louvain Method**:
    - Popular for its speed and precision in modularity optimization.
    - Involves moving nodes to adjacent communities to increase modularity, followed by building a meta-network of these communities​​.
3. **Extensions and Drawbacks**:
    - Applicable to directed, weighted, and large networks.
    - Challenges include ensuring the quality of partitions and identifying individual community quality​​.


### Random walkers 

4. **Random Walkers Method**:
    - Random walkers tend to remain within a community due to denser internal connections.
    - Infomap, based on information theoretic coding of random paths, is a notable implementation​​.
    - Example: Applied to a citation network revealing thematic modules​​.
5. **Persistence Probabilities**:
    - Communities should have large persistence probabilities, indicating longer times spent by random walkers within them​​.

#### Infomap 

The Infomap algorithm is a network analysis method used for detecting communities in networks. It leverages the concept of information flow on the network to reveal community structure. Here's an explanation of how it works:

1. **Random Walks as a Proxy for Information Flow**: Infomap uses random walks to mimic the flow of information in a network. The basic idea is that a random walker (an agent moving from node to node) tends to get trapped for longer periods within densely interconnected groups of nodes, which are interpreted as communities.
    
2. **Minimizing the Description Length of Random Walks**: The algorithm aims to find a community division that minimizes the description length of these random walks. Description length here refers to the amount of information required to describe the path of a random walker. It's a measure derived from information theory.
    
3. **Two-Level Description**: Infomap uses a two-level description of the random walk:
    
    - At the first level, each community is assigned a unique code.
    - At the second level, individual nodes within a community are given unique codes.
4. **Encoding the Random Walks**: During a random walk, when the walker stays within the same community, only the second-level code (node code) is used. When the walker moves to a different community, both the first-level (community code) and the second-level codes are used. This dual coding scheme tends to be more efficient when the walker spends long periods within the same community, highlighting the community structure.
    
5. **Optimization Process**: Infomap iteratively adjusts the community assignments to minimize the overall description length of the random walks. The more efficient the description (i.e., the shorter the description length), the better the community structure the algorithm has uncovered.
    
6. **Result**: The output of the algorithm is a partition of the network into communities where the information flow (as approximated by random walks) is efficiently described, indicating strong internal community structure and weaker between-community interactions.
    

Infomap is particularly effective for networks where the flow of information is a relevant characteristic, and it's widely used due to its efficiency and the quality of the communities it detects.

![](../images/9c717dc77631739526f250220c728abb.png)



### Lumped Markov Chain 


A "Lumped Markov Chain" is a concept in the study of Markov chains, which are mathematical systems that undergo transitions from one state to another according to certain probabilistic rules. The idea of "lumping" in this context refers to the process of simplifying a Markov chain by grouping together certain states into a single state, under specific conditions. In summary, lumping in Markov chains is a technique for simplification that aggregates states under certain conditions, allowing for a more manageable analysis of the system’s behavior.



1. **Basic Concept of Markov Chains**: A Markov chain is a stochastic process that transitions from one state to another based on a fixed set of probabilities. The key property of a Markov chain is that the future state depends only on the current state and not on the sequence of events that preceded it (this is known as the Markov property).    
2. **Lumping States**: In a Lumped Markov Chain, multiple states of the original Markov chain are aggregated or "lumped" together to form a new, reduced state in a simplified chain. For instance, if a Markov chain has states {A, B, C, D}, one might lump states B and C together into a single state, say X, resulting in a new chain with states {A, X, D}.
3. **Conditions for Lumping**: Not all states can be lumped together arbitrarily. For a valid lumping, the combined transition probabilities from any lumped state to other states (lumped or not) must be consistent with the original chain. Specifically, the probability of moving from the lumped state to any other state in the reduced chain should be independent of the particular original state within the lumped state.
4. **Purpose of Lumping**: The lumping of states is typically done to reduce the complexity of a Markov chain. By decreasing the number of states, the chain becomes easier to analyze and understand. This is particularly useful in applications where certain states of the system naturally group together or when the fine details of the state transitions are not critical for the analysis.
5. **Applications**: Lumped Markov Chains are used in various fields including queuing theory, economics, biology, and other areas where complex stochastic systems are modeled. They help in understanding the broader behavior of the system without getting lost in the intricacies of individual state transitions.
6. **Analysis and Computation**: Analyzing a Lumped Markov Chain involves calculating the new transition probabilities between the lumped states and then studying the properties of the simplified chain, like steady-state distributions, absorption probabilities, and expected time to absorption, which often become more tractable in the reduced model.


$\mathbb{C}_c$ is a set of nodes (a "candidate" community), and $\mathbb{P}_q=$ $\left\{\mathbb{C}_1, \mathbb{C}_2, \ldots, \mathbb{C}_q\right\}$ is a partition.

The dynamics of the random walker at this aggregate scale ("meta-network") is described, at stationarity $\left(\pi_0=\pi\right)$, by the $q$-state lumped Markov chain
$$
\Pi_{t+1}=\Pi_t U \quad \text { where } \quad U=[\operatorname{diag}(\pi H)]^{-1} H^{\prime} \operatorname{diag}(\pi) P H
$$
$u_{c d}=$ probability that the random walker is at time $t+1$ in any of the nodes of $\mathbb{C}_d$ provided it is in $t$ in any of the nodes of $\mathbb{C}_c$

where $\Pi$ probability of transition from each node to each node
and $\pi$ probability that at regime the random walker is at a specific node 
$H$ codifica della partizione, mi dice in che comunitá $\mathbb{C}$ é ogni nodo. Quindi una matrice in cui c'é su un'asse la lista di nodi e sull'altra la lista di comunitá. Ogni nodo a quale partizione appartiene

Tutto ció mi induce la matrice $U$



The aggregate scale ("meta-network") dynamics of a random walker, at stationarity $\left(\pi_0=\pi\right)$, can be defined by its $q$-state lumped Markov chain, represented by $\Pi_{t+1}=\Pi_t U$, where $U=[\operatorname{diag}(\pi H)]^{-1} H^{\prime} \operatorname{diag}(\pi) P H$.

In this case, $u_{cd}$ denotes the probability that the walker is at any of $\mathbb{C}_d$ nodes at time $t+1$, given it was at any of $\mathbb{C}_c$ nodes at time $t$. The transition probability between nodes is represented by $\Pi$, while $\pi$ signifies the probability that the walker is at a specific node at regime.

The $H$ code, which considers partitioning, informs about the community $\mathbb{C}$ each node belongs to. It serves as a matrix having a list of nodes on one axis and a list of communities on the other, mapping each node to its respective partition.

Altogether, these elements contribute to the induced matrix $U$.




The diagonal terms $u_{c c}, i=1,2, \ldots, q$, of the lumped Markov matrix $U$ are called PERSISTENCE PROBABILITIES.

Significant communities are expected to have large persistence probability $u_{c c}$ (thus large escape time $\tau_c=$ $\left.\left(1-u_{c c}\right)^{-1}\right)$.
$u_{c c}=\frac{\sum_{i, j \in \mathbb{C}_c} \pi_i p_{i j}}{\sum_{i \in \mathbb{C}_c} \pi_i}=$ fraction of time spent by the random walker on the $\frac{\text { links }}{\text { nodes }}$ of community $\mathbb{C}_c$

If the network is undirected:

Quanto della somma di quella strength is inside the community? Very intuitive. 

$u_{c c}=\frac{\sum_{i \in \mathbb{C}_c} s_i^{\text {int }}}{\sum_{i \in \mathbb{C}_c} s_i}=\frac{\text { total internal strength }}{\text { total strength }}$ of community $\mathbb{C}_c=$ fraction of strength internally directed

If the network is undirected and unweighed:
$u_{c c}=\frac{\text { total internal degree }}{\text { total degree }}$ of community $\mathbb{C}_c>0.5 \Leftrightarrow \mathbb{C}_c$ is a "community" according to Radicchi et al.


The diagonal terms of the lumped Markov matrix are known as **persistence probabilities**, represented as $u_{c c}$. These indicate the fraction of time a random walker spends on the links/nodes of community, $\mathbb{C}_c$. 

These probabilities play a notable role in communities with significant largeness where that is indicated by a larger persistence probability i.e $u_{c c}$. Additionally, these probabilities correlate to large escape time calculations, $\tau_c=$ $\left.\left(1-u_{c c}\right)^{-1}\right)$.

In the case of undirected networks, $u_{c c}$ equals the ratio of the total internal strength to the total strength of community $\mathbb{C}_c$ i.e, the fraction of strength internally directed. 

Alternatively, if the network is undirected and unweighed, $u_{c c}$ is the ratio of the total internal degree to the total degree of the community, $\mathbb{C}_c$, where a value over 0.5 signifies that $\mathbb{C}_c$ is considered a community by Radicchi et al.




More value $u_{cc}$  is near $1$ and more they are "trap" to the random walker and can consistently represent a community $\mathbb{C}$ . 




This priority is well defined in the hypothesis that the random walkers is present in a network with its stationary distribution. The assumption of stationarity is crucial here. It means that the random walker's long-term behavior is predictable and stable, which is a key requirement for the lumping process to be meaningful. In a stationary distribution, the proportions of walkers in different states stabilize over time, allowing for a consistent aggregation of states.


![](../images/007ddfe80698b34b86f3171726d63772.png)


$\alpha$-COMMUNITIES AND $\alpha$-PARTITIONS

Set a quality level $0<\alpha<1$.
- $\mathbb{C}_c$ is an $\alpha$-community if the persistence probability $u_{c c} \geq \alpha$.
- $\mathbb{P}_q=\left\{\mathbb{C}_1, \mathbb{C}_2, \ldots, \mathbb{C}_q\right\}$ is an $\alpha$-partition if $\mathbb{C}_1, \mathbb{C}_2, \ldots, \mathbb{C}_q$ are $\alpha$-communities (i.e., $\min _c u_{c c} \geq \alpha$ ).

A strategy for community analysis:
- set the quality level $\alpha$
- generate a set of "good" candidate partitions, with different number $q$ of clusters (many algorithms are available)
- take the $\alpha$-partition with the largest $q$ (i.e., the finest decomposition with the desired quality level)
Remark: the "quality" (significance) of each individual community is simultaneously assessed.



![](../images/3fc767896d85d72aa62d64f73cba6e9d.png)

The discussion continues with an application of community analysis to an international trade network, exploring the significance of various communities within the network.

The European Union is highlighted as a strong community in terms of internal commerce. The geopolitical influence of China in Africa is also discussed, showcasing the application of community analysis in understanding global economic dynamics.







**[00:00:00]** Community analysis studies relationships between nodes in a network, aiming to identify structures at an intermediate scale between individual nodes and the entire network, specifically communities. A community is a subnet, consisting of a group of nodes and the links connecting them. In probabilistic terms, like in the Stochastic Block Model, the probability of connections within a community is typically higher than between different communities.

**[00:03:27]** The Stochastic Block Model is utilized for generating networks with structured communities. It's based on maximizing modularity, a measure ranging from -1 to +1. Higher modularity values indicate a greater number of internal links within communities compared to a random network with a similar degree sequence, suggesting strong community structure.

**[00:06:18]** Negative modularity values indicate anti-community structures, with numerous links between communities but few within them. Tools like iGraph and NetworkX are commonly used for modularity maximization. In detecting true communities, one should look for methods that capture the characteristic of having dense internal links and rare external connections.

**[00:09:11]** Defining a partition on a network leads to what's called a Lumped Markov Chain. This is a stochastic representation of a random walker's evolution over time. By understanding the transition probabilities at each state, we can describe the dynamics of the system under this partition.

**[00:12:04]** The matrix U, previously called P, is used to denote the transition probabilities in a Lumped Markov Chain. The probability of transitioning from one group to another depends on the specific node, indicating that these probabilities must be well-defined and calculated considering the network's stationary distribution.

**[00:14:54]** The probability of moving from one group to another in a Lumped Markov Chain can be understood as a weighted average, encapsulated in the formula involving the transition matrix P and the matrix H, which encodes the partitioning of the network.

**[00:18:08]** The probability that a random walker in a given node group will remain in that group in the next step is well-defined under the assumption of a stationary distribution. A higher probability indicates that the walker is likely to remain within that community, increasing the expected escape time from it.

**[00:20:52]** The concept of persistence probability in communities is explored, emphasizing the search for sub-networks with high persistence probabilities. This involves calculating the relative persistence probability of a community, considering the state probabilities aggregated over the nodes.

**[00:23:50]** The transition probabilities on internal links of a community are evaluated, along with the relationship between these probabilities and the nodes' strengths. This forms the basis for understanding the random walk centrality in the network.

**[00:26:54]** The analysis then delves into the relationship between the sum of the nodes' strengths in a community and how much of this sum is internal to the community. This helps in assessing whether nodes predominantly interact within their community.

**[00:29:56]** The concept of persistence probability is further elaborated upon, discussing its extremes in the context of single nodes and entire networks, and its relevance in defining significant communities.

**[00:32:40]** The idea of using locations in network analysis is introduced, with a focus on understanding the significance of different partitions and their persistence probabilities.

**[00:35:56]** The discussion shifts to the idea of "forcing a crack" in the network, leading to the creation of sub-networks with varying persistence probabilities. This helps in understanding the quality of different partitions and community structures.

**[00:39:14]** The concept of an alpha community is introduced, defined by a specific persistence probability. This segment explohe use of information theory measures in community analysis, looking for sub-networks with strong internal cohesion and low external cohesion.

**[00:42:26]** The quality of different communities is examined through a case study, analyzing the significance of communities based on their internal and external link structures.



The transcript then shifts to a case study involving the 'Ndrangheta, a mafia organization, using network analysis to understand its structure and operations. A bi-partite network of meetings and participants is analyzed, highlighting the use of network analysis in criminal investigations.

The network's density and the co-participation of individuals in meetings





## Core-periphery analysis 


1. **Core-Periphery Paradigm**:
    
    - Conceptualizes networks as a dense core with a sparsely connected periphery.
    - Used in economics, social sciences, and various other fields.
    - Focuses on whether a central core dominates network flow​​.
## Block-Modelling

    
Ideal structure: core nodes are interconnected and connected to some periphery nodes, but periphery nodes are not interconnected.


![](../images/df89551d0f152877a51ae80988b5798a.png)


1. **Core-Periphery Structure**:

    - Origin: Emerged in economics in the 1970s, later adopted in social sciences.
    - Concept: Dense, interconnected core and a scattered periphery, similar to urban areas.
2. **Corporate Analysis**:

    - Focus: Analyzing interconnectedness within corporate structures.
    - Centrality Indicator: Used to understand network structure in corporations.


## k-Core Decomposition

k-Core decomposition involves organizing the network into layers or shells based on node connectivity​​.


The k-core is the (maximal) subgraph $S$ whose nodes have (internal) $d e g_s \geq \mathrm{k}$. The $k$-shell is the set of nodes belonging to the $k$-core but not to the $(k+1)$-core. Thus the network is organized into "concentric" layers, the k-shells. The union of all k'-shells with $k^{\prime} \geq k$ is the $\mathrm{k}$-core. 


3. **K-Core and K-Shell Decomposition**:
    - K-core: Subgraph where each node has at least 'k' connections within the subgraph.
    - K-shell: Nodes in k-core but not in (k-1)-core, useful for identifying network hierarchy levels.
4. **Centrality and Peripheral Indicators**:
    - Centrality: Measures node importance or influence within a network.
    - Peripheral Indicators: Identify less connected, edge-positioned nodes.

## Core-Periphery Profile

Heuristic procedure to order nodes from periphery to core based on strength and **persistence probabilities**.

Core-Periphery score measures the area between the network's core-periphery profile and a complete network profile​​.


What is a persistence probability? 

5. **Probability of Persistence**:
    - Definition: Likelihood of a random walker staying at the same node over time.
    - Application: Identifies network periphery by finding nodes with low interconnectivity.



A heuristic procedure for **ordering** the nodes from to the periphery to the core:

"ordinare in some way i nodi from dai piú periferici a quelli piú centrali" 

Genera una sequenza di insiemi a via via generati con la minima probabilitá di persistenza possibile.

- start by the node $i$ with minimal strength
- generate a sequence of sets $\{i\}=S_1 \subset S_2 \subset \cdots \subset S_N=$ $\{1,2, \ldots, N\}$ by adding, at each step, the node attaining the minimal persistence probability $\alpha_1, \alpha_2, \ldots, \alpha_N$.

The sequence $0=\alpha_1 \leq \alpha_2 \leq \cdots \leq \alpha_N=1$ is the Core-Periphery profile (and $\alpha_k$ is the coreness of the node inserted at step $k$ ).

The Core-Periphery score $C$ is the $([0,1]$-normalized) area between the Core-Periphery profile and the profile of the complete network.

Core - periphery profile avrá curve differenti. 

La curva a seconda di come cresce mostra qual'é la quantitá di nodi effettivamente in periferia.




The heuristic procedure is aimed at ordering the nodes from the most peripheral to the most central. 

This process begins with the node $i$ which has the minimum strength, and progressively creates a series of sets $\{i\}=S_1 \subset S_2 \subset \cdots \subset S_N=$ $\{1,2, \ldots, N\}$. These sets are produced by, at every step, incorporating the node that holds the minimum persistence probability $\alpha_1, \alpha_2, \ldots, \alpha_N$.

The resultant string $0=\alpha_1 \leq \alpha_2 \leq \cdots \leq \alpha_N=1$ defines the Core-Periphery profile, where $\alpha_k$ represents the coreness of the node included at step $k$.

The Core-Periphery score $C$ is calculated as the area (normalized to $[0,1]$) between the Core-Periphery profile and the profile of the entire network.

The shape of the Core-Periphery profile curve varies, and its growth rate signifies the quantity of nodes situated in the periphery.





![](../images/0a6592a816062d80b5de7d4cba6ac2d8.png)














1. **Network Fragility and Centralization**:
    - Centralization: Indicates potential network fragility due to dependence on central nodes.
    - Network Resilience: More connections can offer resilience but also increase risk of widespread impact.
2. **Complex Products and Network Structure**:
    - Correlation: Complex products often have more centralized distribution networks.
    - Implication: Centralization in product distribution networks indicates potential vulnerability.


The focus of this study revolves around the relationship between product complexity and the centralization of trade networks. Specifically, we delve into how complex, or high-tech, products are distributed via centralized, and consequently more vulnerable, networks. 

In context, centralized networks, indicated by an intense dependency on central nodes, are inherently more fragile. This fragility extends to circumstances where a key player is unable to operate, such as in 2010 when Riken - a company that dominated 50% of the piston ring market - was affected by the volcanic eruption in Iceland. This example illustrates severe fragility. 

However, the resilience of a network increases with more connections, presenting a double-edged sword as this expansion also raises the potential for widespread consequences. 

A trend has been observed between complex products and their corresponding distribution network structures. More often than not, complex products are routed through highly centralized distribution networks. This indicates that these networks could be inherently vulnerable due to the product's complexity. Statistically, more complex products align more with starlike, fragile network structures. 

In our research we've assigned a numerical index to each product, identifying its technological complexity. Each product's distribution network is captured akin to a photograph with the index serving as the defining characteristic. All in all, the complexity of a product is critical in determining its fragility in relation to the network through which it is distributed.


--- 





The results confirm the conjecture on the positive correlation between complexity of products and centralization of their trade networks.

Centralization implies fragility: The more complex are the traded goods, the more fragile are their trade networks.

Given the relevant role played by complex goods in world trade, the global trade network appears to be uncomfortably vulnerable.



In network distribution studies, a significant relationship exists between the complexity of products and their distribution network structures. Products with higher complexity tend to be distributed via centralized, starlike network structures which, due to their design, are notably fragile.

Each product in our research is assigned a numerical index denoting its technological complexity. This index serves as the defining attribute of a product's distribution network. The central argument here is that the complexity of a product is intrinsically linked to the susceptibility of its distribution network.

We have corroborated our primary argument that a positive correlation exists between a product's complexity and the centralization of its respective distribution network. Yet, this centralization can make these networks overly susceptible to disruptions - a commonly identified flaw by network analysts.

Given the critical role of complex goods in global trade, the global trade network is uncomfortably vulnerable. I highly recommend perusing the associated articles for more insights. We'll continue discussing this in our following class.