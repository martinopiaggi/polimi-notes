# Link prediction

Networks are crucial in modelling interactions observed in experiments or data processing. However, it's essential to consider that many interactions might be overlooked and some observed ones might be artificial: which means that the observed network $A^O$ is a "noisy" observation of the actual network $A^{\text{true}}$.

Reconstruct $A^{\text{true}}$ given $A^O$ has multiple applications:

- Finding missing links, which can suggest ad-hoc experiments.
- Deleting spurious links.
- Predicting future links in time-varying networks, such as forecasting future friendships.

Different way to do quantifying structural similarity of nodes, examples:

- **Common Neighbours**: check for each pair of nodes the number of common neighbours and then sort them based on this "score". The nodes with higher common neighbours not linked probably should have a link.
- **Preferential Attachment Index** to sort the nodes $s_{ij} = k_i k_j$. The nodes with highest $s_ij$ should be connected.


## Recommender Systems


A similar formulation of the link prediction problem is to consider a set of users $U = \{u_1, u_2, \ldots, u_m\}$ and a set of objects $O = \{o_1, o_2, \ldots, o_n\}$.

The **Bipartite Network Representation** represent the user-object relationships $B = [b_{ij}]$ where:

$$b_{ij} = 1 \text{ if } u_i \text{ owns } o_j, \quad b_{ij} = 0 \text{ otherwise }$$

We can then define:

- **User Degree** $k(u_i) = \sum_j b_{ij}$ as Number of objects owned by user $i$ 
- Object Degree $k(o_j) = \sum_i b_{ij}$ as the number of users owning object $j$.
  

Assuming users like the objects they own, predict and recommend additional objects they might like.

- **Global Ranking Method**: Recommend objects with the largest degree $k(o_j)$, implying the most commonly owned objects. This approach lacks personalization.
- **Content-Based Filtering**: Define object similarity and recommend to user $u_i$ objects most similar to those they already own. This method offers personal recommendations without utilizing network structure.

# Robustness 

The robustness or resiliency of a network refers to the network's ability to maintain acceptable levels of service despite network faults.
Network faults are generally modelled as either:

- **Failures**: Random removal of nodes/links.
- **Attacks**: Targeted removal of crucial nodes/links (most central or most loaded).

![](images/14c2dba6b57e8f6238473ab9b9c959f7.png)

We can measure the impact of removing a fraction $f$ of nodes or links by looking the **loss of efficiency** after removal of all links incident to node $i$:

$$I_i = \frac{\Delta E_i}{E} > 0$$

Different network structures, like the Barabasi-Albert and Erdös-Rényi models, have different responses when nodes or links are removed. 

- **Erdös-Rényi Networks Under Stress**: Homogeneous networks are robust.
- **Scale-Free Networks Under Stress**: while removing random nodes doesn't significantly impact connectivity, targeting high-degree nodes rapidly decays connectivity. So they are pretty **fragile** to **attacks**.

## Motter and Lai model of network breakdown

Understanding the propagation of breakdown phenomena can be interesting.

- Nodes exchange units of material along the shortest path.
- At $t=0$, node load is proportional to betweenness $b_i(0)$.
- Node capacity $C_i = (1 + \alpha) b_i(0)$, with $\alpha$ as the tolerance parameter.

The key part is that: 

- Failure/attack leads to changes in betweenness and thus their load
- The subsequent change of loads can create other node fails since $b_i>C_i$
- These other node failures change the betweennesses $b_i$: the cycle repeats 

At the end $G = S / N$ is used as a metric with $S$ as the size of the largest connected component of the network after a failure or attack has occurred.



