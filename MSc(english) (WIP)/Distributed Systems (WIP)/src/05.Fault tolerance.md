# Fault tolerance

Another  very  broad  and  very  typical  system problem is  fault  tolerance.  
A **fault** can cause an error which eventually can cause a failure. 
For example a fault can be to not check zero division which gives the error division by zero. 

Faults can be classified into:

- **Transient** faults which occur once and then disappear. 
- **Intermittent** faults appear and vanish without any apparent reason. 
- **Permanent** faults continue to exist until the failed components are repaired. 

There are different types of **failure** models:

- **Omission** failures which can occur in processes
- **Timing** failures, which only occur in synchronous systems, happen when one of the time limits defined for the system is violated (for example, bounds on request's latency).
- **Byzantine** failures can occur in processes, where intended processing steps may be omitted or additional steps may be added. The "key characteristic" of byzantine failures is that weird results are produced. 

**Fault tolerance** refers to the ability of a system or network to handle failures without causing a complete disruption. One of the main techniques used to achieve fault tolerance is redundancy. Redundancy can be implemented in different ways to mask failures:

- **Information** redundancy: instead of sending only the packet, we send the packet and also some additional information that may help in recovering a byzantine failure 
- **Time** redundancy: TCP if does not receive an ack, tries to re-send the message later
- **Physical** redundancy: be redundant in physical layer, so use multiple channels

## Protection against process failures

**Redundant process groups** are groups of processes which can collectively handle the work that should be done by an individual process. In this way, even if some processes fail, the remaining healthy processes can continue to work.
Using process groups can be challenging to manage the membership of distributed groups. Multicast join/leave announcements are necessary but the problem is that if a process crashes, it will not notify others, which can complicate maintaining the group's integrity: if multiple processes crash or leave, the groups structure may need to be rebuilt. 

In general, if processes fail silently, then $k + 1$ processes allow the system to be $k$-fault-tolerant .
In case of Byzantine failures matters become worse: $2k + 1$ processes are required to achieve k-fault tolerance (to have a working voting mechanism). Byzantine means that you need other servers (the majority) to check that the first one has produced a weird result. 

### FloodSet algorithm 

The FloodSet algorithm is used for fault tolerance in distributed systems. Each process begins with a variable called `W`, which is initialized with its own start value. 
In each round, each process sends its `W` to all other processes and adds the received sets to its own `W`. 
This process is repeated for `k + 1` rounds, where `k` is the maximum number of faults the system can tolerate (`k`-fault-tolerance): so for example to be 5-fault-tolerant, we need 6 steps to reach an agreement. 

Each process may stop in the middle of the send operation.

After `k + 1` rounds a decision is made based on the size of `W`:
- If the cardinality of `W` is 1, no problem at all 
- If the cardinality of `W` is greater than 1, it must be use a previously decided common function/criteria on all processes to make the decision on which value to take.


A more efficient implementation consists to broadcast `W` only if the process learns of a new value. 

### Lamport's algorithm intuition 

In case of byzantine failure, things get more complex: the processes can both stop or exhibit any arbitrary behavior like sending arbitrary message or performing arbitrary state transitions.
This problem has been described by Lamport in 1982 in terms of armies and generals.

During the initial round, all processes exchange their respective values. However among them there is a traitor which sends unusual values. Starting from the second round onwards, each process adopts the value that is most commonly shared among them.

Lamport (1982) showed that if there are $m$ traitors, $2 m+1$ loyal generals are needed for an agreement to be reached, for a total of $3m+1$ generals. 

Fischer, Lynch, and Paterson proved that no solution exists: "Impossibility of Distributed Consensus with One Faulty Process‚Äù (FLP Theorem). In general, for an **asynchronous** system, even a single failure is enough for not being able to reach a consensus.
Basically the FLP theorem says that every protocol that may come to your mind must be a **synchronous** protocol: so it must be a protocol that somehow fixes the bound in the sending of messages, fixes the bound in the processing speed of the chat-optic processes, and fixes the bound in the maximum delay jitter between the values cross.

## Reliable group communication 

In this lecture, we will discuss how to achieve reliable group communication. The goal is to enable members of a group to communicate with each other in a reliable manner.
Two approaches:

- ACKs, where each recipient sends an acknowledgement after receiving the message. If an acknowledgement is not received, the sender resends the message. However, this can lead to an "ack implosion" if all recipients send acknowledgements simultaneously.
- NACKs, where each recipient sends a nack after a random delay indicating which packet was missed. This prevents multiple nacks from being sent simultaneously and ensures scalability in large groups.


Hierarchical Feedback Control is a communication method where receivers are grouped together, with each group having a coordinator. These groups are then organized in a tree structure, with the sender serving as the root of the tree. The coordinator of each group has the flexibility to adopt any strategy within its group, and can request retransmissions from its parent coordinator if necessary. 
One challenge in implementing this control method is constructing and maintaining the hierarchy (or tree) structure.



#### The case of faulty processes

Things get harder if processes can fail or join and leave the groups during communication. What is usually needed is that a message is delivered either to all the members of a group or to none and that the order of messages is the same at all receivers. This is known as the atomic multicast problem. Consider an update in a replicated database: everything is fine if all the (non faulty) replicas receive the same commands in the same order. 


Virtual synchrony is a mechanism used to detect failures in a distributed system. In this mechanism, crashed processes are removed from the group and must rejoin later, reconciling their state. Messages sent by correct processes are processed by all correct processes, while messages sent by failing processes are either processed by all correct members or by none. Furthermore, only relevant messages are received in a specific order.

In this model, there is a distinction between receiving and delivering a message. Messages received by the communication layer are buffered and delivered later when a certain condition is met. A group view refers to the set of processes to which a message should be delivered, as seen by the sender at the time of sending. It is necessary to ensure that group view changes are delivered in a consistent order with respect to other multicasts and with respect to each other. This results in a form of reliable multicast known as virtual synchrony.

When a view change occurs due to a process joining or leaving the group, all multicasts must occur between view changes. View changes can be seen as multicast messages that announce changes in group membership. It is important to guarantee that messages are always delivered before or after a view change. If the view change is a result of the sender of a message leaving, the message is either delivered to all group members before the view change is announced or it is dropped.

Multicasts occur in epochs that are separated by group membership changes. When a process crashes, the message it sent is discarded and not delivered to anyone. If a process crashes but other processes receive its message before knowing about the crash (before the cut), this is also unacceptable. It must be ensured that messages are only received after becoming aware of the sender's crash.

#### Message ordering

Retaining the **virtual synchrony property**, we can identify different orderings for the multicast messages
- Unordered multicasts
- FIFO-ordered multicasts
- Causally-ordered multicasts
- 
In addition, the above orderings can be combined with a total ordering requirement

- Whatever ordering is chosen, messages must be delivered to every group member in the same order
- Virtual synchrony includes this property in its own definition


FIFO (First-In, First-Out) messaging can be either total or non-total. In the case of total FIFO, all messages maintain the same order. For example, if I send messages one and two in FIFO order, recipient E will receive them as one, two, and then E will also send messages one, three, and four. The resulting order for E would be one, two, three, four or one, three, two, four - both maintaining the FIFO order. However, this order does not necessarily guarantee total ordering, which would require all recipients to receive the packets in the same order across all sources.




Atomic multicast Atomic multicast is defined as a virtually synchronous reliable multicast offering totally-ordered delivery of messages. Total ordering means that all processes receive messages in the same order.


In atomic multicast, there is no ordering but it's total. This means that everyone receives the messages in the same order. The term "atomic" is used because the messaging happens instantaneously, so all messages are considered as a single order and we receive them in that order. There are various types of atomic multicast, such as atomic causal, reliable piper multicast, causally multicast, and reliable causal.



#### Checkpoints 


We are discussing fault tolerance and recovery techniques. There are two types of recovery: backward and forward recovery. Backward recovery involves going back to a previous state if a state resulting from a crash is undesirable. For example, in TCP, if a packet is sent but no acknowledgement is received, the sender goes back to the previous state where the packet was not sent and resends it. Forward recovery involves trying to correct errors without going back to a previous state.

To enable backward recovery, we need to save previous states that can be retrieved later. This can be done through checkpointing or logging. Checkpointing involves saving the state of each process periodically without synchronizing with other processes since synchronization is not feasible in distributed systems.

However, when recovering using checkpoints, we need to ensure that going back to different states at different times still makes sense for the application as a whole. We define this by aiming for consistent cuts or recovery lines where every process is in a state where all messages received were actually sent by other processes at that recovered state.

The challenge then becomes finding the best possible recovery line which consists of consistent checkpoints from each process representing an overall consistent cut.

Overall, independent checkpoints are taken by each process and when there's a crash, we aim for the most recent and reasonable recovery line consisting of consistent checkpoints from all processes.


##### Independent checkpointing

How  do  we  implement,  how  do  we  discover  when  a  checkpoint  is  valid?

To implement and discover valid checkpoints, we need to obtain an approximation that allows us to reconstruct the best set of checkpoints for a good recovery line. 




 a  consistent  recovery  line.  We  can  use  two  different  algorithms.  Those  are  not  distributed  algorithms.  Those  are  centralized  algorithms.


 We  reconstructed  on  a  centralized  node  this  graph.  The  distributed  part  of  the  protocol  was  the  part  that  I  described  here.  When  I  said  each  process  stores  the  dependencies  and  sends  the  dependency  to  the  centralized  node,

Two approaches:

- **Rollback-dependency graph**:  each  dependency  between  the  interval of checkpoints  is  translate it  into a dependency  between  the  ending  checkpoint  of  the  center  interval  and  the  ending  checkpoint  of  the  arrival.
- **Checkpoint-dependency graph**: the dependency is added from the starting state of the interval to the final state of the other interval (where the arrow arrives). Crashes eliminate arrival point of dependence. Then an hypothesis is made (a set of checkpoints is chosen): if there are no dependencies in the hypothesis a recovery line is found, otherwise the arrival checkpoint of the dependency which is in the hyp set is deleted and a new iteration of the algorithm is done.


Those  are  2  algorithms  that  should  bring  to  the  same  result. The  coordinator  can  rebuild  this  graph.  This  graph  is  costly,  but  it's  not  an  infinite  cost  because  it  does  not  depend  on  the  number  of  messages.  It  only  depends  on  the  dependency  between  intervals.
