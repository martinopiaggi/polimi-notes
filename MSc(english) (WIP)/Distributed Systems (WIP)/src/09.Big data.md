# Big data

Data science relies on the availability of large volumes of data, commonly known as "big data." Big data comes from various sources such as recommender algorithms, genomic data, and medicine. For example, Netflix's recommendation algorithm uses historical data, while Google Translate uses snippets of books in different languages.

Companies like Google and Facebook use data as a business model. They offer free services to users but collect and sell their data to advertisers. Collecting more data allows for more statistically relevant insights. It is often more cost-effective to keep all the data rather than deciding what to keep selectively.

Big data has four characteristics, known as the "4 V's." These are volume, velocity, variety, and veracity. Volume refers to the large amount of data, velocity to the speed at which it arrives, variety to the different versions, formats, and sources, and veracity to the varying levels of accuracy and correctness of the data.

To effectively handle big data, certain functions are required. These functions include automatic parallelization and distribution of data processing, fault-tolerance, status and monitoring tools, and a clean abstraction for programmers. These functions enable scalability to accommodate large data volumes and support for rapidly changing data. The goal is to process and extract valuable knowledge from data as quickly as possible.



## Map Reduce 

The MapReduce programming model, introduced by Google in 2004, allows application programs to be written using high-level operations on immutable data. The runtime system handles scheduling, load balancing, communication, and fault tolerance. 

The computation in MapReduce consists of two phases: 

- The Map phase processes individual elements and outputs one or more 〈key, value〉 pairs.
- The Reduce phase processes all the values with the same key and outputs a value.

The map function is stateless and its output depends only on the specific word received as input. Tuples with the same key are guaranteed to be received by the same receiver. 

Reducers receive an immutable list (iterator) of values associated with each key. This ensures that the data is read sequentially from the disk only once. 

The "reduce" function iterates over the list and outputs one or more values for each key. In this case, it outputs the sum of the elements in the list.



Designing an algorithm that updates data only once and avoids excessive state storage can result in improved performance in the given context.


Conclusions
Typical MapReduce application:
- Sequence of steps, each requiring map \& reduce
- Series of data transformations
- Iterating until reach convergence (e.g., Google Page Rank)

Strenghts
- The developers write simple functions
- The system manages complexities of allocation, synchronization, communication, fault tolerance, stragglers, ...
- Very general
- Good for large-scale data analysis

Limitations
- High overhead
- Lower raw performance than HPC
- Very fixed paradigm: each MapReduce step must complete before the next one can start


### Support functionalities

The platform, as said before, provide us with many functionalities:

- **Scheduling**: allocates resources for mappers there is a master and multiple workers (slaves). The input data is divided into map tasks, typically 64MB in size. The reduce phase consists of reduce tasks, which are determined based on hashing the keys. Tasks are dynamically assigned to workers. The master assigns each map task to an available worker, taking into account the data's locality to the worker. The worker then reads the input for its assigned task, usually from its local disk. After processing, each worker generates R local files containing intermediate key-value pairs. Next, the master assigns each reduce task to an available worker. The worker reads the intermediate key-value pairs from the map workers and applies sorting and the user's reduce operation to produce the final output.
- **Data distribution**: moves data from mappers to reducers. The main objective of data locality is to reduce network bandwidth usage. In the Google File System (GFS), data files are split into 64MB blocks and stored in three different machines. The Master program in GFS schedules map() tasks by considering the location of these replicas. It ensures that the map() tasks are executed on the same machine or at least on the same rack/network switch as one of the input replicas. This allows for efficient reading of input data at local disk speed on thousands of machines. Without data locality, the read rate would be limited by the rack switches.
- **Fault tolerance**: transparently handlers the crash of one or more nodes. In the context of worker failure, the master is responsible for detecting failures by monitoring the periodic heartbeats. When a worker fails, both completed and in-progress map tasks that were being executed by that worker need to be re-executed. The output of these map tasks is stored on the local disk. However, only in-progress reduce tasks on the failed worker should be re-executed, as the output of these tasks is stored in the global file system. In the event of a master failure, the state of the system is check-pointed to the Google File System (GFS). This allows for seamless recovery and continuation of operations by a new master.


## Big data processing platforms

Over the last decade, the MapReduce framework has undergone several advancements. One notable improvement is the ability to handle arbitrary acyclic graphs of transformations. In addition, there has been a shift from batch processing to stream processing. Another significant change is the transition from disk-based processing to utilizing main-memory or hybrid approaches.

Highlights: 

- arbitrary number of stages join filter groupBy oltre a map and reduce with possibility of caching intermediate results if reused multiple times. 
- **Apache Spark** adopts a scheduling approach which optimize **throughput**, reduce overhead and eventually compress data. The scheduling approach of Spark simplifies **load balancing**.
- **Apache Flink** eliminates scheduling and all the operators are instantiated as soon as the job is submitted in a pipelined approach. This makes Flink ideal for stream processing because its **lower latency** approach. Load balancing is basically impossible since everything is statically decided when the job is deployed and there isn't scheduling. 
- **Elasticity**  indicates the possibility of a system to dynamically adapt resource usage to the load of the system. It is a common feature in big data processing platforms since often are offered as a service. Scheduling approaches (Spark) are better for elasticity because scheduling decisions take place dynamically at runtime while elasticity in pipelined approaches is not possible.
- **Fault tolerance** in Big Data processing platforms is achieved in 2 ways: 
	- if the processing is scheduled: Re-scheduling and re-compute if a node fails.
	- if the processing is pipelined: checkpointing to a distributed file system is the way. 