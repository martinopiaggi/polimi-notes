# Peer-to-peer


> "Take advantages of resources at the edges of the network"

Alternative paradigm that promotes the sharing of resources and services through direct exchange between peers. 

### Napster 

Napster was the first P2P file sharing application that allowed users to download and share music for free over the Internet. Napster used small computers on the edges to contribute although they relied on the **centralized server** for lookup purposes (which some argued made it not a pure P2P system).

The main operations in Napster were:
- joining
- publishing
- searching
- fetching.

Napster's simplicity and `O(1)` search were advantageous. However, the **centralized** server for searching was a drawback since it was a single point of failure and a single point of control.

 Of course, that's also so. Every time you have a central location, it's good on one side because protocol you know that the amount of messages that you have to send is only one.

 On the other side, it's a single point of failure and a single point of control. So if someone takes down that service, you lose basically the entire functioning of your protocol.


### Gnutella

Gnutella has the advantage of being a fully decentralized network, eliminating the need for central coordination. This means that search costs are distributed among the network using a **flooding** algorithm. 
When a node joins the Gnutella network, it connects to a known "anchor" node and sends a `PING` message to discover other nodes. These nodes respond with `PONG` messages, providing new connections to the joining node. 
The topology of the Gnutella network is **random** and constantly changing based on flooded queries. To join the network, a node needs to know the address of at least one anchor node.
Peer nodes are used for both resource searching and recursively routing and flooding queries.
To prevent congestion and endless queries, each query packet has a `HopToLive` field that decreases with each hop. Once this field reaches zero, the query stops.

### KaZaA

KaZaA was created in 2001 from a Dutch company called Kazaa BV. 

- Pros:
	- Tries to consider node heterogeneity
	- Bandwidth
	- Host computational resources
	- Host availability
	- Kazaa rumored to consider network locality
- Cons:
	- Still no real guarantees on search scope or search time
	- proprietary, not open source

### BitTorrent

BitTorrent is a file sharing network that limits the participation of free riders. It allows multiple people to download the same file without slowing down everyone else's download by having downloaders swap portions of the file with each other. BitTorrent focuses on efficient fetching, not searching.

To join BitTorrent, you need to know the address of a tracker server, which can be found outside of BitTorrent. A torrent is a meta-data file describing the file to be shared, including its name, size, checksum, and tracker and peer addresses. Seeds are peers that have the complete file and offer it for upload, while leeches are peers with incomplete downloads. The swarm is the collection of seeders and leeches, and the tracker is the server that keeps track of them.

BitTorrent breaks the file down into smaller fragments and peers download missing fragments from each other and upload to those who don't have them. Peers download fragments in a non-sequential order and assemble them on their machines. They typically use a "rarest-first" approach to request the fragment held by the fewest peers, spreading the load. Peers start uploading what they have before their download is finished. Once a peer finishes downloading the entire file, they should keep uploading and become an additional seed.

Choking is a temporary refusal to upload and is evaluated every 10 seconds. Peers un-choke a fixed number of peers (usually 4) based on download rate. The more a peer downloads from another, the higher the chances are that they will upload to that peer. BitTorrent also has an "optimistic un-choke" that rotates every 30 seconds to discover better peers and connections.

Approximate Pareto Efficiency: if two peers get poor download rates for the uploads they are providing, they can start uploading to each other and get better download rates than before.
Pareto effiency is actual a relatively weak condition. 

Also central tracker server needed to **search** . The protocol is mainly used to only share. 



### Freenet - Secure Storage 

Freenet is a peer-to-peer application that allows for anonymous publishing and reading of information on the internet. Its goals are to enable one-to-many publishing, provide anonymity for users, have no centralized control, be scalable, and be robust against failures and attacks.

- Join: clients contact a few other nodes they know about; get a unique node id
- Publish: route file contents toward the node which stores other files whose id is closest to file id
- Search: route query for file id using a steepest-ascent hill-climbing search with backtracking
- Fetch: when query reaches a node containing file id, it returns the file to the sender

The routing protocol in Freenet involves forwarding requests to the "best guess" neighbor unless the information is already known locally. Requested information is passed back to the original requester through a chain of nodes, with intermediate nodes caching the information.
Only nodes along the search path are involved in the search, **no flooding**. 
Routing properties in Freenet ensure that similar file ids are stored on the same node, creating a "small world" network. Caching properties in the network prioritize popular information and delete unpopular files when disk space runs out.

Pros of Freenet include intelligent routing, small search scope, and anonymity.

Freenet provides anonymity by forwarding messages and randomly modifying the source of the packet as it traverses the network. Security is maintained through robust hashing of file ids and private key signatures for updating files.

Cryptography in Freenet includes link-level encryption, document encryption, and document verification to prevent snooping, traffic analysis, and unauthorized access.

When adding nodes to the network, their public keys and addresses are announced to existing nodes, and a random GUID is assigned collectively by nodes in the chain.

 Cons include the lack of provable guarantees and difficulties in measuring and debugging due to anonymity features.


### Final comparison 


Compare the following approaches in terms of search expressivity and performance

- Centralized search
- Query flooding
- Hierarchical query flooding
- Structured topology

There are several approaches to search in terms of search expressivity and performance. 

Centralized search is optimal in terms of the number of messages sent through the network. 

Hierarchical query flooding is a middle ground approach. It offers predictable performance and is programmed. 

Structured topology provides predictable performance and limits search capabilities to what is published. It can be used in centralized search or hierarchical query flooding. 

In terms of network fragility, a centralized node is fragile and can be taken down. 
Query flooding is more robust if good connections and network balance are maintained. 

Hierarchical approach is even better as it can make assumptions based on the history of superpeers. 

Topology is fragile, but there are algorithms that strive to keep the network stable.

O