
# Model prediction 


Steps:

0) analysis of the system
1) evaluation of the canonical representation of the system 
2) computation of the predictor 
3) evaluate the prediction error


The variance of the prediction is the variance of the process if it is optimal.

## ARMA prediction


Note that the variance of the error increases with r, meaning that more distant prediction result to be less precise. Practically ˆv(t + r|t) = Wˆ r(z)η(t), where Wˆ r(z) is the result of the r-step LONG DIVISION of the numerator and denominator of W(z) in canonical form.

![](images/Pasted%20image%2020240320184427.png)


### K-steps predictor


Using the Long Division Algorithm , given any transfer function, we can always write it as above, with the coefficients found with the long division algorithm (that now we will call with the generic notation wi). Let’s analyze the meaning of these coefficients

Diophantine equation

We can solve this using the "Diophantine equation" which basically the formula obtained by a long division from the $W(z)$  : 

![](images/Pasted%20image%2020240320185208.png)

$$W(z)=\frac{C(z)}{A(z)}=\text{long division}=E(z)+\frac{z^{-u}F(z)}{A(z)}$$

where $E(z)$ is the result of the long division, while $F(z)$ is the rest. 



computation of the prediction error



Remember that the predictior $\hat y(t+k,s)$ is a stochastic process, stationary (because $C(z)$ the denominator still has roots inside the unit circle) .

Asymptotically the variance of the prediction error tends to the variance of $y(t)$ which is the best we can!


computation of the variance of the prediction error 





### 1-step predictor

Generic **one step** predictor from noise: 

$$\hat{y}(t|t-1)=\frac{C(z)-A(z)}{A(z)}e(t)$$

from data: 

$$\hat{y}(t|t-1)=\frac{C(z)-A(z)}{C(z)}y(t)$$

We would like to minimize the mean squared prediction error:

$$E[y(t+u)-\hat{y}(t+u|t]^{2}]$$

Optimal predictor from the noise:

$$\hat{y}(t+u|t)=\sum_{i=0}^{\infty}w_{u+i}e(t-i).$$


The problem here is that the prediction depends by an infinite series of past samples $e(t)$ with zero mean. 



The prediction error for one step predictor is always:

$$\epsilon(t|t-1)=\frac{A_m(z)}{C_m(z)}\cdot y(t)$$

since:

$$\epsilon(t|t-1)=y(t)-\hat{y}(t|t-s)=y(t)-\frac{C_m-A_m}{C_m}y(t)=\left(\frac{C_m-C_m+A_m}{C_m}\right)y(t)$$





### ARMA with not null mean

$$\begin{aligned}y(t+k)&=\hat{y}(t+k)+my\\\hat{y}(t+k|t)&=\hat{y}(t+k|t)+my\end{aligned}$$



Remember [02.MA, AR and ARMA processes](projects/polimi-notes/MSc(english)%20(WIP)/Model%20Identification%20and%20Data%20Analysis%20(WIP)/src/02.MA,%20AR%20and%20ARMA%20processes.md#Processes%20with%20non-null%20term) 

1. Find mean with gain theorem:
   $$ E[y(t)] = W(1) \cdot \mu = \bar{y} $$

2. Define de-biased processes:
   $$
   \begin{align*}
   \tilde{y}(t) &= y(t) - \bar{y} \\
   \tilde{e}(t) &= e(t) - \mu
   \end{align*}
   $$

3. Redefine the process using unbiased elements:
   $$ \tilde{y}(t+k|t) = \frac{C(z)}{A(z)} \cdot \tilde{e}(t)$$

4. Prediction adjustment for non-zero-mean:
   $$ \hat{y}(t+k|t) = \tilde{y}(t+k|t) + \mu_y $$
   
   Where the adjusted prediction is given by:
   $$ \hat{y}(t+k|t) = \frac{F(z)}{C(z)} \cdot y(t) + \left(1 - \frac{F(1)}{C(1)}\right) \cdot \mu_y $$







$$\begin{aligned}\hat{y}(t+k|t)&=\frac{F(z)}{C(z)}\tilde{y}(t)+my=\frac{F(z)}{C(z)}(y(t)-my)+my\\&=\frac{F(z)}{C(z)}y(t)-\frac{F(z)}{C(1)}my+my\end{aligned}$$


and the bias term due to $\mu$ : 

$$\hat y (t+k|t)=\frac{F(z)}{C(z)}y(t)+\left(1-\frac{F(1)}{C(1)}\right)my$$


Since $E(z)$ is unpredictable, we will only consider the second part and we can say that:

$$\hat{y}(t+u|t,s)=\frac{F(z)}{C(z)}y(t,s)$$

and in case of no zero mean arma $y(t)=\frac{C(z)}{A(z)}e(t)$ with $e(t)=wn(\mu\lambda ^2)$  it's possible to compute the unbiased $\tilde y(t)$ (removing the mean $\bar y$ and $\mu$ from respectively $y(t)$ and $u(t)$), use the Diophantine equation over the $\tilde y (t)$ and then re add the mean. 
Actually this is not optimal and this formula exists:

$$\hat{y}(t+u|t)=\frac{F(z)}{C(z)}y(t)+(1-\frac{F(1)}{C(1)})\overline{y}$$

where $\overline y$ is the mean of $y(t)$.



## ARX and ARMAX


Remember that the generic ARMAX process looks like this: 

$$y(t)=\frac{C(z)}{A(z)}e(t)+\frac{B(z)}{A(z)}u(t-1)$$

we only need predictor for sthocastic part because the derministic one is trivially computable. 


$$y(t+k)=\frac{C(z)}{A(z)}e(t+k)=E(z)+z^{-k}\frac{F(z)}{A(z)}e(t+k)$$

$$E(z)\cdot e(t+k)+\frac{F(z)}{A(z)}z^{-k}e(t+k)$$

The first part will never be computable at time $t$ while the second will be since $z^{-k}e(t+k)$ it's obviously $e(t)$ . 



$$
\begin{aligned}
\hat{y}(t+k|t) &= \frac{B(z)}{A(z)}u(t+k-d) + \frac{F(z)}{C(z)}z(t) \\
&= \frac{B(z)}{A(z)}u(t+k-d) + \frac{F(z)}{C(z)}\left(y(t) - \frac{B(z)}{A(z)}u(t-d)\right) \\
&= \frac{B(z)}{C(z)}\left[\frac{C(z)}{A(z)} - \frac{F(z)}{A(z)}z^{-k}\right]u(t+k-d) + \frac{F(z)}{C(z)} \cdot y(t)
\end{aligned}
$$


We make last passage keeping in consideration the diophantine equation:

$$\frac{C(z)}{A(z)} = E(z) + \frac{F(z)}{A(z)}z^{-k}$$


In this way we can replace $\left[\frac{C(z)}{A(z)} - \frac{F(z)}{A(z)}z^{-k}\right]$ with $E(z)$ to obtain:

$$\hat{y}(t+k|t)=\frac{B(z)E(z)}{C(z)}\cdot u(t+k-d)+\frac{F(z)}{C(z)}y(t)$$


Why all of this? So to have a "simple" elegant equation with depends only on past data of $u(t)$ and $y(t)$ . 




