
# Model prediction 

The variance of the prediction is the variance of the process if it is optimal.



The prediction error for one step predictor is always:

$$\epsilon(t|t-1)=\frac{A_m(z)}{C_m(z)}\cdot y(t)$$

since:

$$\epsilon(t|t-1)=y(t)-\hat{y}(t|t-s)=y(t)-\frac{C_m-A_m}{C_m}y(t)=\left(\frac{C_m-C_m+A_m}{C_m}\right)y(t)$$


## Computation of step predictor


Note that the variance of the error increases with r, meaning that more distant prediction result to be less precise. Practically ˆv(t + r|t) = Wˆ r(z)η(t), where Wˆ r(z) is the result of the r-step LONG DIVISION of the numerator and denominator of W(z) in canonical form.

![](images/Pasted%20image%2020240320184427.png)

Steps:

0) analysis of the system
1) evaluation of the canonical rapresentation of the system 
2) computation of the predictor 
3) evaluate the prediction error

Generic **one step** predictor from noise: 

$$\hat{y}(t|t-1)=\frac{C(z)-A(z)}{A(z)}e(t)$$

from data: 

$$\hat{y}(t|t-1)=\frac{C(z)-A(z)}{C(z)}y(t)$$

We would like to minimize the mean squared prediction error:

$$E[y(t+u)-\hat{y}(t+u|t]^{2}]$$

Optimal predictor from the noise:

$$\hat{y}(t+u|t)=\sum_{i=0}^{\infty}w_{u+i}e(t-i).$$


The problem here is that the prediction depends by an infinite series of past samples $e(t)$ with zero mean. 


computation of the prediction error

computation of the variance of the prediction error 




### K-steps predictor




Diophantine equation

We can solve this using the "Diophantine equation" which basically the formula obtained by a long division from the $W(z)$  : 

![](images/Pasted%20image%2020240320185208.png)

$$W(z)=\frac{C(z)}{A(z)}=\text{long division}=E(z)+\frac{z^{-u}F(z)}{A(z)}$$

where $E(z)$ is the result of the long division, while $F(z)$ is the rest. 

Since $E(z)$ is unpredictable, we will only consider the second part and we can say that:

$$\hat{y}(t+u|t,s)=\frac{F(z)}{C(z)}y(t,s)$$

and in case of no zero mean arma $y(t)=\frac{C(z)}{A(z)}e(t)$ with $e(t)=wn(\mu\lambda ^2)$  it's possible to compute the unbiased $\tilde y(t)$ (removing the mean $\bar y$ and $\mu$ from respectively $y(t)$ and $u(t)$), use the Diophantine equation over the $\tilde y (t)$ and then re add the mean. 
Actually this is not optimal and this formula exists:

$$\hat{y}(t+u|t)=\frac{F(z)}{C(z)}y(t)+(1-\frac{F(1)}{C(1)})\overline{y}$$

where $\overline y$ is the mean of $y(t)$.


## Prediction for ARMAX

And in case of **ARMAX**

Remember that the generic armax process looks like this: 

$$y(t)=\frac{C(z)}{A(z)}e(t)+\frac{B(z)}{A(z)}u(t-1)$$

The prediction of both stochastic and deterministic (exogenous) part: 

$$\hat{y}(t+u|t)=\frac{F(z)}{C(z)}y(t)+\frac{B(z)E(z)}{C(z)}\mu(t-d)$$




we only need predictor for sthocastic part because the derministic one is trivially computable. 


$$\boxed{\hat{y}(t+k\mid t)=\frac{B(z)E(z)}{C(z)}u(t+k-d)+\frac{F(z)}{C(z)}y(t)}$$






$$y(t+k)=\frac{C(z)}{A(z)}e(t+k)=E(z)+z^{-k}\frac{F(z)}{A(z)}e(t+k)$$

$$E(z)\cdot e(t+k)+\frac{F(z)}{A(z)}z^{-k}e(t+k)$$

The first part will neve be computable at time $t$ while the second will be since $z^{-k}e(t+k)$ it's obviously $e(t)$ . 



Remember that the predictior $\hat y(t+k,s)$ is a stochastic process, stationary (because $C(z)$ the denominator still has roots inside the unit circle) .

Asymptotically the variance of the prediction error tends to the variance of $y(t)$ which is the best we can!

$$\mathrm{var}\left[\mathrm{e}(t)\right]\leq\mathrm{var}\left[\mathrm{e}(t+\mathrm{r}(t)\right]<\mathrm{var}\left[\mathrm{y}(t)\right]$$



Optimal predictors

ARMA with not null mean

$$\begin{aligned}y(t+k)&=\hat{y}(t+k)+my\\\hat{y}(t+k|t)&=\hat{y}(t+k|t)+my\end{aligned}$$



$$\begin{aligned}\hat{y}(t+k|t)&=\frac{F(z)}{C(z)}\tilde{y}(t)+my=\frac{F(z)}{C(z)}(y(t)-my)+my\\&=\frac{F(z)}{C(z)}y(t)-\frac{F(z)}{C(1)}my+my\end{aligned}$$


and the bias term due to $\mu$ : 

$$\hat y (t+k|t)=\frac{F(z)}{C(z)}y(t)+\left(1-\frac{F(1)}{C(1)}\right)my$$



Armax predictor 

$$
\begin{aligned}
\hat{y}(t+k|t) &= \frac{B(z)}{A(z)}u(t+k-d) + \frac{F(z)}{C(z)}z(t) \\
&= \frac{B(z)}{A(z)}u(t+k-d) + \frac{F(z)}{C(z)}\left(y(t) - \frac{B(z)}{A(z)}u(t-d)\right) \\
&= \frac{B(z)}{C(z)}\left[\frac{C(z)}{A(z)} - \frac{F(z)}{A(z)}z^{-k}\right]u(t+k-d) + \frac{F(z)}{C(z)} \cdot y(t)
\end{aligned}
$$


We make last passage keeping in consideration the diophantine equation:

$$\frac{C(z)}{A(z)} = E(z) + \frac{F(z)}{A(z)}z^{-k}$$


In this way we can replace $\left[\frac{C(z)}{A(z)} - \frac{F(z)}{A(z)}z^{-k}\right]$ with $E(z)$ to obtain:

$$\hat{y}(t+k|t)=\frac{B(z)E(z)}{C(z)}\cdot u(t+k-d)+\frac{F(z)}{C(z)}y(t)$$


Why all of this? So to have a "simple" elegant equation with depends only on past data of $u(t)$ and $y(t)$ . 

