
We would like to minimize the mean squared prediction error:

$$E[y(t+u)-\hat{y}(t+u|t]^{2}]$$

Optimal predictor from the noise:

$$\hat{y}(t+u|t)=\sum_{i=0}^{\infty}w_{u+i}e(t-i).$$


The problem here is that the prediction depends by an infinite series of past samples $e(t)$ with zero mean. 

Diophantine equation

We can solve this using the "Diophantine equation" which basically the formula obtained by a long division from the $W(z)$  : 


$$W(z)=\frac{C(z)}{A(z)}=\text{long division}=E(z)+\frac{z^{-u}F(z)}{A(z)}$$

where $E(z)$ is the result of the long division, while $F(z)$ is the rest. 

Since $E(z)$ is unpredictable, we will only consider the second part and we can say that:

$$\hat{y}(t+u|t,s)=\frac{F(z)}{C(z)}y(t,s)$$

and in case of no zero mean arma $y(t)=\frac{C(z)}{A(z)}e(t)$ with $e(t)=wn(\mu\lambda ^2)$  it's possible to compute the unbiased $\tilde y(t)$ (removing the mean $\bar y$ and $\mu$ from respectively $y(t)$ and $u(t)$), use the Diophantine equation over the $\tilde y (t)$ and then re add the mean. 
Actually this is not optimal and this formula exists:

$$\hat{y}(t+u|t)=\frac{F(z)}{C(z)}y(t)+(1-\frac{F(1)}{C(1)})\overline{y}$$

where $\overline y$ is the mean of $y(t)$.


And in case of **ARMAX** ? 
The prediction of both stochastic and deterministic (exogenous) part: 

$$\hat{y}(t+u|t)=\frac{|B(z)E(z)|}{C(z)}\mu(t+u-d)+\frac{F(z)}{C(z)}\mu(t)$$