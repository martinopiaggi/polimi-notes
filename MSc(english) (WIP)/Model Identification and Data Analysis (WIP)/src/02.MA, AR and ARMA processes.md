![](images/Pasted%20image%2020240408110853.png)

![](images/Pasted%20image%2020240408111428.png)





# MA, AR and ARMA processes

We will see these families of $SSP$:

- **MA (Moving Average):** Use past errors to predict future values.
- **AR (Autoregressive):** Use past values to predict future values.
- **ARMA (Autoregressive Moving Average):** Combine both AR and MA elements

## Moving averages

With $e(t)\sim wn(0,\lambda^2)$ we can define the moving average process of order $n$ $MA(n)$ in this way: 

$$y(t)=c_0\cdot e(t)+c_1e(t-1)+c_2\cdot e(t-2)+\cdots+c_ne(t-n)$$

$MA(n)$ is still stationary . A generalization of $MA(n)$ is:

$$MA(\infty)=\sum_{i=0}^{+\infty}c_ie(t-i), \space e(t)\sim\mathrm{wn}(0,\lambda^{2})$$

but the series must converge! 
The properties to remember here are:

$$m_y(t)=m_y(0)=0$$

$$\gamma_{_y}(\tau)=\begin{cases}\left({c_0}^2+{c_1}^2+{c_2}^2+...+{c_n}^2\right)\cdot\lambda^2&\text{if }\tau=0\\\left({c_0}c_1+{c_1}c_2+...+{c_{n-1}}c_n\right)\cdot{\lambda^2}&\text{if }\tau=\pm1\\\left({c_0}c_2+{c_1}c_3+...+{c_{n-2}}c_n\right)\cdot{\lambda^2}&\text{if }\tau=\pm2\\\vdots\\\left({c_0}c_n\right)\cdot{\lambda^2}&\text{if }\tau= n\\0&\text{if }|\tau|> n\end{cases}$$


So for example, for $y(t)=e(t)+c\cdot e(t-1)$:

$$\gamma_y(0) = 1+ c^2$$

$$\gamma_y(1) = \gamma_y(-1) = c$$


An example of $MA(2)$ (always with $e(t) \sim (0,1)$ ) : 

$$y(t)=e(t)+\frac{1}{2}e(t-1)+\frac{1}{4}e(t-2)$$


### AR

Autoagressive models will allow us to have $\gamma(\tau) \ne 0$ for all $\tau$ using a finite set of coefficients. 


$$y(t)=a_1y(t-1)+a_{2}y(t-2)+\cdots+a_{m}y(t-m)+e(t)$$

with $e(t) \sim wn(0,\lambda ^2)$ . 
Where $n$ is the order of the process and $e(t)\sim WN(0,\lambda^2)$. 

Mean is computed applying $E[v(t)]$.

Covariance function with $n=1$ (case of $AR(1)){:}$

$$\begin{cases}\frac{1}{1-a^2}\lambda^2&\tau=0\\\ \gamma(\tau)=a\gamma(\tau-1)&|\tau|\geq0\end{cases}$$
well defined if $|a|<1$.

Yule Walker Equations, can be used to evaluate the covariance.


We can compute the $MA(\infty)$ equivalent to $AR(1)$ and we use the rules for $MA$ processes to compute $m_y$ and $\gamma _y (\tau)$ . 



We have then to impose the basic condition: $\sum_{i=0}^\infty c_i^2$ needs to be FINITE, because we need to have $|\gamma(\tau)|\leq\gamma(0)$. Under this condition, $MA(\infty)$ is well defined and a stationary process.



Yule Walker Equations, can be used to evaluate the covariance and analyze $AR(1)$

$$\gamma(\tau)=\begin{cases}a\gamma(\tau-1)&|\tau|\geq0\\\frac1{1-a^2}\lambda^2&\tau=0&\end{cases}$$



So, the first approach is to find the MA of infinite order equivalent to the AR(1) model and compute the mean and covariance function using MA process rules. 

$$W(z)=\frac{C(z)}{A(z)}$$

To find the MA infinite equivalent to an AR process, we can consider it as a geometric series that converges to this form and show the equivalence between the polynomials. By dividing the two polynomials, we obtain another **polynomial as the result**. If the division is exact, $y(t)$ can be written as $e(z)$ times $e$ of $t$, which is an $MA$ process. To obtain the $MA$ infinite, we need to divide the polynomials an infinite number of times, assuming perfect division. 

Therefore, **long division** is the key to finding the MA infinite equivalent to an AR process.

In this process, they divide the coefficients of the transfer function infinitely many times to obtain an equivalent process with decreasing powers of z in the numerator. The maximum power decreases with each division step. The result is a set of coefficients, which can be used to compute the mean and covariance function of the AR process directly, without relying on the infinite equivalent process. 

If a process is stationary regardless of the timestamp the expectation of its square is always equal to the covariance function in $0$.


First of all we analyze the terms $E[e(t)y(t-1)]$ and $E[e(t)y(t-2)]$; in particular, they are null! We will try to understand why.

$y(t-1)$ doesn't depend on $e(t)$ but only from $e(t-1), e(t-2), \ldots$ past of $e(t)$  that are not correlated with $e(t)$ at the same way

$y(t-2)$ depends from $e(t-2), e(t-3), e(t-4), \ldots$ past of $e(t)$ from $t-2$ influences $y(t-2)$ and so $e(t)$ is not correlated with $y(t-1)$ and $e(t)$ is not correlated with $y(t-2)$.

To show it better, we use the MA(\(\infty\)) representation of the AR process: \( y(t) = \frac{1}{2}y(t-1) - \frac{1}{4}y(t-2) + e(t) \), a recursive equation called the implicit representation of \( y(t) \). To derive the explicit representation of \( y(t) \), it is necessary to perform recursion.

The MA(\(\infty\)) representation of an AR process refers to representing an autoregressive (AR) process as an infinite moving average (MA) process.

The \( \psi_i \) coefficients can be derived recursively from the AR coefficients \( \phi_i \) using the Yule-Walker equations or through other equivalent methods like the Durbin-Levinson algorithm.

The intuition behind this representation is that an AR process relies on its past values which, in turn, also depend on their own past values, and so on. If you keep substituting back these dependencies, you end up with a representation that expresses the current value of the process as the sum of the effects of all past and current shock terms \( \varepsilon_t \), each weighted by a \( \psi_i \) coefficient.

The MA(\(\infty\)) representation is mainly of theoretical interest because it helps understand the underlying properties of AR processes and because it is mathematically elegant and useful in certain proofs. However, it is not generally used for practical time series modeling because it involves an infinite number of parameters.


The steady-state solution of an AR process can be expressed as an MA(∞) representation. Here's an illustrative example:

$$
\begin{array}{rl}
y(t) &= \alpha q(t-1) + e(t) \\
y(t) &= q(t-1) + e(t-1) + e(t-2) \\
     &= e(t) + ae(t-1) + a^2q(t-2) \\
y(t) &= e(t) + ae(t-1) + a^2[\alpha(t-3) + e(t-2)] \\ 
     &= e(t) + ae(t-1) + a^2e(t-2) + a^3q(t-3) \\
\vdots & \vdots \\
y(t) &= e(t) + ae(t-1) + a^2e(t-2) + ... = \sum_{i=0}^{\infty} a^{i} e(t-i)  \quad\leftarrow\text{MA}(\infty)
\end{array}
$$




This is achieved by applying the constraints |a| < 1,
We also need to check if the variance is finite. As previously seen we need to compute:

$$\sum_{i=0}^{+\infty}c_i^2=\sum_{i=0}^{+\infty}a^{2i}$$


which is a geometric series, convergent if $|a|<1.$ Under this hypothesis, the series is:
We can observe that an AR(1) process can be expressed as a type of MA}(\infty).

This result shows that for the variance of the AR(1) process to be finite, the parameter $a$ must be strictly less than 1 in absolute value, ensuring the convergence of the geometric series to a finite value. This is a critical condition for the stability and stationarity of the AR(1) process.



Another possible method makes use of the Yule-Walker equations.

$$
\begin{aligned}Var[v(t)]=E[v(t)^2]=E[(av(t-1)+\eta(t))^2]=a^2E[v(t-1)^2]+E[\eta(t)^2]+2aE[v(t-1)\eta(t)\end{aligned}
$$

Where $E[v(t-1)\eta(t)]$ is the correlation between $v(t-1)$ and $\eta(t).$ We know that $v(t-1)$ depends on $\eta(t-1),\eta(t-2),...$ but given that $\eta(\cdot)$ is a white noise, it is not correlated with any other values at previous times. Thus, $v(t-1)$ and $\eta(t)$ are uncorrelated.

 Furthermore, $v(\cdot)$ is a stationary process, so $Var[v(t)]=Var[t-1]=\gamma(0).$
 We obtain, from the first equation:

$$
Var[v(t)]=\gamma(0)=a^2\gamma(0)+\lambda^2+0\quad\to\quad\gamma(0)=\frac{\lambda^2}{1-a^2}
$$

We compute the covariance in the same way and obtain: 







### ARMA




An ARMA process is a combination of Autoregressive (AR) and Moving Average (MA) models. ARMA processes allow for both recurrent terms based on **past outputs** and input terms based on **past errors**. 
The order of the AR and MA parts are denoted by $m$ and $n$, respectively. ARMA processes include all other AR and MA models as special cases. 


Also we can say that given an ARMA process $(n_a,n_b)$ the covariance function: 

- if $n_a > n_b$: $\gamma_y(\tau)$ becomes recursive for $\tau=n_a$
- if $n_a \le n_b$: $\gamma_y(\tau)$ becomes recursive for $\tau=n_b +1$

### ARMAX


While ARMA processes are a general model for time series, ARMAX are a model for I/O systems. 

$$e^{-j\omega}+e^{+j\omega}=2\cos(\omega)$$



Additionally, ARMA models can be extended to include **exogenous inputs** by modeling the signal as a filtering of the inputs. For example, an acceleration signal can be modeled as a filtering of engine torque or pedal position.

The concept of $e(t)$ is a mathematical artifact representing the stochastic nature of measurements. In reality, $e(t)$ does not exist, but when is the output of a system, both deterministic and stochastic effects need to be considered. The deterministic effect is caused by an input U, while the stochastic effect is due to the **inherent errors in measurements**. 

ARMAX($m,n,p,k$) with $k$ pure input/output (I/O) delay. $p$ order of the exogenous part:

$$\begin{aligned}y(t)&=a_1y(t-1)+a_2y(t-2)+\cdots+a_my(t-m)+\\&+c_0t(t)+c_1e(t-1)+\cdots\cdots c_n e(t-n)+\\&+b_0u(t-k)+b_{1}u(t-k-1)+\cdots b_{p}u(t-k-p)\end{aligned}$$


There is also N-ARMAX (non linear) where Unlike linear Armax models, no linear combinations or weighted averages are used, and f can be any non-linear, parametric function. Modeling and learning this type of model in system identification is complex due to the added complexity of the dynamic system and time axis, and preserving formal properties. 



## Processes with non-null term

The key takeaway is that incorrect results are obtained when dealing with non-zero mean processes if one fails to recognize their non-zero mean status or doesn't read the exercise carefully.

$$E[y(t)]=W(1)\cdot\mu$$

To calculate the correlation function for non-zero mean white noise, one must take expectations involving the mean, which involves more computations compared to the zero-mean case. However, a tip to make the computation more efficient is to introduce unbiased versions of the processes $\tilde{y}$ and $\tilde{e}$, which are obtained by removing the means directly. By writing the equation for $\tilde{y}$, the terms can be simplified to make the computations more manageable. 


$$\begin{cases}\tilde{y}(t)=y(t)-m_y\\\tilde{e}(t)=e(t)-m_e&\end{cases}$$



$$\gamma_{\tilde{y}}(\tau)=\gamma_{\gamma}(\tau)\quad\forall\tau $$

Estimation for Non-Zero-Mean Processes

1. Find mean with gain theorem:
   $$ E[y(t)] = W(1) \cdot \mu = \bar{y} $$

2. Define debiased processes:
   $$
   \begin{align*}
   \tilde{y}(t) &= y(t) - \bar{y} \\
   \tilde{e}(t) &= e(t) - \mu
   \end{align*}
   $$

3. Redefine the process using unbiased elements:
   $$ \tilde{y}(t+k|t) = \frac{C(z)}{A(z)} \cdot \tilde{e}(t)$$

4. Prediction adjustment for non-zero-mean:
   $$ \hat{y}(t+k|t) = \tilde{y}(t+k|t) + \mu_y $$
   
   Where the adjusted prediction is given by:
   $$ \hat{y}(t+k|t) = \frac{F(z)}{C(z)} \cdot y(t) + \left(1 - \frac{F(1)}{C(1)}\right) \cdot \mu_y $$










## All Pass Filter


![](images/Pasted%20image%2020240320162330.png)

An All Pass Filter (APF) is useful for converting a system to canonical form: we can do this by aggregating coefficients and multiplying by a fraction of identical polynomials (so that we essentially multiply by one) and then we redefine a new white noise process. 


Consider an all-pass filter with input u(t) and output y(t).
If u(t) is a stationary process, the spectrum of the output is proportional to the spectrum of the input.

If u(t) is constant, y(t) is constant.

If u(t) is sinusoidal, y(t) is sinusoidal.









The properties of a SSP don't depend on time t. In other words, what happens at time t is rapresentative also of what happens at time t + tau : there isn't a probabilistic difference between time instants.



An All pass filter is a particular process/filter which transform a white noise in a white noise with a different variance. 