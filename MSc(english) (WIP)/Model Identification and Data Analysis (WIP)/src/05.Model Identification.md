# Model identification

LS (Least Squares) identification method
â€¢ Identifiability: invertibility of S(N) and R(N)
Persisten excitation
Uniqueness of estimation
Maximum likelihood
Asymptotic analysis of PEM methods:
performance of identification models
Asymptotic behaviour of PEM
LS estimate procedure

## intro

![](images/Pasted%20image%2020240402185207.png)

An **identification problem** consists of four elements: 

- $S$ a system generating data
- $M$ a model used to explain the system:
	- **White-box**:
		- Pros:
			- Physical meaning of variables
			- The model can be generalized
		- Cons:
			- Everything must be known exactly a-priori
			- Expensive and time-consuming
			- Non feasible with complex systems
	- **Black-box**: 
		- Pros:
			- Implementable even without deep process knowledge
			- Fast and cheap
		- Cons:
			- Non physically interpretable
			- Non general: if the system changes, the experiment must be repeated


an identification algorithm to find the best model

- $I$ an identification experiment to obtain data. The system, model, identification algorithm, and identification experiment make up an identification problem. Data provided can limit the model's ability to extract more information than what's available. The model can only extract information that exists within it. For instance, steady-state data can provide information about a system's gain but cannot provide information related to its time constant. The linear model is an example of a commonly used model, but its assumptions may not always hold, requiring different optimization algorithms.




Model identification process answers to the question "How can I obtain a model?"

We would like to make a model relying on data as the source like in [Machine Learning](../../Machine%20Learning/Machine%20Learning.md) .

## ML (Maximum Likelihood) method

This method differs from the Least Squares method because it is based on a different class of models, ARMA (or ARMAX) instead of AR (or ARX). This means that there is no more linearity in the parameters and no normal equations.
traditional methods like Least Squares cannot be directly applied due to the non-linearity introduced by the MA component.


We use an iterative (numerical approach) an update rule.

Netwon's~rule:

$$\theta^{(1+1)}=\theta^{(1)}-[\text{Hessian}]^{-1}\cdot \text{gradient}$$
gradient descent:}

$$\theta^{(1+1)}=\theta^{(1)}-\eta\cdot \text{gradient}$$


Quasi-Newton's rule:

$$\theta^{(i+1)}=\theta^{(i)}-[\frac{\text{aproximate}}{\text{Hessian}}]^{-1}\text{gradient}$$



## Parametric system identification 


$$M=y(t)=\frac{B(z,\theta)}{A(z,\theta)}\mu(t-d)+\frac{C(z,\theta)}{A(z,\theta)}e(t)$$

where $\theta$ are the parameters $\in \mathbb{H}$ (the parameters domain). 

We must decide $\theta$ but also $\lambda^2$ , $d$,$m$,$p$,$n$ . 

PEM identification (prediction error minimization)

A good model should return a low emprical variance of the prediction error:

$$J_N(\vartheta)=\frac{1}{N}\sum_{i=1}^{N}\left(\mathrm{y}(i)-\hat{y}(i\mid i-1,\vartheta)\right)^2=\frac{1}{N}\sum_{i=1}^{\text {\# samples}}\varepsilon(i,\vartheta)^2$$


we would like to minimize it. 



### Identification of ar/arx models 

$$y(t)=\frac{B(z)}{A(z)}u(t-d)+\frac1{A(z)}e(t)$$

$$A(z)=1-z^{-1}-^{-2}-\cdots-2mz$$$$B(z)=b_{o}+b_{1}z^{-1}+b_{2}z^{-2}+\cdots-b_{p-1}z^{-p+1}$$ 

and since AR has no MA port:

$$C(z)=1\quad\color{red}$$

Explicit minimization is possible 


$$\hat{\vartheta}_N=\left(\sum_{t=1}^N\varphi(t)\varphi^T\left(t\right)\right)^{-1}\sum_{t=1}^Ny(t)\varphi(t)$$



We have a set of data $y(1), y(2), \dots (u(1), u(2), \dots)$ and we want to find the best model that approximate these data. 

In this discussion, we focus on parametric identification of dynamic systems. The process involves five main steps: 

1) designing experiments
2) selecting models
3) defining quality measures
4) optimizing models
5) checking model validity. 

The first step is transforming given data into specific parameter values to learn a model. Parametric identification simplifies mathematics but requires identifying a model with specific parameters, making the process more complex.

An identification problem is essentially a parametric optimization problem. We recast the learning problem as an optimization task with a quadratic cost function, seeking the minimum stationary point to find the optimal parameters.

A positive definite matrix is a quadratic matrix where the associated quadratic form, represented as $x^T M x$, is positive for any non-zero vector x. The speaker explains that a positive matrix, where all elements are positive, is not the same as a positive definite matrix. 


$$J(\vartheta)=\frac1N\sum_{t=1}^N\left(y(t)-\varphi(t)^T\vartheta\right)^2$$


The hessian is then always positive semi-definite and two case happens: 

1) hessian is singular 
2) hessian is invertible 

In the generative paraboloid case, multiple global minimums are found but it is an error since it's known that only one system originated the data. 

Multiple global are not feasible solutions and this can happen because:

- **over**: the chosen models are too complex for the system 
- **under**: data isn't representative enough 


Identifiability We say that the system is identifiable when the above equation admits only one solution. This happens when the matrix


In ideal situations, where data is thoroughly explored and the right model class is selected, the optimization problem will be well-posed, ensuring a minimum solution. This concept applies to both AR and ARMAX models.

For ARMAX models, the identification process uses the maximum likelihood approach. 


We want to minimize

$$
J(\vartheta)=\frac1N\sum_{t=1}^N\varepsilon_\vartheta(t)^2
$$

but this time the relation with the parameters $\vartheta$ is not linear anymore. Therefore, we need to use an iterative method.

The prediction error epsilon\_t at time t given data up to t-1 is dependent on the coefficients of the z polynomial non-linearly. This non-linear dependence was the reason for the use of the least squares formula in the past due to its linear dependence on the coefficients a and b. However, since theta contains the coefficient of c, the optimization problem becomes non-linear, requiring numerical approaches for solution. 

An iterative numerical method such as gradient descent can be used with an initial estimate theta\_1 and an update rule that computes the next guess for theta based on the previous iteration's y and an update to the parameters. 

The speaker discusses the issue of local optima in optimization and mentions the Newton=n method as a potential solution. In the context of R Max models, the method involves taking the quadratic expansion of the cost function and finding the minimum of the resulting paraboloid to update the parameter.


![](images/Pasted%20image%2020240315154109.png)

The speaker is discussing the second-order Taylor expansion of a cost function J with respect to a variable theta. They explain that to find vi, the second-order term, we need to compute the gradient and Hessian of the cost function and plug them into the expression. The condition for the minimum of vi occurs when theta i plus one is the value of theta for a stationary point. The update rule for the next guess of theta is derived using Newton's approach, which involves taking the inverse of the Hessian and pre-multiplying it with the difference between theta i plus one and theta i. The speaker emphasizes that this calculation is only used as a tool to obtain the final update, and they will actually work with the Hessian and gradient of the original cost function to find a variation of the current guess of theta from the data.

We can compute the cost function directly from data if we have the correct initial condition, resulting in the global solution. Each iteration requires retrieving the gradient and Hessian or its inverse from the data. 



The speaker discusses the second term in the Newton's method, which is often neglected due to it potentially being negative and small when close to the optimal solution. The matrix involved in the method, Hessian, is positive semi-definite, ensuring the method moves towards the minimum. The second term's negligibility arises from being small near the solution and the Hessian's positive semi-definiteness, making the Newton's method safer. 

The formula for the Newton's method is rewritten using the gradient and the action of the cost function. The matrix dimensions are confirmed as consistent, resulting in a column vector representing the variation of theta with respect to theta i.




 The derivative of the error function epsilon with respect to the parameter theta is required to compute the gradient for optimization. While we have an expression for epsilon, the derivative is unknown. We need to find the derivative of epsilon with respect to each coefficient in theta, including a, m, b, and c. We write epsilon as an explicit function of these coefficients and compute the derivatives in classes: first for the parameters characterizing a, then for b, and finally for c. The derivative of epsilon with respect to a one is z - 1 multiplied by (a - 1) over c times y. We define a new signal, alpha t, as -1/c * y, and the derivative of epsilon with respect to a one is equivalent to alpha t - 1.


The fact that Hessian is $\ge 0$ is good in terms of "safety" of the method

![](images/Pasted%20image%2020240315161420.png)

The second term is likely to be "small" close to the minimum (at the end of the iterations)

and if we neglect the second order derivative term we also have



$$\frac{\partial^2J(\vartheta)}{\partial\vartheta^2}=\frac1N\sum_{t=1}^N\varphi_\vartheta(t)\varphi_\vartheta(t)^T$$

so the new iteration formula is

$$\vartheta^{(r+1)}=\vartheta^{(r)}-\left[\frac1N\sum_{t=1}^N\varphi_\vartheta(t)\varphi_\vartheta(t)^T\right]^{-1}\left[\frac1N\sum_{t=1}^N\varphi_\vartheta(t)\varepsilon_\vartheta(t)\right]$$

![](images/Pasted%20image%2020240315161623.png)



![](images/Pasted%20image%2020240315161942.png)





This method is one way to solve a numerical optimization algorithm, but there are other methods as well.

$$\theta^{(i+i)}=\theta^{(i)}-[\text{hessian}]^{-1}-\text{gradient}$$


while the gradient descent:

$$\theta^{(i+1)}=\theta^{(i)}-\eta \cdot \text{gradient},$$

Same philosophy since both moves in the directions where the right direction.

The newton's rule depending on the region you are "moves" smarter and uses less iterations ( so it's more computational efficient)

The quasi newton is computationally lighter than Newwton's (we neglect a part) but at the same time is more accurate than the gradient descent method

$$Q^{(i+1)}=\theta^{(i)}-[\text{approximate Hession}]\cdot \text{gradient}$$


 Quasi-Newton methods, such as BFGS, use an approximate Hessian to strike a balance between accuracy and computational efficiency. These methods are safer because they avoid the issue of encountering negative definite conditions and require fewer computations than Newton's method. However, they may introduce some inaccuracy in the computation of the Hessian at the beginning.

In optimization, gradient descent is a common method used to find the minimum of a function. The gradient tells you the direction and the magnitude of the next step. Variants of gradient descent exist, such as **time-varying** ones, but in the context of system identification with a limited number of parameters, the quasi-Newton rule is typically used. 


In NN gradient descent variants are more effective (since the are a lot of parameters) .

In this framework (system identification) we use quasi-newton's rule.




## Nonparametric identification 


So far if we want to infer something about the underling stochastic process we need to learn the model first. 

We could directly estimate from data the mean $m_y$ $\gamma _y(\tau)$ and $\Gamma_y(\omega)$ without first identifying a full model of $W(z)$ . 



Estimation of mean, covariance and spectrum The true generating mechanism y(t) is usually not known, but we have the data y(1), y(2), ..., y(N) collected from its realization. For the same reason we donâ€™t know the real value of the mean, the covariance and the spectrum, but we can compute their estimated values from the data. The estimate can be computed either directly from the data, or by finding a suitable model which represents the generating mechanism through the identification process and then by estimating the spectral properties of that model. In this section we will refer to a generic estimator from the N data (e.g. the sample mean or sample covariance) as Ë†sN .


Consistency means That is, the estimate error variance tends to zero as the number of measured data tends to infinity.



### Sample mean

Estimating the mean of something from a series of points is rather straightforward; you take the mean of the points. The most natural estimator is $\hat{\mu}_n$, where $n$ refers to the number of samples. But the main question is not how to compute the mean; it's about understanding the properties of this estimator. We want to understand if this is a good estimator of the real mean.

To determine if an estimator is good, we need to define what we mean by "good." We will provide a couple of definitions. 


#### Estimator correctness

An estimator's **correctness**: An estimator is correct if the expected value of the estimator is equal to the probabilistic property to be estimated. In the case of the sample mean, we're asking that the expected value of the estimator is equal to the quantity to be estimated. However, it's important to note that this does not mean that the estimation is equal to the quantity to be estimated. It means that the expected value of the estimator should be equal to the mean. The expected value is taken with respect to the realization of the process.

Now, let's check if our estimator, $\hat{\mu}_n$, is correct. We have to verify that the expectation of the estimation is equal to the real mean. In other words, we need to prove that:

$E[{\hat{\mu}}_n] = \mu$ where $\mu$ is the real mean.

#### Estimator consistency

Sometimes having a correct estimator doesn't mean that the estimator is good. What we also want is that when you increase the amount of information, say that you have more data, then you also want that your estimation improves.

? "An estimator is consistent if the estimate error variance tends to zero as the number of data grows... This is equivalent to say that the estimate must tend to the real quantity. Of course, it's only asymptotic."

**An estimator is consistent if the estimate error variance tends to zero as the number of data grows... This is equivalent to say that the estimate must tend to the real quantity. Of course, it's only asymptotic.**




We can use the sample covariance function:

$$\gamma_{y}(t)=\frac{1}{N}\sum_{t=1}^{N}y(t)y(t-\tau)$$


The intuition behind this is that as we add more data, we are able to estimate the covariance function at more points, and the additional data provides new information about the process. This is because the process is not constant over time, unlike the previous example, and has some dynamics that depend on the selection of parameters.

This estimator is consistent for ARMA processes, which have dynamics that depend on the selection of parameters and do not have full memory of the past. This means that as we add more data, we are able to get a better estimate of the covariance function, as the new data brings new information about possible different realizations of the process.

Practically speaking, the estimator is reliable only for $\tau$ much lower than $n-1$, so that you have a sufficient number of samples to estimate well the statistical average. Okay, then we have the other problems of the negative taus, but that's not a real problem. So for negative taus, we can define, let's say, the estimator is slightly different than that and say this is n minus tau, the sum for t that goes from 1 to n minus tau of yt times yt.

$$\hat\gamma_{n}(\tau)=\frac{1}{N-\tau}\sum_{t=1}^{N-\tau}y(t)y(t+\tau)$$



#### sample spectrum


$$\Gamma_{y}(w)=\sum_{z=-\infty}^{\infty}\gamma(z)e^{-jwz}$$

$$\hat{\Gamma}_{N}(w)=\sum_{\tau=-(N-1)}^{N-1}\hat{\gamma}_{N}(\tau)e^{-jwz}$$

The difference with respect to the previous cases is that this is even more difficult than the covariance function, because the covariance function was a function and not a scalar like the mean. But this bad guy is a derived quantity. So it's a quantity that depends on a function that's already not available. So that's the covariance function. So in order to compute the spectrum I need first to compute the covariance. Okay, so it's an indirect estimation in a sense that I have to pass through an intermediate estimate.

We say that this estimator is not correct, but it's asymptotically correct. So it's not correct, but it's only asymptotically correct. So if the data set is large, good, if it's not large, you might have a bias. 
And the bias could be different at different $\omega$ because remember that $\gamma$ is a function. So you may have a large bias for some omegas and a small bias for others.

And what about consistency? Let's see consistency. 

$$\mathbb{E}[(\hat{\Gamma}_{N}(\omega)-\Gamma_{\gamma}(\omega))^{2}] \xrightarrow[n\to\infty]{} \Gamma(\omega)^2$$

But we have, let's say, a negative theorem that says that if you take the variance of the estimation error for n that tends to infinity, not only, we cannot prove that it, that it tends to zero, but it can also prove that it tends to gamma omega squared. Okay. So it's really, really bad news because usually this guy is different from zero. Usually. So this is consistent even if you take infinitely many samples. 

And moreover, we can also prove that the error in the frequency domain is completely uncorrelated.

$$\mathbb{E}[(\hat{\Gamma}_{N}(\omega_{1})-\Gamma(\omega_{1}))((\hat{\Gamma}_{N}(\omega_{2})-\Gamma(\omega_{2}))]\xrightarrow[N\rightarrow\infty]{}0$$


This is a problem actually. The solution to this is called averaging, or sometimes regularization:

You can take one dataset from 1 to n, divide it into parts, say you take $n/4$, $n/2$, and $3n/4$. Then you compute an estimator of the spectrum for each of these parts. What happens is that if $n/4$ is large enough, then what we can prove is that the variance of this new estimator is equal to the variance of the previous estimator divided by the number of times you have divided the dataset approximately.

The explanation is simple, we are trying to averaging out the noise.

We get the variance of the estimation divided by a number that is approximately the number of parts used to divide the dataset.

You might be tempted to say, "Okay, then I will take an infinite amount of parts and then I will divide the variance by infinity." But the problem is that if you do that, you lose the signal itself. You have to make a trade-off between the number of parts you divide the data into and the variance of the estimator. In this course, we will see how to make this trade-off and how to choose the optimal number of parts to divide the data into.

#### sampled prediction error variance
consider the following measured samples
using the available data
$$J_k(a)=\frac1k\sum_{t=0}^{t=k}\left(y(t+1)-\hat{y}(t+1|t)\right)^2$$



### Data preprocessing


What if $y(t)$ is non-stationary? So these two are possible causes of non-stationarity:

- trend 
- seasonality 


To work with non-stationary processes, we first need to estimate possible trends or seasonalities. Then we can remove them and work with the reminder **SSP (Stationary Stochastic Process)**.


#### Trend removal

$$y(t)=\tilde y (t) + kt +m$$

So in order to work with $\tilde y(t)$ first we need to estimate $k$ and $m$. 
So we first estimate m and k, characterizing the trend.
Then we remove the trend from the data set. We have a corresponding data set for this stationary stochastic process. And then with that data, we can do whatever we want. 

I can actually say that I expect this quantity to be equal to zero on average. 

$$\mathbb{E}[y(t)-kt-m]=\mathbb{E}[\tilde{y}(t)]=0$$

Inspired by the above equality, we can find $\hat{m}$ and $\hat{k}$ as the argument of the minimum with respect to $m$ and $k$ :

$$(\hat{m},\hat{k})=\operatorname{argmin}_{m,k}\frac{1}{N} \sum_{t=1}^{N}\left(y(t)-k(t-m)\right)^2$$


This minimization problem we formulated is precisely a least squares problem. We're trying to minimize the **cost**, or squared error, between the observed data and the fitted trend line. Since the error term, `y(t) - k(t-m)`, is linear in both `k` and `m`, squaring it results in a squared error function. 

Geometrically, this squared error function typically forms a paraboloid in the space of `k` and `m`. Fortunately, for paraboloids, we expect there to be a single **global minimum**. Just like with the ordinary least squares formula used for ARX processes, we can find this minimum by taking the derivative of the squared error function with respect to `k` and `m` and setting the gradient equal to zero. This will provide us with the optimal estimates `k` and `m` that characterize the underlying trend in the data.

#### Seasonality


$$y(t)=\tilde{y}(t)+s(t)$$


where $s(t)=s(t+k \mathbb{T})$ where $\mathbb{T}$  is the period.
In the same way we need to estimate $s(t)$ . 
The underlying idea is :

$$\hat{S}(t)=\frac{1}{M}\sum_{k=0}^{M-1}y(t+hT)=\frac1M\sum_{k=0}^{M-1}\widetilde{y}(t+hT)+\frac1M\sum_{k=0}^{M-1}S(t+hT)$$
But remember that we had also to estimate $T$ period. 
