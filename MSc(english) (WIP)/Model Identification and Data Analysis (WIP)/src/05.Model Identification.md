

# Model identification

Model identification process answers to the question "How can I obtain a model?"
We would like to make a model relying on data as the source like in [Machine Learning](../../Machine%20Learning/Machine%20Learning.md) .

![](images/Pasted%20image%2020240402185207.png)


## Identification of ar/arx  

An identification problem is essentially a parametric optimization problem: we have a set of data $y(1), y(2), \dots (u(1), u(2), \dots)$ and we want to find the best model that approximate these data focusing on **parametric** identification of dynamic systems. 
For AR(X) models we can apply Least Squared method. 
Generic expression of AR(X):
$$M=y(t)=\frac{B(z,\theta)}{A(z,\theta)}\mu(t-d)+\frac{1}{A(z,\theta)}e(t)$$
where $\theta$ are the parameters $\in \mathbb{H}$ (the parameters domain). 

$$\theta=\begin{vmatrix}a_1\\\vdots\\a_{n_a}\\b_1\\\vdots\\b_{n_b}\end{vmatrix}\quad\varphi(t)=\begin{vmatrix}y(t-1)\\\vdots\\y(t-n_a)\\u(t-1)\\\vdots\\u(t-n_b)\end{vmatrix}$$

A good model should return a low empirical variance of the prediction error:

$$J_N(\vartheta)=\frac{1}{N}\sum_{i=1}^{N}\left(\mathrm{y}(i)-\hat{y}(i\mid i-1,\vartheta)\right)^2=\frac1N\sum_{t=1}^N\left(y(t)-\varphi(t)^T\vartheta\right)^2$$

we would like to minimize it:

$$\frac{\partial J_{N}(\theta)}{\partial\theta}=\frac{d}{d\theta}[J_{N}(\theta)]=\frac{d}{d\theta}\left[\frac{1}{N}\sum_{t=1}^{N}(y(t)-\varphi(t)^{T}\theta)^{2}\right]=0$$

$$\begin{aligned}
\frac{\partial J(\theta)}{\partial\theta}& =-\frac1N\sum_{t=1}^N2\left(y(t)-\theta'\varphi(t)\right)\varphi(t)'  \\
&=-\frac{2}{N}\left(\sum_{t=1}^{N}y(t)\varphi(t)'-\sum_{t=1}^{N}\theta'\varphi(t)\varphi(t)'\right)=0\end{aligned}$$

$$\left[\sum_{t=1}^{N}\varphi(t)\varphi(t)^{T}\right]\hat{\theta}_{N}=\sum_{t=1}^{N}\varphi(t)y(t)$$

reversing the last equation:
$$\sum_{t=1}^N\varphi(t)\varphi(t)'\theta=\sum_{t=1}^Ny(t)\varphi(t)'$$
and we obtain the Least Squares method for AR(X) models family:
$$\hat{\theta}=\left[\sum_{t=1}^N\varphi(t)\varphi(t)'\right]^{-1}\sum_{t=1}^Ny(t)\varphi(t)'$$
  
LS estimates converge to $\Delta$ (set of global minimum of $J(\theta)$): 

  1. $\mathbb{E}[\varphi(t)\varphi(t)^T]$ non-singular $\rightarrow \Delta = \{\theta^*\}$ and we have **identifiability**
  2. $\mathbb{E}[\varphi(t)\varphi(t)^T]$ singular $\rightarrow J(\theta)$ has $\infty$ global minima, including $\theta^*$ 

![](images/Pasted%20image%2020240410175412.png)

Actually if multiple global minimums are found, it's an error since it's known that only one system originated the data. So multiple minimums are not feasible solutions and this can happen because:

- **over**: the chosen models are too complex for the system 
- **under**: data isn't representative enough 

At the end we say that **identifiability** is when there is only **one** solution. Since AR has no MA part ($C(z)=1$), explicit minimization is possible:

$$\hat{\vartheta}_N=\left(\sum_{t=1}^N\varphi(t)\varphi'\left(t\right)\right)^{-1}\sum_{t=1}^Ny(t)\varphi'(t)$$

In the above formula, the crucial thing is that the matrix  $\left[\sum_{t=1}^N\varphi(t)\varphi'\left(t\right)\right]$ is invertible: it must be positive semi-definite. 

A necessary condition for the invertibility is that the input $u(t)$ is **persistently exciting** of order $k$ which is a property which interest the $k \times k$  matrix:
 $$\begin{vmatrix}\gamma_{uu}(0)&\gamma_{uu}(1)&\gamma_{uu}(2)&\cdots\\\gamma_{uu}(1)&\gamma_{uu}(0)&\gamma_{uu}(1)&\ddots\\\vdots&\ddots&\ddots&\ddots\end{vmatrix}$$

This matrix must be invertible $\leftarrow$ is a necessary condition to make invertible $\bar R$ !
Note that a $WN$ is **persistently exciting** of any order:

$$\begin{vmatrix}\gamma_{uu}(0)&\gamma_{uu}(1)&\gamma_{uu}(2)&\cdots\\\gamma_{uu}(1)&\gamma_{uu}(0)&\gamma_{uu}(1)&\ddots\\\vdots&\ddots&\ddots&\ddots\end{vmatrix}=\begin{vmatrix}\lambda^2&0&0&\cdots\\0&\lambda^2&0&\ddots\\\vdots&\ddots&\ddots&\ddots\end{vmatrix}=\lambda^2I$$

## Identification of ARMA(x): Maximum Likelihood method

ARMA (or ARMAX)  identification differs from AR (or ARX) because there is no more linearity in the parameters. Traditional methods like Least Squares cannot be directly applied, we must use an iterative numerical method such as gradient descent can be used with an initial estimate $\theta_1$ and an update rule that computes the next guess for $\theta$ based on the previous iteration.
This update rule comes in 3 main main flavors in this course: 

- **Newton's rule**:

$$\theta^{(i+1)}=\theta^{(i)}-[\text{Hessian}]^{-1}\cdot \text{gradient}$$
- **Gradient descent**: [gradient descent in neural networks](projects/polimi-notes/MSc(english)%20(WIP)/Artificial%20Neural%20Networks%20and%20Deep%20Learning%20(WIP)/src/02.%20FFNN%20in%20practice.md#Gradient%20descent%20algorithms) is more effective since the are a lot of parameters.

$$\theta^{(i+1)}=\theta^{(i)}-\eta\cdot \text{gradient}$$

- **Quasi-Newton's rule**: in the system identification framework, it's often used quasi-newton's rule.

$$\theta^{(i+1)}=\theta^{(i)}-[\frac{\text{aproximate}}{\text{Hessian}}]^{-1}\text{gradient}$$

### Newton's rule

Newton method basically involves taking the quadratic expansion of the cost function and finding the minimum of the resulting paraboloid to update the parameter. 


![](images/Pasted%20image%2020240315154109.png)

If you consider a quadratic approximation, the approximating function can be obtained by the Taylor development.

$$V(\theta)=\varepsilon(t|t-1,\theta)|_{\theta=\theta^{(i)}} + (\theta-\theta^{(i)})^\top\left.\frac{\partial\varepsilon(t|t-1,\theta)}{\partial\theta}\right|_{\theta=\theta^{(i)}} + \frac{1}{2}(\theta-\theta^{(i)})^\top\left.\frac{\partial^2\varepsilon(t|t-1,\theta)}{\partial\theta^2}\right|_{\theta=\theta^{(i)}}(\theta-\theta^{(i)})$$

We need to compute the minimum (deriving) of $V(\theta)$ . The crucial thing here is $\frac{\partial\varepsilon(t|t-1,\theta)}{\partial\theta}$ (which is a vector since it's a derivative of a scalar respect to a vector of parameters.

$$\begin{aligned}\frac{\partial\varepsilon(t|t-1,\theta)}{\partial\theta}=[\alpha(t-1)\alpha(t-2)\ldots\alpha(t-m)\beta(t-1)\ldots\ldots\gamma(t-1)\ldots\gamma(t-m)]^{T}\\\alpha(t)=-\frac{1}{C(t)}u(t)\\\beta(t)=-\frac{1}{C(z)}u(t)\\\gamma(t)=-\frac{1}{C(z)}\varepsilon(t|t-1,\theta)\end{aligned}$$

So we need to compute polynomials considering $\theta$ at step $i$:
   $$ A(z, \theta^{(i)}), B(z, \theta^{(i)}), C(z, \theta^{(i)}) $$

After computing the signals $\varepsilon(t|t-1, \theta^{(i)}),\alpha(t, \theta^{(i)}),\beta(t, \theta^{(i)}),\gamma(t, \theta^{(i)})$ and so the minimum we also need the hessian (which is a matrix):

$$\frac{\partial^2\varepsilon(t|t-1,\theta)}{\partial\theta^2}$$

Skipping the endless computation, at the end the update rule:

$$\vartheta^{(i+1)}=\vartheta^{(i)}-\left[\frac1N\sum_{t=1}^N\frac{\partial\varepsilon(t)}{\partial\theta}\left(\frac{\partial\varepsilon(t)}{\partial\theta}\right)^T\right]^{-1}\left[\frac1N\sum_{t=1}^N\frac{\partial\varepsilon(t)}{\partial\theta}\varepsilon_\vartheta(t)\right]$$

$$\theta^{(i+1)}=\theta^{(i)}-[\text{Hessian}]^{-1}\cdot \text{gradient}$$

![](images/Pasted%20image%2020240315161942.png)



## Non-parametric identification 

So far if we want to infer something about the underling stochastic process we need to learn the model first and then make a **parametric identification**. 

Now let's try a non-parametric one directly estimating from data $m_y$, $\gamma _y(\tau)$ and $\Gamma_y(\omega)$ without first identifying a full model of $W(z)$ . 

But before diving in the estimators, we we need to define what we mean by "good estimator". We will use two definitions: 

- **Correctness:** An estimator is considered correct (or unbiased) if its expected value is equal to the true parameter it is estimating, indicating that the mean of many independent estimates would converge to the true parameter. $E[{\hat{\mu}}_n] = \mu$ where $\mu$ is the real mean. 
- **Consistency:** An estimator is consistent if the probability of the estimates being close to the true parameter increases as the sample size grows, meaning that with more data, the estimates become progressively more precise. $Var[\hat s_N ] \rightarrow 0$ as $N \rightarrow \infty$ . 

### Estimation of Mean

The most natural estimator is $\hat{\mu}_n$, where $n$ refers to the number of samples.

$$\hat\mu_n=\frac1N\sum_i^N y(i)$$

This estimator grows more informative with additional data, capturing the dynamics of ARMA processes without relying on full historical memory.

### Estimation of Covariance

$$\gamma_{y}(t)=\frac{1}{N}\sum_{t=1}^{N}y(t)y(t-\tau)$$

The sample covariance function $\hat{\gamma}_n(\tau)$ becomes more reliable as the sample size increases, especially when considering lags $\tau$ significantly smaller than the sample size. For negative lags, adjustments are made to the estimator to maintain accuracy, emphasizing the importance of having enough data points to estimate statistical averages accurately.

### Estimation of Spectrum 

We can estimate this:
$$\Gamma_{y}(w)=\sum_{z=-\infty}^{\infty}\gamma(z)e^{-jwz}$$
with this: 

$$\hat{\Gamma}_{N}(w)=\sum_{\tau=-(N-1)}^{N-1}\hat{\gamma}_{N}(\tau)e^{-jwz}$$

Estimating the spectrum $\hat{\Gamma}_N(\omega)$ is complex as it is derived from the covariance function, making it an indirect estimation. This estimator is not initially correct but becomes asymptotically correct with a large dataset, although it may exhibit bias at different frequencies.
Consistency in this context is challenging, as it's shown that the error variance does not tend to zero even with infinite samples:

$$\mathbb{E}[(\hat{\Gamma}_{N}(\omega)-\Gamma_{\gamma}(\omega))^{2}] \xrightarrow[n\to\infty]{} \Gamma(\omega)^2$$

The best we can do to address this issue is applying the so called Bartlett method: average the spectrum by dividing the dataset into $r$ parts and computing the spectrum for each part: 
$$ \hat{\Gamma}^{(i)}(\omega), \quad i,...,r $$
Then we average these estimators: 
$$ \bar{\hat{\Gamma}}(\omega) = \frac{1}{r} \sum_{i=1}^r \hat{\Gamma}_{\hat{N}}^{(i)}(\omega) $$
Under the assumption that the data of different sub-series are uncorrelated (hence the requirement $N \gg r$), the variance is approximately:
$$ \text{Var}[\bar{\hat{\Gamma}}(\omega)] \approx \frac{1}{r^2} \Gamma^2(\omega) $$
The uncertainty is now "significantly" reduced. 

### Estimation of Prediction Error Variance 

$$J_k(a)=\frac1k\sum_{t=0}^{t=k}\left(y(t+1)-\hat{y}(t+1|t)\right)^2$$

Using measured samples, one can estimate the prediction error variance through $J_k(a)$, which evaluates the performance of the predictive model against actual outcomes, with the goal of minimizing this variance for better predictions.
At the end we can say that asymptotically we will converge to the minimum of the asymptotic cost since:
$$J_N(\vartheta)\xrightarrow[N\to\infty]{}[\bar{J}(\vartheta)=\mathbf{E}[\varepsilon_\vartheta(t)^2]$$
which implies that:

$$\min\{J_N(\vartheta)\}\xrightarrow[N\to\infty]{}\min\{\bar{J}(\vartheta)\}$$

The prediction error minimization guarantees that the minimum of the asymptotic cost (which is guarantee to converge to) corresponds to the set of parameters that describe the real system.
This theorem is very, very significant. 
If the system that you want to model and learn from data is inside the model set, if you minimize the **variance of the prediction error**, asymptotically so with a large data set, you will converge exactly to that system. 
$$\hat \theta _N \xrightarrow[N\to\infty]{} \theta ^ o$$

So it means that the prediction error algorithm that we are using in this course is actually good, meaning that they will lead you to the right model (at least in this very ideal case where the system is in the model set). 

### Data preprocessing

What if $y(t)$ is non-stationary? So these two are possible causes of non-stationarity that we address in this course:

- trend 
- seasonality 

To work with non-stationary processes, we first need to estimate possible trends or seasonalities. Then we can remove them and work with the reminder **SSP (Stationary Stochastic Process)**.

#### Trend removal

$$y(t)=\tilde y (t) + kt +m$$

So in order to work with $\tilde y(t)$ we first estimate $k$ and $m$. 
Then we remove the trend from the data set. We can also say:
$$\mathbb{E}[y(t)-kt-m]=\mathbb{E}[\tilde{y}(t)]=0$$
Inspired by the above equality, we can find $\hat{m}$ and $\hat{k}$ as the argument of the minimum with respect to $m$ and $k$ :
$$(\hat{m},\hat{k})=\operatorname{argmin}_{m,k}\frac{1}{N} \sum_{t=1}^{N}\left(y(t)-k(t-m)\right)^2$$

This expression is a classic least squares optimization, which aims to determine the "best fit" by finding the line (or trend) that best fits the data, where $k$ can be thought of as a scaling factor and $m$ as a time shift.
As the LS cost function, the expression forms a paraboloid, which again has a single global minimum making possible to find the best linear trend estimate in the data.

#### Seasonality

$$y(t)=\tilde{y}(t)+s(t)$$

where $s(t)=s(t+k \mathbb{T})$ where $\mathbb{T}$  is the period.
In the same way we need to estimate $s(t)$ . 
The underlying idea is :

$$\hat{S}(t)=\frac{1}{M}\sum_{k=0}^{M-1}y(t+hT)=\frac1M\sum_{k=0}^{M-1}\widetilde{y}(t+hT)+\frac1M\sum_{k=0}^{M-1}S(t+hT)$$
But remember that we had also to estimate $T$ period. 
