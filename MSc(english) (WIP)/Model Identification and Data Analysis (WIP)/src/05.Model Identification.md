

# Model identification

Model identification process answers to the question "How can I obtain a model?"
We would like to make a model relying on data as the source like in [Machine Learning](../../Machine%20Learning/Machine%20Learning.md) .

![](images/Pasted%20image%2020240402185207.png)


## Identification of ar/arx  

An identification problem is essentially a parametric optimization problem: we have a set of data $y(1), y(2), \dots (u(1), u(2), \dots)$ and we want to find the best model that approximate these data focusing on **parametric** identification of dynamic systems. 
For AR(X) models we can apply Least Squared method. 
Generic expression of AR(X):
$$M=y(t)=\frac{B(z,\theta)}{A(z,\theta)}\mu(t-d)+\frac{1}{A(z,\theta)}e(t)$$
where $\theta$ are the parameters $\in \mathbb{H}$ (the parameters domain). 

$$\theta=\begin{vmatrix}a_1\\\vdots\\a_{n_a}\\b_1\\\vdots\\b_{n_b}\end{vmatrix}\quad\varphi(t)=\begin{vmatrix}y(t-1)\\\vdots\\y(t-n_a)\\u(t-1)\\\vdots\\u(t-n_b)\end{vmatrix}$$

A good model should return a low empirical variance of the prediction error:

$$J_N(\vartheta)=\frac{1}{N}\sum_{i=1}^{N}\left(\mathrm{y}(i)-\hat{y}(i\mid i-1,\vartheta)\right)^2=\frac1N\sum_{t=1}^N\left(y(t)-\varphi(t)^T\vartheta\right)^2$$

we would like to minimize it:

$$\begin{aligned}
\frac{\partial J(\theta)}{\partial\theta}& =-\frac1N\sum_{t=1}^N2\left(y(t)-\theta'\varphi(t)\right)\varphi(t)'  \\
&=-\frac{2}{N}\left(\sum_{t=1}^{N}y(t)\varphi(t)'-\sum_{t=1}^{N}\theta'\varphi(t)\varphi(t)'\right)\end{aligned}$$


$$\sum_{t=1}^N\varphi(t)\varphi(t)'\theta=\sum_{t=1}^Ny(t)\varphi(t)'$$


$$\hat{\theta}=\left[\sum_{t=1}^N\varphi(t)\varphi(t)'\right]^{-1}\sum_{t=1}^Ny(t)\varphi(t)'$$
  
LS estimates converge to $\Delta$ (set of global minimum of $J(\theta)$). 
So at the end for AR/ARX if $\theta^* \in M(\theta)$, then $\theta^* \in Δ$ :

  1. $\mathbb{E}[\varphi(t)\varphi(t)^T]$ non-singular $\rightarrow \Delta = \{\theta^*\}$ (identifiability)
  2. $\mathbb{E}[\varphi(t)\varphi(t)^T]$ singular $\rightarrow J(\theta)$ has $\infty$ global minima, including $\theta^*$ 

![](images/Pasted%20image%2020240410175412.png)

Actually if multiple global minimums are found, it's an error since it's known that only one system originated the data. So multiple minimums are not feasible solutions and this can happen because:

- **over**: the chosen models are too complex for the system 
- **under**: data isn't representative enough 

At the end we say that **identifiability** is when the above equation admits only **one** solution. Since AR has no MA part ($C(z)=1$), explicit minimization is possible 

$$\hat{\vartheta}_N=\left(\sum_{t=1}^N\varphi(t)\varphi^T\left(t\right)\right)^{-1}\sum_{t=1}^Ny(t)\varphi(t)$$

In the above formula, the crucial thing is that the matrix  $\bar R =\left[\sum_{t=1}^N\varphi(t)\varphi^T\left(t\right)\right]^{-1}$ is computable. A necessary condition for the matrix to be positive semi-definite and also invertible (and so that the normal equations have a unique solution) is that the input $u(t)$ is **persistently exciting** of order $k$ which is a property which interest the $k \times k$  matrix:

 $$\begin{vmatrix}\gamma_{uu}(0)&\gamma_{uu}(1)&\gamma_{uu}(2)&\cdots\\\gamma_{uu}(1)&\gamma_{uu}(0)&\gamma_{uu}(1)&\ddots\\\vdots&\ddots&\ddots&\ddots\end{vmatrix}$$

This matrix must be invertible $\leftarrow$ is a necessary condition to make invertible $\bar R$ . 


## Identification of ARMA(x): Maximum Likelihood method


ARMA (or ARMAX)  identification differs from AR (or ARX) because there is no more linearity in the parameters.

This non-linear dependence was the reason for the use of the least squares formula in the past due to its linear dependence on the coefficients $a$ and $b$. However, since theta contains the coefficient of $c$, the optimization problem becomes non-linear, requiring numerical approaches for solution.

Traditional methods like Least Squares cannot be directly applied, we must use an iterative numerical method such as gradient descent can be used with an initial estimate $\theta_1$ and an update rule that computes the next guess for $\theta$ based on the previous iteration.
This update rule comes in 3 main main flavors in this course: 

- **Newton's rule**:

$$\theta^{(1+1)}=\theta^{(1)}-[\text{Hessian}]^{-1}\cdot \text{gradient}$$
- **Gradient descent**: in NN gd is more effective since the are a lot of parameters.

$$\theta^{(1+1)}=\theta^{(1)}-\eta\cdot \text{gradient}$$

- **Quasi-Newton's rule**: in the system identification framework, it's often used quasi-newton's rule.

$$\theta^{(i+1)}=\theta^{(i)}-[\frac{\text{aproximate}}{\text{Hessian}}]^{-1}\text{gradient}$$

#### Newton's rule

Newton method basically involves taking the quadratic expansion of the cost function and finding the minimum of the resulting paraboloid to update the parameter.


![](images/Pasted%20image%2020240315154109.png)


The speaker discusses the second term in the Newton's method, which is often neglected due to it potentially being ne andgative small when close to the optimal solution. The matrix involved in the method, Hessian, is positive semi-definite, ensuring the method moves towards the minimum. The second term's negligibility arises from being small near the solution and the Hessian's positive semi-definiteness, making the Newton's method safer. 

The formula for the Newton's method is rewritten using the gradient and the action of the cost function. The matrix dimensions are confirmed as consistent, resulting in a column vector representing the variation of theta with respect to theta i.


 The derivative of the error function epsilon with respect to the parameter theta is required to compute the gradient for optimization. While we have an expression for epsilon, the derivative is unknown. We need to find the derivative of epsilon with respect to each coefficient in theta, including a, m, b, and c. We write epsilon as an explicit function of these coefficients and compute the derivatives in classes: first for the parameters characterizing a, then for b, and finally for c. The derivative of epsilon with respect to a one is z - 1 multiplied by (a - 1) over c times y. We define a new signal, alpha t, as -1/c * y, and the derivative of epsilon with respect to a one is equivalent to alpha t - 1.


The fact that Hessian is $\ge 0$ is good in terms of "safety" of the method

![](images/Pasted%20image%2020240315161420.png)

The second term is likely to be "small" close to the minimum (at the end of the iterations)

and if we neglect the second order derivative term we also have



$$\frac{\partial^2J(\vartheta)}{\partial\vartheta^2}=\frac1N\sum_{t=1}^N\varphi_\vartheta(t)\varphi_\vartheta(t)^T$$

so the new iteration formula is

$$\vartheta^{(r+1)}=\vartheta^{(r)}-\left[\frac1N\sum_{t=1}^N\varphi_\vartheta(t)\varphi_\vartheta(t)^T\right]^{-1}\left[\frac1N\sum_{t=1}^N\varphi_\vartheta(t)\varepsilon_\vartheta(t)\right]$$

![](images/Pasted%20image%2020240315161623.png)



![](images/Pasted%20image%2020240315161942.png)



#### GD and Quasi-newton

As said before, Newton is one way to solve a numerical optimization algorithm, but there are other methods as well.

$$\theta^{(i+i)}=\theta^{(i)}-[\text{hessian}]^{-1}-\text{gradient}$$


while the gradient descent:

$$\theta^{(i+1)}=\theta^{(i)}-\eta \cdot \text{gradient},$$

Same philosophy since both moves in the directions where the right direction.

The newton's rule depending on the region you are "moves" smarter and uses less iterations ( so it's more computational efficient).

The quasi newton is computationally lighter than Newton's (we neglect a part) but at the same time is more accurate than the gradient descent method.

$$Q^{(i+1)}=\theta^{(i)}-[\text{approximate Hession}]\cdot \text{gradient}$$


 Quasi-Newton methods, such as BFGS, use an approximate Hessian to strike a balance between accuracy and computational efficiency. These methods are safer because they avoid the issue of encountering negative definite conditions and require fewer computations than Newton's method. However, they may introduce some inaccuracy in the computation of the Hessian at the beginning.

In optimization, gradient descent is a common method used to find the minimum of a function. The gradient tells you the direction and the magnitude of the next step. Variants of gradient descent exist, such as **time-varying** ones, but in the context of system identification with a limited number of parameters, the quasi-Newton rule is typically used. 

## Non-parametric identification 

So far if we want to infer something about the underling stochastic process we need to learn the model first and then make a **parametric identification**. 

Now let's try a non-parametric one directly estimating from data $m_y$, $\gamma _y(\tau)$ and $\Gamma_y(\omega)$ without first identifying a full model of $W(z)$ . 

But before diving in the estimators, we we need to define what we mean by "good estimator". We will use two definitions: 

- **Correctness:** An estimator is considered correct (or unbiased) if its expected value is equal to the true parameter it is estimating, indicating that the mean of many independent estimates would converge to the true parameter. $E[{\hat{\mu}}_n] = \mu$ where $\mu$ is the real mean. 
- **Consistency:** An estimator is consistent if the probability of the estimates being close to the true parameter increases as the sample size grows, meaning that with more data, the estimates become progressively more precise. $Var[\hat s_N ] \rightarrow 0$ as $N \rightarrow \infty$ . 

Now the estimators:

- **Estimation of Mean**: 
The most natural estimator is $\hat{\mu}_n$, where $n$ refers to the number of samples.

$$\hat\mu_n=\frac1N\sum_i^N y(i)$$

This estimator grows more informative with additional data, capturing the dynamics of ARMA processes without relying on full historical memory.


- **Estimation of Covariance**: 

$$\gamma_{y}(t)=\frac{1}{N}\sum_{t=1}^{N}y(t)y(t-\tau)$$

The sample covariance function $\hat{\gamma}_n(\tau)$ becomes more reliable as the sample size increases, especially when considering lags $\tau$ significantly smaller than the sample size. For negative lags, adjustments are made to the estimator to maintain accuracy, emphasizing the importance of having enough data points to estimate statistical averages accurately.


- **Estimation of Spectrum**: 

We can estimate this:
$$\Gamma_{y}(w)=\sum_{z=-\infty}^{\infty}\gamma(z)e^{-jwz}$$
with this: 

$$\hat{\Gamma}_{N}(w)=\sum_{\tau=-(N-1)}^{N-1}\hat{\gamma}_{N}(\tau)e^{-jwz}$$

Estimating the spectrum $\hat{\Gamma}_N(\omega)$ is complex as it is derived from the covariance function, making it an indirect estimation. This estimator is not initially correct but becomes asymptotically correct with a large dataset, although it may exhibit bias at different frequencies.

$$\mathbb{E}[(\hat{\Gamma}_{N}(\omega)-\Gamma_{\gamma}(\omega))^{2}] \xrightarrow[n\to\infty]{} \Gamma(\omega)^2$$

Consistency in this context is challenging, as it's shown that the error variance does not tend to zero even with infinite samples.

This issue is addressed through averaging or regularization, dividing the dataset into parts and averaging the estimators, which reduces variance at the cost of potentially losing signal fidelity.


- **Estimation of Prediction Error Variance**: 

$$J_k(a)=\frac1k\sum_{t=0}^{t=k}\left(y(t+1)-\hat{y}(t+1|t)\right)^2$$

Using measured samples, one can estimate the prediction error variance through $J_k(a)$, which evaluates the performance of the predictive model against actual outcomes, with the goal of minimizing this variance for better predictions.



Now the question becomes: suppose that we estimated $\theta^N$ , with $N$ samples of data and that the real data generator has $\theta ^O$ parameters. 

$$J_N(\vartheta)\xrightarrow[N\to\infty]{}[\bar{J}(\vartheta)=\mathbf{E}[\varepsilon_\vartheta(t)^2]$$

It can be easily seen, since $\bar{R} > 0$ that the minimum of $\bar{J}(\vartheta)$ is reached for $\vartheta=\vartheta^{\circ}.$ 
This implies that in such a situation

$$\min\{J_N(\vartheta)\}\xrightarrow[N\to\infty]{}\min\{\bar{J}(\vartheta)\}$$

Asymptotically we will converge to the minimum of the asymptotic cost. 

$\theta ^o$ is "the description of the real system" 

$$\theta^{o} \in \Delta$$

Prediction error minimization guarantees that $M(\hat \theta _N) \xrightarrow[n \rightarrow \infty]{} S$ 

The theorem consists in prove that the minimum of the asymptotic cost (which is guarantee to converge to) corresponds to the set of parameters that describe the real system. 

$$\hat \theta _N \xrightarrow[N\to\infty]{} \theta ^ o$$


This theorem is very, very significant. 
If the system that you want to model and learn from data is inside the model set, if you minimize the **variance of the prediction error**, asymptotically so with a large data set, you will converge exactly to that system. 

So it means that the prediction error algorithm that we are using in this course is actually good, meaning that they will lead you to the right model (at least in this very ideal case where the system is in the model set). 




### Data preprocessing


What if $y(t)$ is non-stationary? So these two are possible causes of non-stationarity:

- trend 
- seasonality 


To work with non-stationary processes, we first need to estimate possible trends or seasonalities. Then we can remove them and work with the reminder **SSP (Stationary Stochastic Process)**.


#### Trend removal

$$y(t)=\tilde y (t) + kt +m$$

So in order to work with $\tilde y(t)$ first we need to estimate $k$ and $m$. 
So we first estimate m and k, characterizing the trend.
Then we remove the trend from the data set. We have a corresponding data set for this stationary stochastic process. And then with that data, we can do whatever we want. 

I can actually say that I expect this quantity to be equal to zero on average. 

$$\mathbb{E}[y(t)-kt-m]=\mathbb{E}[\tilde{y}(t)]=0$$

Inspired by the above equality, we can find $\hat{m}$ and $\hat{k}$ as the argument of the minimum with respect to $m$ and $k$ :

$$(\hat{m},\hat{k})=\operatorname{argmin}_{m,k}\frac{1}{N} \sum_{t=1}^{N}\left(y(t)-k(t-m)\right)^2$$


This minimization problem we formulated is precisely a least squares problem. We're trying to minimize the **cost**, or squared error, between the observed data and the fitted trend line. Since the error term, `y(t) - k(t-m)`, is linear in both `k` and `m`, squaring it results in a squared error function. 

Geometrically, this squared error function typically forms a paraboloid in the space of `k` and `m`. Fortunately, for paraboloids, we expect there to be a single **global minimum**. Just like with the ordinary least squares formula used for ARX processes, we can find this minimum by taking the derivative of the squared error function with respect to `k` and `m` and setting the gradient equal to zero. This will provide us with the optimal estimates `k` and `m` that characterize the underlying trend in the data.

#### Seasonality


$$y(t)=\tilde{y}(t)+s(t)$$


where $s(t)=s(t+k \mathbb{T})$ where $\mathbb{T}$  is the period.
In the same way we need to estimate $s(t)$ . 
The underlying idea is :

$$\hat{S}(t)=\frac{1}{M}\sum_{k=0}^{M-1}y(t+hT)=\frac1M\sum_{k=0}^{M-1}\widetilde{y}(t+hT)+\frac1M\sum_{k=0}^{M-1}S(t+hT)$$
But remember that we had also to estimate $T$ period. 
