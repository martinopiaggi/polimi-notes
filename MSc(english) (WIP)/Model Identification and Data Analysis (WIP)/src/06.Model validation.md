
# Model validation


Given an optimal model $M(\hat \theta _n)$ the goal is to validate its optimality.

After computing $\hat{\theta}_N$, we have $J_N(\hat{\theta}_N)^{(n)}$ for a model of order $n.$ How can we select the best value of $n, $namely the best model complexity?
## Model order selection

4 possible scenarios:

- $S \in M(\theta)$ and $\Delta$ is a singleton : for $n \to \infty$ $M(\hat \theta _n)$ will converge to $S$. The identification problem is properly defined, ensuring a singular, accurate solution that aligns with the system's dynamics.
- $S \in M(\theta)$ and $\Delta$ is not a singleton  : for $n \to \infty$ $M(\hat \theta _n)$ will not necessary converge to $S$, but it's guarantee $\hat{\vartheta}_N$ tends to one of the values in $\Delta$ (multiple optimal minima subset). This most scenarios case, where multiple models equally represent the system. 
-  $S \notin M(\theta)$ and  $\Delta$ is a singleton: for $n \to \infty$ $M(\hat \theta _n)$ will converge to $M(\theta _n ^{*})$.  In this case the model with $\hat{\vartheta}_N$ is the best proxy of the true system in the selected family. This case is when there is an insufficiency of the model set to capture the system's complexity.
-  $S \notin M(\theta)$ and  $\Delta$ is not a singleton: for $n \to \infty$ $M(\hat \theta _n)$ no guarantees.  In this case the model with $\hat{\vartheta}_N$ tends to one of the best proxies of the true system in the selected family.


Remember that $J_n^{n}(\theta)$ in function of $n$ is always non increasing but it's not the optimal predictor! We can't simply continue to use more parameters, we would over fit it. 

![](images/Pasted%20image%2020240321180538.png)

### Whiteness test on residuals

If the model obtained through the identification process is actually the true model (i.e. $\mathbb M(\hat \theta) = S$) then, the prediction error is a white noise. The Anderson Whiteness Test allows to conclude this on a probabilistic basis.

![](images/Pasted%20image%2020240321180948.png)

Actually this method is not robust and it's not used. Naive approach: compute the performance index $J_N$ for multiple increasing values of $n$ until we find the best performance

### Cross Validation

divide the dataset with identification and validation set; use the former to build the model and evaluate the performance index using the validation set (we are
 wasting some of the data, because they canno be used in the identification process)

Cross validation consists into dividing the dataset with identification and validation set; use the former to build the model and evaluate the performance index using the validation set. 


 Obviously we are "wasting" some of the data to build the validation test, since they cannot be used in the identification process.


### Identification with model order penalties


- **Final Prediction Error (FPE):** FPE estimates the error of a model on a new dataset. It attempts to find a model that minimizes prediction error on unseen data. FPE takes into account both the training error and the model complexity.
- **Akaike Information Criterion (AIC):** AIC is another model selection criterion that balances goodness of fit with model complexity. It's based on information theory and aims to find the model that best explains the data with the fewest parameters.
- **Regularization:** Methods like L1 (Lasso) and L2 (Ridge) regularization add a penalty term to the model's cost function. This penalty term is proportional to the size of the model's coefficients. Regularization helps prevent overfitting by favoring simpler models that are less likely to memorize the noise in the training data.

**The Connection**

FPE, AIC, and regularization all address the problem of overfitting by seeking this balance between model fit and complexity:

- **FPE and AIC as Guides:** Both FPE and AIC can be used to select the best model from a set of candidate models, potentially including models with different regularization strengths. The model with the lowest FPE or AIC is often considered the optimal one.
- **Regularization's Influence:** The strength of regularization directly impacts the complexity of the model. By adjusting the regularization parameter, you can control the trade-off between fit and complexity. FPE and AIC can help you find the optimal regularization strength.

**In Summary**

FPE and AIC are tools for model selection that help prevent overfitting. Regularization methods directly control model complexity to combat overfitting. FPE and AIC can be used in conjunction with regularization to find the optimal level of model complexity that leads to the best generalization performance.



## FPE (Final Prediction Error)

Identification with model order penalties 
$$FPE=\frac{N+n}{N-n}J_N(\hat{\theta}_N)^{(n)}$$

We are giving a penalty to the models with high complexity. The FPE functions is not monotonically decreasing, and the complexity corresponding to its minimum value can be chosen as complexity of the model 

· Final Prediction Error: $FPE=\frac{N+n}{N-n}J_N(\hat{\theta}_N)^{(n)}$
We are giving a penalty to the models with high complexity. The FPE functions is not monotonically decreasing, and the complexity corresponding to its minimum value can be
 chosen as complexity of the model

## AIC (Akaike Information Criteria)


**Akaike Information Criterion** is:

$$AIC=\frac{2n}{N}+\ln \left( J_N(\hat{\theta}_N)^{(n)} \right)$$


The first term is the one regarding the complexity of the model, while the second is the one regarding the fitting of the data
For high values of N, this is equivalent to FPE.

AIC and FPE return the same optimal model order. 


## MDL (Minimum Description Length)


$$MDL=\ln (N) \frac n N + \ln \left( J_N(\hat{\theta}_N)^{(n)} \right)$$
The model order is penalized more by MDL since $ln(N) > 2$ . Since in general case $S \notin M(\theta)$ we usally prefer to (slightly) overfit and then we use AIC.  



· Minimum Description Length: $MDL=\frac nN\ln N+\ln J_N(\hat{\theta}_N)^{(n)}$
Asymptotically is simliar to AIC but with lower penalization $\to $MDL leads to more parsimonious models





