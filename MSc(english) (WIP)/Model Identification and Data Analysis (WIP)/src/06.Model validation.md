
# Model validation

Given an optimal model $M(\hat \theta _n)$ the goal is to validate its optimality also considering its model order $n.$ 
## Model order selection

How can we select the best value of $n$ namely the best model complexity?
4 possible scenarios:

- $S \in M(\theta)$ and $\Delta$ is a singleton : for $n \to \infty$ $M(\hat \theta _n)$ will converge to $S$. The identification problem is properly defined, ensuring a singular, accurate solution that aligns with the system's dynamics.
- $S \in M(\theta)$ and $\Delta$ is not a singleton  : for $n \to \infty$ $M(\hat \theta _n)$ will not necessary converge to $S$, but it's guarantee $\hat{\vartheta}_N$ tends to one of the values in $\Delta$ (multiple optimal minima subset). This most scenarios case, where multiple models equally represent the system. 
-  $S \notin M(\theta)$ and  $\Delta$ is a singleton: for $n \to \infty$ $M(\hat \theta _n)$ will converge to $M(\theta _n ^{*})$.  In this case the model with $\hat{\vartheta}_N$ is the best proxy of the true system in the selected family. This case is when there is an insufficiency of the model set to capture the system's complexity.
-  $S \notin M(\theta)$ and  $\Delta$ is not a singleton: for $n \to \infty$ $M(\hat \theta _n)$ no guarantees.  In this case the model with $\hat{\vartheta}_N$ tends to one of the best proxies of the true system in the selected family.

Remember that $J_n^{n}(\theta)$ in function of $n$ is always non increasing but it's not the optimal predictor! 

![](images/Pasted%20image%2020240321180538.png)

So in general an $ARX(n, n)$ is a better fit to the data than an $ARX(n − 1, n − 1)$ model, but we can't simply continue to use more parameters, we would **overfit** it. 

### Whiteness test on residuals

If we simply compute the performance index for multiple increasing values of $n$ we will see that is monotonically decreasing with $n$: we can use the whiteness test on residuals to understand when "it's enough" and that further decreasing will not reflect in better performance for new unseen data (overfit).


![](images/Pasted%20image%2020240321180948.png)

Actually this method is not robust and it's not used. 

### [Cross-validation](Machine%20Learning/src/05.Model%20Evaluation.md#Model%20Evaluation)

In cross-validation, the dataset is partitioned into an **identification set**, which is used to construct the model, and a **validation set**, which is utilized to assess the model's performance. 
The validation portion of the data is "reserved" and not used in the model-building phase, potentially leading to data "wastage".

### Identification with model order penalties

For better model selection that help prevent over-fitting we can directly control model complexity.

#### FPE (Final Prediction Error)

Identification with model order penalties 
$$FPE=\frac{N+n}{N-n}J_N(\hat{\theta}_N)^{(n)}$$

We are giving a penalty to the models with high complexity. The FPE functions is not monotonically decreasing, and the complexity corresponding to its minimum value can be chosen as complexity of the model.

#### AIC (Akaike Information Criteria)


$$AIC=\frac{2n}{N}+\ln \left( J_N(\hat{\theta}_N)^{(n)} \right)$$
For high values of $N$, this is equivalent to $FPE$.

#### MDL (Minimum Description Length)

Asymptotically is similar to AIC but with higher penalization since $ln(N) > 2$ . 
$$MDL=\ln (N) \frac n N + \ln \left( J_N(\hat{\theta}_N)^{(n)} \right)$$
In general case $S \notin M(\theta)$ we usually prefer to (slightly) overfit, so generally AIC is preferred.  










