
# Model validation


After computing $\hat{\theta}_N$, we have $J_N(\hat{\theta}_N)^{(n)}$ for a model of order $n.$ How can we select the best value of $n, $namely the best model complexity?

· Naive approach: compute the performance index $J_N$ for multiple increasing values of $n$ until we find the best performance

· Cross Validation: divide the dataset with identification and validation set; use the former to build the model and evaluate the performance index using the validation set (we are
 wasting some of the data, because they canno be used in the identification process)

Cross validation consists into dividing the dataset with identification and validation set; use the former to build the model and evaluate the performance index using the validation set. 


 Obviously we are "wasting" some of the data to build the validation test, since they cannot be used in the identification process.


Given an optimal model $M(\hat \theta _n)$ the goal is to validate its optimality.

Now the question becomes: suppose that we estimated $\theta^N$ , with $N$ samples of data and that the real data generator has $\theta ^O$ parameters. 

$$J_N(\vartheta)\xrightarrow[N\to\infty]{}[\bar{J}(\vartheta)=\mathbf{E}[\varepsilon_\vartheta(t)^2]$$

It can be easily seen, since $\bar{R} > 0$ that the minimum of $\bar{J}(\vartheta)$ is reached for $\vartheta=\vartheta^{\circ}.$ 
This implies that in such a situation

$$\min\{J_N(\vartheta)\}\xrightarrow[N\to\infty]{}\min\{\bar{J}(\vartheta)\}$$

Asymptotically we will converge to the minimum of the asymptotic cost. 

$\theta ^o$ is "the description of the real system" 

$$\theta^{o} \in \Delta$$

Prediction error minimization guarantees that $M(\hat \theta _N) \xrightarrow[n \rightarrow \infty]{} S$ 

The theorem consists in prove that the minimum of the asymptotic cost (which is guarantee to converge to) corresponds to the set of parameters that describe the real system. 

$$\hat \theta _N \xrightarrow[N\to\infty]{} \theta ^ o$$


This theorem is very, very significant. 
If the system that you want to model and learn from data is inside the model set, if you minimize the **variance of the prediction error**, asymptotically so with a large data set, you will converge exactly to that system. 

So it means that the prediction error algorithm that we are using in this course is actually good, meaning that they will lead you to the right model (at least in this very ideal case where the system is in the model set). 





The system is in the selected family of models and $\bar{J}$ has a unique minimum. In this case, as $N\to\infty$ we have that $\hat{\vartheta}_N\to\vartheta^\circ$
 
 The system is in the selected family of models and $\bar{J}$ has a set of minimum values $\Delta$. In this case, as $N\to\infty$ we have that $\hat{\vartheta}_N$ tends to one of the values in $\Delta$. 
 
 
The system is not in the selected family of models and $\bar{J}$ has a unique minimum. In this case the model with $\hat{\vartheta}_N$ is the best proxy of the true system in the selected family.

The system is not in the selected family of models and $\bar{J} has a set of minimum values $\Delta$. In this case the model with $\hat{\vartheta}_N$ tends to one of the best proxies of the true system in the selected family.


## Model order selection

4 possible scenarios:

- $S \in M(\theta)$ and $\Delta$ is a singleton : for $n \to \infty$ $M(\hat \theta _n)$ will converge to $S$ 
- $S \in M(\theta)$ and $\Delta$ is not a singleton  : for $n \to \infty$ $M(\hat \theta _n)$ will not necessary converge to $S$, but it's guarantee to one of the possible optimal solutions (multiple optimal minima subset).

-  $S \notin M(\theta)$ and  $\Delta$ is a singleton: for $n \to \infty$ $M(\hat \theta _n)$ will converge to $M(\theta _n ^{*})$ 
-  $S \notin M(\theta)$ and  $\Delta$ is not a singleton: for $n \to \infty$ $M(\hat \theta _n)$ no guarantees



In the first case, the identification problem is properly defined, ensuring a singular, accurate solution that aligns with the system's dynamics. However, in most scenarios, challenges arise either due to the presence of multiple models that equally represent the system or the insufficiency of the model set to capture the system's complexity. 

Despite these issues, one might still find the results satisfactory from a practical standpoint. It's possible to evaluate the model's effectiveness by examining various factors and conducting different experiments. This can help determine if the model, although not perfectly representing the system, is capable of predicting future outputs accurately enough for practical use. This approach is commonly adopted in practice. However, it is not the ideal path. The preferred approach is to ensure the most diligence in designing the identification process and in choosing the model set for optimal modeling. This involves revisiting the procedure and considering aspects like model order, which are often overlooked. Typically, the model's parameters (theta) are estimated by minimizing the prediction error, assuming a fixed polynomial length in the model.



$J_n^{n}(\theta)$ in function of $n$ is always non increasing but it's not the optimal predictor! 
We can't simply continue to use more parameters, we would over fit it. 


![](images/Pasted%20image%2020240321180538.png)



### Whiteness test on residuals


If the model obtained through the identification process is actually the true model (i.e. M( ˆθ) = S) then, the prediction error is a white noise. The Anderson Whiteness Test allows to conclude (on a probabilistic basis) on the whiteness of the prediction error.



Whiteness Test allows you to have a probability about the prediction error is a $WN$. 

$$\epsilon(t/t-1,\hat{\theta}_{n}^{(n)})=y(t)-\hat{y}(t/t-1,\hat{\theta}_{n}^{(n)})$$
is $\epsilon$ is a realization of a $WN$ ? 

![](images/Pasted%20image%2020240321180948.png)


Actually not robust and not used. 

## FPE (Final Prediction Error)

Identification with model order penalties 
$$FPE=\frac{N+n}{N-n}J_N(\hat{\theta}_N)^{(n)}$$

We are giving a penalty to the models with high complexity. The FPE functions is not monotonically decreasing, and the complexity corresponding to its minimum value can be chosen as complexity of the model 

· Final Prediction Error: $FPE=\frac{N+n}{N-n}J_N(\hat{\theta}_N)^{(n)}$
We are giving a penalty to the models with high complexity. The FPE functions is not monotonically decreasing, and the complexity corresponding to its minimum value can be
 chosen as complexity of the model

## AIC (Akaike Information Criteria)


**Akaike Information Criterion** is:

$$AIC=\frac{2n}{N}+\ln J_N(\hat{\theta}_N)^{(n)}$$


The first term is the one regarding the complexity of the model, while the second is the one regarding the fitting of the data
For high values of N, this is equivalent to FPE.

AIC and FPE return the same optimal model order. 


## MDL (Minimum Description Length)


$$MDL=\ln \left( J_N(\hat{\theta}_N)^{(n)} \right) + \ln (N) \frac n N$$
The model order is penalized more by MDL since $ln(N) > 2$ . Since in general case $S \notin M(\theta)$ we usally prefer to (slightly) overfit and then we use AIC.  





· Minimum Description Length: $MDL=\frac nN\ln N+\ln J_N(\hat{\theta}_N)^{(n)}$
Asymptotically is simliar to AIC but with lower penalization $\to $MDL leads to more parsimonious models



