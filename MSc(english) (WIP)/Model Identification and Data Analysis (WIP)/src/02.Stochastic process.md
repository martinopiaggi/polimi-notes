
# Stochastic process

The concept of a stochastic process is important when studying data, whether it's a time series or the output of a dynamical system. The purpose of making predictions on the future behavior of a signal requires taking into account the uncertainty in the model. 

This leads us to the need for a new concept, the stochastic process, which is an infinite sequence of variables that depend on the outcome of a random experiment.


A stochastic process cannot be directly visualized because it's an infinite sequence of possible observations. Instead, we deal with a finite sequence of observed data. The stochastic process can be thought of as accounting for all possible observations. Every experiment and subsequent data obtained represent a realization of the stochastic process, which is a small portion of the infinite sequence associated with another outcome of the random experiment.

We can view the stochastic process in two ways: 

- The first way is to fix the outcome of the random experiment and observe one sequence of data over time. 
- The second way is to fix the time and observe the value of the process at that time, which depends on the random experiment's outcome. The distribution of possible values for the process at a given time is induced by the random experiment itself.

A stochastic process can be observed in both the time domain and the domain of the random variable, with the value at any given time taking on any of the possible values for that particular stochastic process. The observed value depends on the data available.


Stationary processes, statistical systems, and estimation theory. This foundation will enable us to study prediction, the first accomplishment and a crucial step towards identification. Prediction and identification are related as we need a model to make predictions, but we also need to know the predictions to understand identification methods. 










Uncertainty is represented as a white noise signal in the model. 




[00:16:10] The speaker discusses the expected value and mean of a stochastic process at different time points, t and t2. For every time point, they calculate the expected value of v of t, taking into account all possible outcomes. The function of t represents the most likely value of v of t. To understand the relationship between values at different times, the concept of auto covariance function or gamma of t1, t2 is introduced. It is the expected value of the product of the deviations of v of t1 and v of t2 from their respective means. This function reveals any correlation between the values of the stochastic process at different times.

The auto-covariance function measures the relationship between a variable and a delayed version of itself within the same process. When comparing variables from different processes, it's called the covariance function, with auto-covariance being a special case of self-comparison. 



The speaker discusses the correlation coefficient of a stochastic process, which is calculated using the normalized covariance function. The correlation coefficient is always less than 1 and reaches a maximum of 1 when comparing a value with itself. The minimal correlation is 0, and a value of -1 indicates a strong correlation. A stationary process, also known as a strongly stationary process, is a type of stochastic process where the probabilistic characteristics remain constant over time. A weaker form of stationarity, called weak stationarity, only requires the mean and covariance function to be stationary.

The speaker discusses stationary processes, which have two properties: the mean and covariance function remain constant when time is shifted. The covariance function depends on the difference between two time indices, simplifying the formulation. Stationary processes are expected to have a constant expected value and variance, which are independent of time shifts. Processes with trends or varying steady states are not stationary. The speaker provides examples of non-stationary processes based on different realizations. In each example, the expected value may vary, but the covariance function should be constant for stationarity.

[00:56:27] This excerpt discusses the concept of stationary and non-stationary processes using the example of a coin flip process. The process is not stationary because its variance is not constant over time. The expected value is zero, but the variance changes depending on the outcome of the random experiment. A Gaussian process is a type of process characterized by its probability distribution functions, which are fully determined by the mean and variance. If a Gaussian process is both Gaussian and weak stationary, it is also strongly stationary. The concept of ergodic processes is also mentioned, but not fully explained.












A stochastic process $SP$ is a $n$ infinite sequence of random variables, all defined on the same probabilistic space. 

$SP$ extends the concept of random variables to signals. 


Different time signals if realizations of the same stochastic process are said to be stochastically equivalent. 

A widesense characterization (mean and covariance function) is enough in practical applications to describe $SP$s.

$$m(t)=E[v(t,s)]=\int_{P}v(t,s)pdf(s)ds$$

Where $v(t,s)$ is the value of the $SP$ at time $t$ for a particular **realization** $s$. The function $pdf(s)$ is the probability density function of the random variable $s$, and the integral is taken over the entire probability space $P$.

So basically we are making the mean of "multiple signals" (actually multiple/different realizations).

And in the same way also the **covariance function**:

$$\gamma(t_1,t_2)=E[(v(t_1,s)-m(t_1))(v(t_2,s)-m(t_2))]$$

Note that the covariance function is the covariance of two samples of the **same** stochastic process.




A stationary $SP$ is a $SP$ with: 

- $m(t) = m \space \forall t$ "any realization will be a constant outcome"
- $\gamma(t_1,t_2)$ depends on $\tau = t_2 - t_1$   which means "it's not important when you take the samples but only the absolute shift between the two"

Regarding the covariance in a **stationary** $SP$ :

- $\gamma(0)=E[(v(t,s)- m)^{2}]\geq 0$ positivity
- $|\gamma(\tau)|\leq\gamma(0),\forall\tau$  not increasing
- $\gamma(\tau)=\gamma(-\tau),\forall \tau$ even



A particular interesting $SSP$ is called **white noise** with mean $\mu$ and variance $\lambda ^2$  and with covariance function $\gamma(\tau)=0$ which means that no variables are correlated. 





### WHite noise



A simple stochastic process that is stationary is the white noise process, defined as a sequence of uncorrelated random variables with zero mean and constant variance.


White noise is a process consisting of independent, identical distributed random variables, each with the same distribution and zero mean. Its defining characteristic is having a covariance function of 0 for all time differences, making it unpredictable and uncorrelated to past values. The process is named for its "white" spectral density, and it's represented as a white noise process with expected value 0 and variance λ² in mathematical notation. White Gaussian noise is a specific type of white noise, where each sample has a Gaussian distribution.


## Moving averages

$e(t)\sim wn(0,\lambda^2)$ we can define the moving average process of order $n$ $MA(n)$ in this way: 

$$y(t)=c_0\cdot e(t)+c_1e(t-1)+c_2\cdot e(t-2)+\cdots+c_ne(t-n)$$



$MA(n)$ is still stationary . A generalization of $MA(n)$ is:

$$MA(\infty)=\sum_{i=0}^{+\infty}c_ie(t-i), \space e(t)\sim\mathrm{wn}(0,\lambda^{2})$$

but the serie must converge! 



$$m_y(t)=m_y(0)=0$$

$$\gamma_{_y}(\tau)=\begin{cases}\left({c_0}^2+{c_1}^2+{c_2}^2+...+{c_n}^2\right)\cdot\lambda^2&\text{if }\tau=0\\\left({c_0}c_1+{c_1}c_2+...+{c_{n-1}}c_n\right)\cdot{\lambda^2}&\text{if }\tau=\pm1\\\left({c_0}c_2+{c_1}c_3+...+{c_{n-2}}c_n\right)\cdot{\lambda^2}&\text{if }\tau=\pm2\\\vdots\\\left({c_0}c_n\right)\cdot{\lambda^2}&\text{if }\tau=\pm n\\0&\text{if }\tau>\pm n\end{cases}$$


So for example, for $y(t)=e(t)+c\cdot e(t-1)$:

$$\gamma_y(0) = 1+ c^2$$

$$\gamma_y(1) = \gamma_y(-1) = c$$



---


Example of $MA(2)$ (always with $e(t) \sim (0,1)$ ) : 

$$y(t)=e(t)+\frac{1}{2}e(t-1)+\frac{1}{4}e(t-2)$$





Autoregressive models will allow us to have $\gamma(\tau) \ne 0$ for all $\tau$ using a finite set of coefficients. 


$$y(t)=a_1y(t-1)+a_{2}y(t-2)+\cdots+a_{m}y(t-m)+e(t)$$

with $e(t) \sim wn(0,\lambda ^2)$ . 







# ARMA


We're discussing classes of models for stochastic processes in this session. So far, we haven't dealt with prediction or learning, but rather introducing tools for describing random signals. These signals depend not only on time but also on the outcome of a random experiment. We've seen that stationary stochastic processes can be described using probability distributions, but it's not practical for computations.

Instead, we focus on stationary stochastic processes with covariance functions characterized by a finite set of coefficients. We introduced the white noise model, which generates signals with any shape of covariance functions, but handling an infinite amount of coefficients is impractical.

To avoid this, we look at autoregressive (AR) models, which allow for non-zero covariance functions for infinite lags but are characterized by a finite set of coefficients. We've only defined this class of models, and the key trick is taking a moving average of past output values instead of past white noise values while keeping the noise in the definition to maintain stochasticity.

AR processes model stationary signals as the steady-state solution of a difference equation, which enforces the output to be a function of the initial condition and the force moving it. Removing the moving force would result in non-stationary signals that depend on both initial conditions and time. By focusing on the steady-state solution, we can get stationary stochastic processes.

We presented a simple example of a stationary AR process in our previous discussion.

An ARMA process is a combination of Autoregressive (AR) and Moving Average (MA) models. ARMA processes allow for both recurrent terms based on **past outputs** and input terms based on **past errors**. The order of the AR and MA parts are denoted by $m$ and $n$, respectively. ARMA processes include all other AR and MA models as special cases. Additionally, ARMA models can be extended to include **exogenous inputs** by modeling the signal as a filtering of the inputs. For example, an acceleration signal can be modeled as a filtering of engine torque or pedal position.

The concept of $e(t)$ is a mathematical artifact representing the stochastic nature of measurements. In reality, $e(t)$ does not exist, but when  is the output of a system, both deterministic and stochastic effects need to be considered. The deterministic effect is caused by an input U, while the stochastic effect is due to the **inherent errors in measurements**. 


To accurately model Y, both effects should be combined as functions of both U and E, resulting in the larger class of ARMA models that includes deterministic and stochastic components.

The key question is ensuring that the mathematical models correspond to the stationary processes used.

$$\begin{aligned}y(t)&=a_1y(t-1)+a_2y(t-2)+\cdots+a_my(t-m)+\\&+c_0t(t)+c_1e(t-1)+\cdots\cdots c_n e(t-n)+\\&+b_0u(t-k)+b_{1}u(t-k-1)+\cdots b_{p}u(t-k-p)\end{aligned}$$


ARMAX($m,n,p,k$) with $k$ pure input/output (I/O) delay. $p$ order of the exogenous part. 

There is also N-ARMAX (non linear) where Unlike linear Armax models, no linear combinations or weighted averages are used, and f can be any non-linear, parametric function. Modeling and learning this type of model in system identification is complex due to the added complexity of the dynamic system and time axis, and preserving formal properties. 






The question at hand is whether AR, ARMA, and ARMAX models are stationary. To answer this, we use **operatorial representation** 


Backward shift operator $z-1$ which shifts a signal back in time

the forward shift operator $z$  which shifts a signal forward in time.

Linear properties of $z$ and $z-1$ include linearity, recursive application, and linear composition. Using these properties, we can rewrite an AR model equation in terms of $z$ and $z-1$, revealing the relationship between the operators and the AR model.

The resulting equation shows that the output Yt is the ratio of two polynomials of Z, where C(Z) is the polynomial at the numerator and 1 - A(Z) is the polynomial at the denominator.

$$y(t)=\frac{c_{0}+c_{1}z^{-1}+\cdots c_{n}z^{n}}{1-a_{1}z^{-1}+\cdots a_{m}z^{-m}}e(t)=\frac{C_{(z)}}{A(z)}e(t)=W(z)e^{it}(t)$$

The speaker discusses the concept of a discrete time transfer function, represented by the ratio of two polynomials $W(z)$.

This transfer function acts as a digital filter, transforming white noise into an armor process. The transfer function can be used to analyze stationary processes using digital filter theory. 


The transfer function can easily represent two transfer functions connected in series like the  their equivalent transfer function is the product of the two. Similarly, in a parallel interconnection, the single output Y is the sum of the outputs from each transfer function , and finding the equivalent single transfer function.


Zeros are values of Z where the transfer function equals zero, while poles are the inverse of the transfer function's roots (roots of $A(z)$ in case of no cancellations). Poles and zeros help determine if a system is stationary by checking if all poles are strictly inside the unit circle in the complex plane. 

-  To compute zeros, set the numerator of the transfer function equal to zero.
-  For poles, set the denominator equal to zero. 


The theorem states that a transfer function is asymptotically stable if and only if all its poles are inside the unit circle (in the complex plain).

The theorem states that for a system to be asymptotically stable, all roots (poles) of the transfer function must be inside the unit circle. If both the zeros and poles are inside, the system is minimum phase. 

The system is asymptotically stable if the absolute value of all poles is less than 1. 

Minimum phase systems are those in which all zeros (roots of the denominator) are inside the unit circle in the complex plane. }



All MA processes have this property (regardless the value of coefficients), as they have all poles (roots of the numerator) at the origin.


All AR processes have all zeros at the origin and all non-trivial poles at the origin. 
All AR process are all minimum phase. 


Therefore, all MA and AR processes of order $n$ are asymptotically stable, as the poles are always at the origin. In the context of AR processes, if some coefficients are zero, it will not affect the number of solutions (roots) of the polynomial as long as it is written in positive powers of t.


However, the real motivation behind this approach is the theorem that states that the steady state solution of an AR process is stationary only if the transfer function is asymptotically stable. 

For $MA$ processes are always asymptotically stable and stationary.
However, when there are **autoregressive parts** in the process, the roots of A of Z depend on the choice of **parameters**, and if they are outside the unit circle, the output process will no longer be stationary. Therefore, it's necessary to restrict attention to stationary processes described by transfer functions to guarantee that the autoregressive part keeps the roots inside the unit circle. 


# Mean and covariance function of ARMA models 


We can compute the $MA(\infty)$ equivalent to $AR(1)$ and we use the rules for $MA$ processes to compute $m_y$ and $\gamma _y (\tau)$ . 


So, the first approach is to find the MA of infinite order equivalent to the AR(1) model and compute the mean and covariance function using MA process rules. 

$$W(z)=\frac{C(z)}{A(z)}$$

To find the MA infinite equivalent to an AR process, we can consider it as a geometric series that converges to this form and show the equivalence between the polynomials. By dividing the two polynomials, we obtain another **polynomial as the result**. If the division is exact, $y(t)$ can be written as $e(z)$ times $e$ of $t$, which is an $MA$ process. To obtain the $MA$ infinite, we need to divide the polynomials an infinite number of times, assuming perfect division. 

Therefore, **long division** is the key to finding the MA infinite equivalent to an AR process.

In this process, they divide the coefficients of the transfer function infinitely many times to obtain an equivalent process with decreasing powers of z in the numerator. The maximum power decreases with each division step. The result is a set of coefficients, which can be used to compute the mean and covariance function of the AR process directly, without relying on the infinite equivalent process. 

### If a process is stationary regardless of the timestamp the expectation of its square is always equal to the covariance function in $0$.



Focusing on the recursive property of the covariance function. These equations are a set of recursive formulas for calculating the covariance function at different lags, where the value at a certain lag depends on the covariance function at the previous lag. The lag represents time, leading to a recursive development of the covariance function. This is related to the recursive nature of the AR model itself. The equations are deterministic, with the parameter $a$ playing a crucial role in shaping the behavior of the covariance function. $a$ acts as a scaling factor, influencing the covariance values for different lags.


**Yule-walker** equations (for the AR(1) process) are a set of recursive equations. 

$m_y=0$

$$\gamma_{y}\left(\tau\right)=a^{{\left|\tau\right|}}\cdot\frac{\lambda^{2}}{1-a^{2}}$$




Generalizing, for ARMA: 

$m_y=0$

Covariance? 

The given text discusses the computation of the covariance function for an Autoregressive Moving Average (ARMA) process with zero mean, which involves the sum of squares of terms and cross products, as well as recursive equations for initialization. When the mean is not zero, the covariance function still has recursive properties, but the mean of the input needs to be taken into account. 


$$\begin{cases}\gamma_y\left(0\right)={a_1}^2\gamma_y\left(0\right)+{a_2}^2\gamma_y\left(0\right)+2a_1a_2\gamma_y\left(1\right)+...\\\gamma_y\left(1\right)=a_1\gamma_y\left(0\right)+...+c_0\mathrm{E}[e(t)y(t-1)]+...\\\vdots\\\gamma_y\left(m-1\right)=a_1\gamma_y\left(m-2\right)+...\end{cases}$$


Processes with non-null term?

The key takeaway is that incorrect results are obtained when dealing with non-zero mean processes if one fails to recognize their non-zero mean status or doesn't read the exercise carefully.

$$E[y(t)]=W(1)\cdot\mu$$

To calculate the correlation function for non-zero mean white noise, one must take expectations involving the mean, which involves more computations compared to the zero-mean case. However, a tip to make the computation more efficient is to introduce unbiased versions of the processes $\tilde{y}$ and $\tilde{e}$, which are obtained by removing the means directly. By writing the equation for $\tilde{y}$, the terms can be simplified to make the computations more manageable. 


$$\begin{cases}\tilde{y}(t)=y(t)-m_y\\\tilde{e}(t)=e(t)-m_e&\end{cases}$$



$$\gamma_{\tilde{y}}(\tau)=\gamma_{\gamma}(\tau)\quad\forall\tau $$


### Frequency domain interp

Let's get into the **frequency domain interpretation** and visualization of stochastic processes. Paralleling deterministic signals, we can analyze stochastic processes from a frequency domain perspective. 

In the deterministic world, through Fourier transformations, we examine signals across frequencies, identifying dominant components and maximum energy points.

Applications of this frequency domain analysis include filtering signals based on their frequency content and designing anti-alias filters for digitization. Additionally, analyzing a system's frequency response reveals how it amplifies specific frequency components, providing insights into the performance of control systems.

Understanding the relationship between time and frequency domain perspectives, we realize that both representations are equivalent, offering unique insights into the properties of signals and systems. Our goal is to extend this concept to the interpretation of sarcastic processing in the frequency domain.

There is a challenge in stochastic world in the frequency domain due to the stochastic nature of the process. 
In a stationary stochastic process, multiple realizations can yield different time signals, resulting in varying frequency content. 

The issue is that the frequency content depends on the specific realization, making it difficult to provide a definitive frequency domain interpretation. The solution is to **focus on deterministic aspects** of the stochastic process, such as the covariance function, which can be translated into the frequency domain to fully describe the process for any realization.

Spectral density of a stationary stochastic process

$$\Gamma_{y}(w)=\sum_{\tau=-\infty}^{\infty}\gamma_{y}(\tau)\cdot e^{-jw\tau}$$

It is derived from the covariance function through the discrete Fourier transform. In the complex domain, omega is the frequency and $j$ is the imaginary part. 
The spectral density is a real, positive, even, and $2\pi$-periodic function. Its graph can be represented in the upper half plane of a 2D plot, as the imaginary part is always zero.





The maximum frequency in a continuous time signal for representation is determined by the Nyquist frequency.


The transformation is invertible, and the inverse transform returns the covariance function based on the spectral density, noted as the inverse discrete Fourier transform. 

$$\gamma_{y}(z)=F^{-1}(\Gamma_{y}(\omega))=\frac{1}{2\pi}\int_{-\pi}^{\pi}\Gamma_{y}^{w}(w)e^{+jw\tau}d \omega$$


What about ARMA processes? 

When filtering a stochastic process, consider the underlying process's frequency content instead of a specific realization's frequency content. This theory is more abstract than traditional signal processing but offers more power by accounting for uncertainty. Computing the spectrum of a process with an infinitely many nonzero covariance function values is unfeasible.




By using Euler's formula, the complex exponentials can be rewritten in terms of cosines and sines. 

$$e^{j\omega } =\cos \omega +j \sin \omega$$
$$e^{-j\omega }=\cos \omega -j\sin \omega$$

Hence, the spectral density function becomes a real-valued function composed of the sum of two cosine terms with frequency ω.


To obtained the same expression, an alternative way, to compute the spectrum of MA(1), instead of using its definition, we can transform signals through the operational domain and compute the corresponding transfer function. 

**Theorem of spectral factorization**

The direct definition of spectral density cannot be directly applied to systems with a transfer function $w(z)$ due to infinite terms in the summation. Instead, we use the spectral factorization theorem to calculate the output spectral density from the input spectral density and the frequency response of $w(z)$, denoted as $w(e^{j\omega})$.


1. **Determine Gain:** The gain for each frequency  ($\omega$) is found by calculating the magnitude of the frequency response: 
   
   $$|w(e^{j\omega})|$$ 

2. **Input Spectral Density:** For white noise input, the spectral density is constant and equal to its variance.

3. **Output Spectral Density:** The output spectral density is the product of the input spectral density and the squared gain.




After the spectral factorization we can analysis the signal in the spectrum graph.

The variance can be computed by finding the area under the spectrum curve, or analytically by integrating the spectral density function.




So recapping rapresentations for ARMA processes:


Time domain representation

$$y(t)=a_1y(t-1)+\cdots a_my(t-m)+ c_o e(t)+\cdots c_ne(t-n)$$


Operatorial representation

$$y(t)=\frac{C(z)}{A(z)}e(t)$$



Probabilistic representation

$$y(t)= m_y,\gamma_y(\tau)$$


Frequency domain 

$$y(t)=m_y,\Gamma_y(\omega)$$


They are all equivalent but not unique! This last little aspect it will be problematic, and so we will see a way to find unique ones. 

## The spectral factorization theorem

The fact that these representations aren't unique, meaning for a given stationary stochastic process $Y_t$, there might be infinitely many representations, $W_z$ and $E_t$, that produce $Y_t$ as an output. This non-uniqueness can lead to ambiguity when interpreting mathematical representations in real-world outcomes. So before introducing prediction theory, we need to better understand the non-uniqueness property and find ways to make the representation unique.


The theorem says .... $C(z)$ and $A(z)$ are:

1) monic: the term with the highest power has coefficient equal to 1
2) coprime: no common factors to that can be simplified
3) have same degree
4) have roots (poles and zeroes) inside the unit circle