
# Stochastic process

The purpose of making predictions on the future behavior of a signal requires taking into account the uncertainty in the model. This leads us to the need for a new concept, the stochastic process. 

A stochastic process $SP$ is a $n$ infinite sequence of random variables that depend on the outcome of a random experiment.

$SP$ extends the concept of random variables to signals. 

In this context is important to refresh this stuff:

- **Mean**: $E[v]$. Key property of expected value $E[\alpha_1v_1 + \alpha_2v_2] = \alpha_1E[v_1] + \alpha_2E[v_2]$
- **Variance**: $E[(v - E[v])^2] \geq 0$
- **Covariance**:

$$\gamma(t_1,t_2)=E[(v(t_1,s)-m(t_1))(v(t_2,s)-m(t_2))]$$

Note that the covariance function is the covariance of two Covariance is a useful statistical measure that indicates how two samples of the **same** stochastic process change together ("covary").

We will focus on **Stationary** Stochastic Process (SSP) which have the following characteristics:

- $m(t) = m \space \forall t$ "any realization will be a constant outcome"
- $\gamma(t_1,t_2)$ depends only on $\tau = t_2 - t_1$ and can therefore be indicated with $\gamma(\tau)$. This basically means "it's not important when you take the samples but only the absolute shift between the two". Also it has these proprieties: 
- 
	- $\gamma(0)=E[(v(t,s)- m)^{2}]\geq 0$ positivity
	- $|\gamma(\tau)|\leq\gamma(0),\forall\tau$  not increasing
	- $\gamma(\tau)=\gamma(-\tau),\forall \tau$ even

## White noise

A White Noise ($WN$) is a $SSP$ with $\gamma(\tau) = 0$ for all $\tau \neq 0$. 

A $WN$ is an unpredictable signal meaning that there is **NO CORRELATION** between $\eta(t_1)$ and $\eta(t_2)$. Usually, a $WN$ is defined as $\eta(t) \sim WN(0, \lambda^2)$. Having them with zero-mean is not mandatory.

The fact that the covariance function is zero everywhere except for the origin means that the notion of the past is not informative to know the future (**whiteness** property).

We will use white noise to represent **uncertainty** in the model. 

## Families of SSP

We will see these families of $SSP$:

- **MA (Moving Average):** Use past errors to predict future values.
- **AR (Autoregressive):** Use past values to predict future values.
- **ARMA (Autoregressive Moving Average):** Combine both AR and MA elements

### Moving averages

With $e(t)\sim wn(0,\lambda^2)$ we can define the moving average process of order $n$ $MA(n)$ in this way: 

$$y(t)=c_0\cdot e(t)+c_1e(t-1)+c_2\cdot e(t-2)+\cdots+c_ne(t-n)$$

$MA(n)$ is still stationary . A generalization of $MA(n)$ is:

$$MA(\infty)=\sum_{i=0}^{+\infty}c_ie(t-i), \space e(t)\sim\mathrm{wn}(0,\lambda^{2})$$

but the series must converge! 
The properties to remember here are:

$$m_y(t)=m_y(0)=0$$

$$\gamma_{_y}(\tau)=\begin{cases}\left({c_0}^2+{c_1}^2+{c_2}^2+...+{c_n}^2\right)\cdot\lambda^2&\text{if }\tau=0\\\left({c_0}c_1+{c_1}c_2+...+{c_{n-1}}c_n\right)\cdot{\lambda^2}&\text{if }\tau=\pm1\\\left({c_0}c_2+{c_1}c_3+...+{c_{n-2}}c_n\right)\cdot{\lambda^2}&\text{if }\tau=\pm2\\\vdots\\\left({c_0}c_n\right)\cdot{\lambda^2}&\text{if }\tau=\pm n\\0&\text{if }\tau>\pm n\end{cases}$$


So for example, for $y(t)=e(t)+c\cdot e(t-1)$:

$$\gamma_y(0) = 1+ c^2$$

$$\gamma_y(1) = \gamma_y(-1) = c$$


An example of $MA(2)$ (always with $e(t) \sim (0,1)$ ) : 

$$y(t)=e(t)+\frac{1}{2}e(t-1)+\frac{1}{4}e(t-2)$$


### AR

Autoagressive models will allow us to have $\gamma(\tau) \ne 0$ for all $\tau$ using a finite set of coefficients. 


$$y(t)=a_1y(t-1)+a_{2}y(t-2)+\cdots+a_{m}y(t-m)+e(t)$$

with $e(t) \sim wn(0,\lambda ^2)$ . 
Where $n$ is the order of the process and $e(t)\sim WN(0,\lambda^2)$. 

Mean is computed applying $E[v(t)].$
 Covariance function with $n=1$ (case of $AR(1)){:}$
$$\gamma(\tau)=\begin{cases}a\gamma(\tau-1)&|\tau|\geq0\\\frac{1}{1-a^2}\lambda^2&\tau=0\end{cases}$$
well defined if $|a|<1$.

Yule Walker Equations, can be used to evaluate the covariance.

### ARMA

An ARMA process is a combination of Autoregressive (AR) and Moving Average (MA) models. ARMA processes allow for both recurrent terms based on **past outputs** and input terms based on **past errors**. 
The order of the AR and MA parts are denoted by $m$ and $n$, respectively. ARMA processes include all other AR and MA models as special cases. 

### ARMAX

Additionally, ARMA models can be extended to include **exogenous inputs** by modeling the signal as a filtering of the inputs. For example, an acceleration signal can be modeled as a filtering of engine torque or pedal position.

The concept of $e(t)$ is a mathematical artifact representing the stochastic nature of measurements. In reality, $e(t)$ does not exist, but when is the output of a system, both deterministic and stochastic effects need to be considered. The deterministic effect is caused by an input U, while the stochastic effect is due to the **inherent errors in measurements**. 

ARMAX($m,n,p,k$) with $k$ pure input/output (I/O) delay. $p$ order of the exogenous part:

$$\begin{aligned}y(t)&=a_1y(t-1)+a_2y(t-2)+\cdots+a_my(t-m)+\\&+c_0t(t)+c_1e(t-1)+\cdots\cdots c_n e(t-n)+\\&+b_0u(t-k)+b_{1}u(t-k-1)+\cdots b_{p}u(t-k-p)\end{aligned}$$


There is also N-ARMAX (non linear) where Unlike linear Armax models, no linear combinations or weighted averages are used, and f can be any non-linear, parametric function. Modeling and learning this type of model in system identification is complex due to the added complexity of the dynamic system and time axis, and preserving formal properties. 



## Spectral rapresentation

Spectral density of a stationary stochastic process

$$\Gamma_{y}(w)=\sum_{\tau=-\infty}^{\infty}\gamma_{y}(\tau)\cdot e^{-jw\tau}$$

It is derived from the covariance function through the discrete Fourier transform. In the complex domain, omega is the frequency and $j$ is the imaginary part. 
The spectral density is a real, positive, even, and $2\pi$-periodic function. Its graph can be represented in the upper half plane of a 2D plot, as the imaginary part is always zero.

The transformation is invertible, and the inverse transform returns the covariance function based on the spectral density, noted as the inverse discrete Fourier transform. 

$$\gamma_{y}(z)=F^{-1}(\Gamma_{y}(\omega))=\frac{1}{2\pi}\int_{-\pi}^{\pi}\Gamma_{y}^{w}(w)e^{+jw\tau}d \omega$$



The spectrum of a $WN$ is constant and equal to its variance:

$$\Gamma_{e}(\omega)==Var[e(t)]=\lambda^{2}$$


Fundamental theorem of spectral analysis is:

$$\Gamma_{yy}(\omega)=|W(e^{j\omega)}|^2*\Gamma_{uu}(\omega)$$




## Canonical representation of $SSP$

The spectral factorization theorem plays a crucial role in the analysis of discrete-time signals, particularly through the use of the transfer function $W(z)$. This function is central to understanding how signals can be transformed in the frequency domain.


Backward shift operator $z-1$ which shifts a signal back in time

the forward shift operator $z$  which shifts a signal forward in time.

Linear properties of $z$ and $z-1$ include linearity, recursive application, and linear composition. Using these properties, we can rewrite an AR model equation in terms of $z$ and $z-1$, revealing the relationship between the operators and the AR model.

**Shift Operators in Time Domain Analysis**

The resulting equation shows that the output $y_t$ is the ratio of two polynomials of $z$:

$$y(t)=\frac{c_{0}+c_{1}z^{-1}+\cdots c_{n}z^{n}}{1-a_{1}z^{-1}+\cdots a_{m}z^{-m}}e(t)=\frac{C_{(z)}}{A(z)}e(t)=W(z)e^{it}(t)$$


**Transfer Function as a Digital Filter**

The transfer function $W(z)$, being a ratio of two polynomials, acts as a digital filter. It is instrumental in transforming $WN$ into an ARMA process and is applied in analyzing stationary processes through digital filter theory. 

**Poles and Zeros in Stability Analysis**

Stability analysis involves examining the poles and zeros of the transfer function:

- Zeros are values of $z$ where the transfer function equals zero
- Poles are the inverse of the transfer function's roots (roots of $A(z)$ in case of no cancellations). 

The locations of poles and zeros, particularly within the unit circle in the complex plane, are indicative of system stability. 

-  To compute zeros, set the numerator of the transfer function equal to zero.
-  For poles, set the denominator equal to zero. 


A system is considered asymptotically stable if all poles lie strictly inside the unit circle, and it is deemed a minimum phase system if both poles and zeros reside within the unit circle.


## Representations for ARMA Processes


ARMA processes can be represented in various domains:

Time domain representation

$$y(t)=a_1y(t-1)+\cdots a_my(t-m)+ c_o e(t)+\cdots c_ne(t-n)$$


Operatorial representation

$$y(t)=\frac{C(z)}{A(z)}e(t)$$


Probabilistic representation

$$y(t)= m_y,\gamma_y(\tau)$$


Frequency domain 

$$y(t)=m_y,\Gamma_y(\omega)$$


They are all equivalent but not unique! This last little aspect it will be problematic, and so before introducing prediction theory, we need to better understand the non-uniqueness property and find ways to make the representation unique.


The theorem says .... $C(z)$ and $A(z)$ are:

1) **monic**: the term with the highest power has coefficient equal to $1$
2) **coprime**: no common factors to that can be simplified
3) have **same degree**
4) have **roots inside the unit circle**




By using Euler's formula, the complex exponentials can be rewritten in terms of cosines and sines. 

$$e^{j\omega } =\cos \omega +j \sin \omega$$
$$e^{-j\omega }=\cos \omega -j\sin \omega$$

Hence, the spectral density function becomes a real-valued function composed of the sum of two cosine terms with frequency ω.


Using Euler's formula, the complex exponential terms in the frequency domain representation can be expressed as a sum of cosine and sine functions, making the spectral density function real-valued and easier to interpret.




## Extra, to finish: 


We can compute the $MA(\infty)$ equivalent to $AR(1)$ and we use the rules for $MA$ processes to compute $m_y$ and $\gamma _y (\tau)$ . 


So, the first approach is to find the MA of infinite order equivalent to the AR(1) model and compute the mean and covariance function using MA process rules. 

$$W(z)=\frac{C(z)}{A(z)}$$

To find the MA infinite equivalent to an AR process, we can consider it as a geometric series that converges to this form and show the equivalence between the polynomials. By dividing the two polynomials, we obtain another **polynomial as the result**. If the division is exact, $y(t)$ can be written as $e(z)$ times $e$ of $t$, which is an $MA$ process. To obtain the $MA$ infinite, we need to divide the polynomials an infinite number of times, assuming perfect division. 

Therefore, **long division** is the key to finding the MA infinite equivalent to an AR process.

In this process, they divide the coefficients of the transfer function infinitely many times to obtain an equivalent process with decreasing powers of z in the numerator. The maximum power decreases with each division step. The result is a set of coefficients, which can be used to compute the mean and covariance function of the AR process directly, without relying on the infinite equivalent process. 

If a process is stationary regardless of the timestamp the expectation of its square is always equal to the covariance function in $0$.







Processes with non-null term?

The key takeaway is that incorrect results are obtained when dealing with non-zero mean processes if one fails to recognize their non-zero mean status or doesn't read the exercise carefully.

$$E[y(t)]=W(1)\cdot\mu$$

To calculate the correlation function for non-zero mean white noise, one must take expectations involving the mean, which involves more computations compared to the zero-mean case. However, a tip to make the computation more efficient is to introduce unbiased versions of the processes $\tilde{y}$ and $\tilde{e}$, which are obtained by removing the means directly. By writing the equation for $\tilde{y}$, the terms can be simplified to make the computations more manageable. 


$$\begin{cases}\tilde{y}(t)=y(t)-m_y\\\tilde{e}(t)=e(t)-m_e&\end{cases}$$



$$\gamma_{\tilde{y}}(\tau)=\gamma_{\gamma}(\tau)\quad\forall\tau $$



