# I/O

## Memory and port mapped devices

A **bus** is an hardware connection channel that facilitates communication between the CPU and other devices. Communication between the CPU and devices can happen over two buses:

- **Memory bus:** Peripheral device registers are integrated into the CPU's address space. This setup allows the CPU to read from and write data to the peripherals, as well as send commands, using standard memory access instructions. It simplifies the interaction between the CPU and the peripherals by treating peripheral I/O as regular memory reads and writes. Memory-mapped I/O is more used than port-based I/O
- **Port bus:** the port bus typically has its own address space separate from the main memory, necessitating specialized commands for data transfer and control.

### How to request access

TODO

- **Port-based I/O**: Accessed through special instructions.
- **Memory-mapped I/O**: Requires IOREMAP before read/write operations.

## Devices and CPU communication


3 main ways to communicate: 

- **Polling** involves the CPU constantly checking the status of devices. While simple, it is inefficient as it consumes significant CPU resources.
- **Interrupts**: Allow devices to notify the CPU of important events, prompting immediate attention. This is more efficient than polling. We can say there are different types of Interrupts:
	- **Asynchronous Interrupts**: Generated by hardware devices.
	- **Maskable Interrupts**: Can be ignored by the CPU. 
	- **Non-Maskable Interrupts**: Always recognized by the CPU.
	- **Synchronous Interrupts**: Produced by the CPU itself during instruction execution.
	- The **Interrupt Handling Process** prioritizes the interrupts and addresses an interrupt event by executing an interrupt handler stopping what the CPU is doing.
	- In **Multi-Core Systems**, interrupts are managed by a dedicated interrupt controller that assigns them to the appropriate CPU core based on priority.
- **DMA** (Direct Memory Access) which involves using a DMA controller. The DMA controller is like an additional device that independently manages data transfers between different devices and memory. It does so without needing the CPU's involvement, thereby **decoupling** it from the transfer process.

![](images/0a382128fff977f48ad07822bb562ae7.png)

### Interrupts

#### Interrupts and you

1. Why we need interrupts?
2. The only way we know up to now to check whether some hardware condition happened is to check status bits.
3. Repeatedly checking status bits is called **polling**.
4. There is an inherent trade-off between responsiveness to events, and polling period.
5. Some events may be sporadic.
6. Interrupts can be thought as letting hardware call a software function when an event occurs.
7. When the interrupt occurs, the CPU is executing some other userspace or kernelspace code.
8. Interrupts can pause the normal code execution in between any two assembly instructions and jump to a function, the "interrupt service routine (ISR)".
9. On certain architectures, higher priority interrupts can even interrupt lower priority ones (interrupt nesting).
10. When the interrupt service routine has completed, the processor reverts back to executing the normal code.
11. Interrupts are always run to completion, an interrupt must never block, or it will block all normal code (the entire OS + applications).
12. Interrupts should be written to be as fast as possible, in order to minimize the time interference with the main code (interrupts can insert lag anywhere in the normal code, reducing its time determinism).


Interrupt Request (IRQ) 

Interrupts, Interrupt Handlers, Top halves versus Bottom Halves


Interrupt handlers: also called Interrupt Service Routine (ISR), the function kernel runs in response to a specific interrupt. What differentiates interrupt handlers from other kernel functions is that: kernel invokes ISR in a special context called interrupt context, also called atomic context sometimes, because code executing in this context is unable to block.


Top halves versus bottom halves: Because the 2 goals: i) ISR executes quickly; ii) ISR performs a large amount of work - conflict with each other, so the processing of interrupts is split into two parts. The interrupt handler (ISR) is the top half, the top half runs immediately on receipt of the interrupt and performs only the work that is time-crirical. Work that can be performed later is deferred until the bottom half, the bottom half runs in the future, at a more convenient time, with all interrupts enabled.

The point of a bottom half is not to do work at some specific point in the future, but simply to defer work until any point in the future when the system is less busy and interrupts again enabled. Often the bottom halves run immediately after the interrupt returns. The key is that they run with all interrupts enabled.

 Tasklets, which has nothing to do with tasks, are flexible and dynamically created bottom halves built on softirqs. Two different tasklets can run concurrently on different processors, but tasklets of the same type couldn't cannot run simultaneously.

If this is a multi-processor machine, check whether the tasklet is running on another processor by checking TASKLET_STATE_RUN flag. If it is currently running, do not execute it now and skip to the next pending tasklet.


`DECLARE_TASKLET(packet_tasklet, packet_tasklet_handler, 0);`

`static DECLARE_WORK(packet_work, packet_work_handler);`

```cpp
static void packet_tasklet_handler(unsigned long data) {
    // Quick deferred work
    printk(KERN_INFO "Tasklet: Processing packet quickly\n");
    // Schedule workqueue for more extensive processing
    schedule_work(&packet_work);
}

static void packet_work_handler(struct work_struct *work) {
    // Complex processing that can block
    printk(KERN_INFO "Workqueue: Processing packet in detail\n");
    // Simulate blocking operation
    msleep(1000);
}
```

- **Tasklets** are used for quick, non-blocking deferred work, suitable for handling small tasks immediately after an interrupt.
- **Workqueues** allow more complex processing that can include blocking operations, providing greater flexibility for deferred work.




Tasklets ensure **non-reentrancy** by design, meaning the same tasklet cannot run on multiple CPUs simultaneously.

**Reentrancy** is a property of code that allows it to be safely called again before its previous execution is complete. This is especially important in a multi-CPU environment because multiple processors might execute the same code concurrently.

**Reentrant Code**:

- Does not rely on shared data.
- Uses local variables or ensures exclusive access to shared resources (e.g., through locking mechanisms).

**Non-reentrant Code**:

- Uses static or global variables without proper synchronization.
- Relies on state that might be altered by concurrent executions.




Tasklets could be created statically or dynamically:
DECLARE_TASKLET(name, func, data)

The tasklet handler must match the following prototyte:
void tasklet_handler(unsigned long data);

Scheduling the tasklet:

tasklet_schedule(&my_tasklet);  /* mark my_tasklet pending */
As an optimization, a tasklet always runs on the processor that scheduled it - making better use of the processor cache.

As with softirqs, tasklets cannot sleep, so you cannot use semaphores or other blocking functions in a tasklet. Tasklets also run with all interrupts enabled. If your tasklet shares data with other tasklet or softirq, you need to use proper locking.



Work queues defer work into a kernel thread, this bottom half always runs in process context; so work queues are schedulable and can therefore sleep. It's easy to decide between using work queues and softirq/tasklet: If deferred work needs to sleep, work queues are used; otherwise, softirqs/tasklets are used.

In the code it is usually preferred to choose atomic operations over more complicated locking mechanisms. On most architectures, 1 or 2 atomic operations incur less overhead and less cache-line thrashing than a more complicated synchronization method.

#### Interrupt flow

1. **Interrupt Generation**: Triggered by a device or event.
2. **CPU Processing**: Upon receiving an interrupt, the CPU halts its current task.
3. **Interrupt Handler Execution**: 
   - **Role**: Determines the interrupt source and performs necessary actions.
   - **Actions**: May include servicing a device or handling a software event.
4. **Multi-Core Handling**: 
   - **Interrupt Controller**: In multi-core systems, an interrupt controller directs the interrupt to the correct CPU core.
   - **Priority Scheme**: Determines handling order, prioritizing higher over lower priority interrupts.
5. **Task Resumption**: 
   - **Preempted Tasks**: If a task was interrupted, it resumes from where it left off.
   - **Completed Tasks**: If the task finished, the CPU proceeds to the next queued task.



1. - Interrupt handlers must be quick and are executed at a high priority, preempting other code.running in interrupt context, such as interrupt handlers and SoftIRQs, cannot sleep. This is because sleeping would delay handling the interrupt and could lead to system instability or missed interrupts. 





tasklets: SoftIRQs APIs. A key point about SoftIRQs is that they run in interrupt context. . Tasklets, like interrupts, cannot sleep. non-reentrancy (you can’t have a certain tasklet run on more than one processor)


work queues: Work elements can sleep since it runs in process context. 


#### Deferring work 

In Linux, the concept of "**deferring work**" involves postponing the execution of a task until a later time. This is typically done when a task cannot be immediately completed due to external factors, such as the availability of a resource or the completion of another task.
Interrupt management is divided into two levels:

- **The top half** is responsible for handling the most urgent aspects of the interrupt. It acknowledges the interrupt and performs minimal processing to stabilize the system. It executes quickly and does not block other tasks.
- **The bottom half** takes care of the actual data processing. It is less time-sensitive and can afford to be interrupted or blocked. It is scheduled to run at a later time, allowing the system to remain responsive to other tasks.

Three methods are available for deferring work:

1) **SoftIRQs**: SoftIRQs are employed for tasks that require prompt attention but are too time-consuming to handle in the interrupt context. Following an interrupt, crucial work is handled immediately by the 'top half' or interrupt handler. Non-critical tasks are postponed and dealt with by SoftIRQs, the 'bottom half' of the procedure. SoftIRQs are scheduled to operate once the interrupt handler is done and are processed when the system is less occupied or under favorable conditions. SoftIRQs are known for running their entire course on the CPU they run on.
2) **Tasklets**: Tasklets, built on SoftIRQs, are used for tasks that do not require order or immediate execution. They offer an easier user-interface for defining bottom-half processes, simplifying the handling of SoftIRQs for kernel developers. Tasklets are **serialized** based on their CPU. Consequently, no two tasklets of the same type will run simultaneously on the same CPU, thereby minimizing concurrency problems. Tasklets are popular for tasks related to deferred data processing or cleanup operations mainly because they can be dynamically allocated and are easier to utilize than pure SoftIRQs.
3) **Work Queues**: Work queues accommodate deferred tasks that may need to **sleep** or **wait** for resources, something that SoftIRQs and Tasklets aren't capable of. This feature makes work queues perfect for tasks that might need to wait for I/O operations, mutexes, or other blocking events. Moreover, work queues operate in **process context** rather than in interrupt context, which is vital for operations requiring a process context such as user-space interactions or complex I/O. Work queues are suited for tasks that require process context like sleeping or blocking operations. They can carry out longer-running tasks that can sleep and are less time-pressured, such as I/O functions or batch processing.


Tasklets, like interrupts, cannot sleep.

Work queues can sleep. It’s the general mechanism for submitting work to a worker kernel thread



Discussing the concepts of tasklets and workqueues in the context of Linux's interrupt management:

1. Linux's interrupt management operates on two levels to improve responsiveness and benefit from aggregation. It splits interrupts into two parts:
    
    - Top half: Here, a minimal amount of work executes in a non-interruptible scheme.
    - Bottom half: The non-critical remaining work is deferred through the SoftIRQs APIs.
2. Commonalities between Tasklets and Workqueues:
    
    - Both are used to handle the bottom half of interrupt processing.
    - They are designed to run concurrently and efficiently.
3. Differences between Tasklets and Workqueues:
    
    - Tasklets are a type of SoftIRQ with a non-reentrant interface. This ensures that only one processor can execute a specific tasklet at a time. Tasklets, like interrupts, cannot sleep.
    - Workqueues, on the other hand, are schedulable work entities that run in process context to execute your bottom half. They are the general mechanism for submitting work to a worker kernel thread, typically labeled as 'events/n'. Work elements can sleep, which is the main difference with tasklets.





Assume you must write an interrupt routine for Linux that has to write to a log to disk; how would you structure your interrupt handler?


In Linux, interrupt handlers are executed in a critical section of the kernel and are responsible for handling hardware interrupts. They operate at the highest priority level, with minimal overhead and latency, and must complete quickly to avoid disrupting the normal flow of the system. Due to these constraints, interrupt handlers are not allowed to block or sleep, as doing so could render the system unresponsive.

When initiating a disk write operation from an interrupt handler, the disk may not be immediately available, and the write operation might need to wait for the disk to become ready. This scenario, known as "block I/O," cannot be executed within an interrupt context due to its potential to block.

To address this, work must be deferred through an appropriate mechanism. However, Softirq mechanisms, which are commonly used for deferring work, are not suitable for writing to a disk from an interrupt handler in Linux, as they cannot block or sleep.

Instead, a work queue is utilized to perform the disk write operation asynchronously, outside of the interrupt context, ensuring the normal flow of the system is not disrupted. A work queue provides a means to execute actions in a normal task context, outside of the interrupt context. Tasks are enqueued, and a worker thread subsequently retrieves and executes them. This approach allows the interrupt handler to return quickly, without waiting for the disk write operation to complete. The work queue mechanism employs a dedicated thread for the blocking I/O operation, thus preserving the system's normal operation.




## Linux Device Management

The inclusion of devices into the file system is a key idea that makes device management easier. Highlights of this approach:

- **Special Files for Devices**: Linux treats devices like files, represented as special files within the file system. This approach is based on the Unix philosophy of "everything is a file". Each device is assigned a path name, typically located in the `/dev` directory. For example, a UART device used in labs was represented as `/dev/serialaos`.
- These files are identified by **major** and **minor** numbers:
	- **Major Device Number**: This is used to identify the driver associated with the device. 
	- **Minor Device Number**: When a driver manages multiple devices of the same type, each device is assigned a unique minor number. 

All of this is handled by systems such as devfs, sysfs, and udev:

- **devfs (Device File System)**: This system is responsible for the management of device files. It dynamically creates and deletes these files as devices are added or removed, reflecting the current hardware configuration of the system.
- **sysfs (System File System)**: A virtual file system that provides a view into the kernel's device model. It exposes information about devices and drivers, as well as the relationships and hierarchy among different components, to user space.
- **udev (Userspace Device Manager)**: A daemon that manages device nodes in the `/dev` directory. It operates in user space to handle the dynamic attachment and detachment of devices, often using rules defined by the system administrator to manage permissions and naming.

### Device Categories

Linux categorizes devices based on their type and function into:

- **Character Devices**:
    - **Functionality**: Operate with a stream of characters, accessed one at a time. Interaction with these devices is immediate, making them ideal for hardware that requires prompt data transfer.
    - **Examples**: Serial ports, keyboards, terminal devices.
    - **Access**: Via special files in `/dev/`.
    - **Key Point**: They do **not use buffering**, directly affecting the device.
- **Block Devices**:
    - **Functionality**: Organized in blocks for random access, employing buffering and caching, suitable for large data storage and retrieval.
    - **Examples**: Hard drives, SSDs, USB drives.
    - **Access**: Through special files in `/dev/`.
- **Network Devices**:
    - **Functionality**: Handle data packet transmission and reception over network interfaces, critical for network communication.
    - **Examples**: Ethernet adapters, wireless interfaces.
    - **Access**: Managed through network configuration tools, not directly via `/dev/`.


### Block Devices

The low-level driver interface is responsible for directly communicating with hardware devices. Key components include:

- **Page Cache**: Caches file data in memory pages to check before generating block requests.
- **Mapping Layer**: Determines block numbers for requested data.
- **I/O Schedulers**: Optimize block I/O operations order for efficiency and fairness. Examples include CFQ, Deadline, and NOOP schedulers, each serving different purposes like minimizing disk seek times or prioritizing specific processes.



### Block Layer Data Structures

In the context of the block layer, the concept of a segment and the bio data structure are essential. Segments refer to the parts of a page that correspond to contiguous sectors on the disk. Internally, the mapping layer works with multiples of sectors called blocks. A bio_vec represents a single segment in memory and includes the following information:

1. The page where the data should be read or written.
2. The offset within that page.
3. The length of this segment.




managing data transfers between the system's memory and storage devices.

1. **bio**: This structure represents a block I/O operation. A bio can contain multiple segments (bio_vec), each pointing to a contiguous area in memory where data resides or will be placed after an I/O operation. 
2. **bio_vec** : Represents a segment within a bio structure. It includes information about the memory page where the data is to be read or written, the offset within that page, and the length of the segment.
3. **request_queue**: This structure represents the queue of pending I/O requests for a block device. It helps in scheduling and optimizing these requests before they are dispatched to the actual device driver. Techniques such as merging adjacent requests are employed to enhance performance.



### High-Level Device Management (The Device Model)

The linux device model is an abstraction layer which aims to **maximize code reuse** between different platforms in the Linux kernel, examples like using the same USB device driver for both a x86 PC and an ARM platform, even though they have different USB controllers, can be done.
This is achieved by the kernel providing a framework and a set of APIs that enable consistent and efficient management of devices. 

- **Kernel Frameworks**: essentially libraries that aid in the configuration of certain types of devices. They simplify setup by focusing on device-specific features while handling standard configurations, as seen in network driver, real-time clock (RTC), and universal asynchronous receiver-transmitter (UART) frameworks.
- **Bus Frameworks**: Manage bus technologies like USB, PCI, and platform buses. Essential for device detection, enumeration, and management, especially in SoC and embedded devices.
- **Kobjects and Sysfs**: Kobjects represent kernel objects and are the foundation of the device model. An important feature of Kobjects is their ability to emit uevents. These are notifications sent to user-space tools like udev, signaling any changes in the Kobjects. Sysfs exports information about these objects to user space, providing structured access to hardware details.

An example of **kernel frameworks** is the Universal Asynchronous Receiver/Transmitter (UART) which manages serial communication. UARTs are widely used for low-speed peripheral connectivity and are an essential part of embedded systems and computer interfacing.

An example of **bus frameworks** is the **The Platform Bus Framework** which is designed specifically for system-on-chip (SoC) and embedded devices. It manages hardware components that are not easily discoverable and represents fixed resources. 
The framework is defined either in board-specific code or the Device Tree.
Some key advantages of the Platform Bus Framework include simplifying driver development for embedded systems and promoting code reusability. 



