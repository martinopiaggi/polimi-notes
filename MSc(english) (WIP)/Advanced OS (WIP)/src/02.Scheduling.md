# Scheduling

When studying the scheduling of processes we have a couple of metrics to consider: 

- **`a_i`**: **Arrival time**, which is when a task becomes ready for scheduling.
- **`s_i`**: **Start time**, which indicates when a task begins execution.
- **`R_i`**: **Response time**, which is the time from arrival to **first quantum slice** ends. 
- **`W_i`**: **Wait time**, which is the **total** time spent waiting in the queue by a task.
- **`f_i`**: **Finishing time**, which denotes when a task completes its execution.
- **`C_i`**: **Computation time** or burst time, which represents the duration required for the processor to execute the task without any interruptions.
- **`Z_i`**: **Turnaround time**, which is the overall time taken from when a task becomes ready until it completes its execution. It is given by the equation **`Z_i = f_i - a_i`**. Note that **`Z_i`** is not necessarily equal to **`W_i + C_i`** because **interruptions** could occur.


Based on the nature of the operations performed by a task, we categorize processes as either CPU-bound or I/O bound:

- CPU-bound processes primarily spend their time executing computations. In this case, `Z_i` is approximately equal to `W_i + C_i`.
- I/O bound processes spend most of their time waiting for I/O operations. Here, `Z_i` is significantly higher than `W_i + C_i`.


Scheduling in an OS is a critical task involving the decision of which process to run next. The scheduling policy should aim to balance several factors:

- **Fairness**: 
   - Ensure the scheduling is fair and that no process is starving.
- **Throughput**: 
   - Aim for maximum process completion rate.
- **Efficiency**: 
   - Minimize the resources used by the scheduler itself.
   - Optimize CPU usage reducing context switching overhead.
- **Priority**: 
   - Reflect the relative importance or urgency of processes.
- **Deadlines**: 
   - Meet time constraints for time-sensitive operations like real-time tasks like multimedia playback or similar.

Note that OS scheduling strategies are **balancing conflicting goals** like deadlines and fairness.
For this reason we must make a distinction **domain-Specific Scheduling**:

- General-Purpose OSes **GPOS**: Balance throughput, fairness, user response; utilize time-sharing, dynamic priorities.
- Real-Time Operating Systems **RTOS**: Prioritize deadlines, predictability; apply RMS, EDF algorithms.

Additionally, **user** and **kernel mode** processes may have different priorities. 
But also **I/O-Bound** and **CPU-Bound** processes need a distinction for resource efficiency.
**Multicore/Multiprocessor** environments add scheduling complexities and **adaptive Scheduling** adjusts priorities and nakes decisions based on system load and process activity.

## Scheduling algorithms

The algorithm used by the scheduler to determine the order is called the **scheduling policy**.
Computing an optimal schedule and resource allocation is an NP-complete problem. To increase the complexity we have to keep in mind that these objectives often conflict with each other: 

- maximize processor utilization
- maximize throughput: number of tasks completing per time unit
- minimize waiting time: time spent ready in the wait queue
- ensure fairness
- minimize scheduling overhead
- minimize turnaround time
- and many more: energy, power, temps, ....

Also there is the problem of **starvation**. 
Schedulers can be categorized into different types based on their characteristics.

- **Preemptive vs Non-preemptive**:
	- Preemptive is the ability to interrupt tasks and allocate the CPU to another task ensuring responsiveness.
	- Non-preemptive schedulers minimizes overhead but can impact responsiveness.
- **Static vs Dynamic**:
	- Static schedulers make decisions based on fixed parameters and are not be realistic in general-purpose systems.
	- Dynamic schedulers make decisions based on runtime parameters.
- **Offline vs Online**:
	- Offline schedulers are executed once before task activation, and the resulting *inflexible* schedule remains unchanged.
	- Online schedulers are executed during task execution at runtime, allowing for the addition of new tasks.
- **Optimal vs Heuristic**:
	- Optimal schedulers typically come with higher overhead and complexity.
	- Heuristic schedulers are not optimal but are usually more efficient in terms of overhead.

| Name | Target (Goal) | Preemptive |
| :--- | :--- | :--- |
| `FIFO` | turnaround | No |
| `SJF` | waiting time | No |
| `HRRN` | waiting time | No |
| `SRTF` | waiting time | Yes |
| `Round-robin` | response time | Yes |
| `CFS` | CPU fair share | Yes |


### First-In-First-Out (FIFO)

Simplest scheduling algorithm possible, also known as First Come First Served (FCFS). 
FIFO blueprint:

- Tasks are scheduled in the order of arrival
- Non-preemptive
- Very simple
- Not good for responsiveness
- Long tasks may monopolize the processor
- Short tasks are penalized

### Shortest Job First (SJF)

Shortest Job First (SJF) scheduler aims to minimize the **waiting time** of processes. Also known as Shortest Job Next (SJN). 
SJF blueprint:

- It selects the process with the shortest computation time `C_i` and executes it first. 
- Non-preemptive
- Starvation for long tasks
- How the fuck you know $C_{i}$ in advance?


The main disadvantage of SJF is the risk of starvation for long tasks. Indeed, in the schedule of this exercise, T2 is activated at time t=2 but it has to wait until t=17 to execute, yielding to all other tasks. If more tasks were present in the system, the risk of a total starvation for T2 would be considerable. A possible alternative that mitigates starvation is the Highest Response Ratio Next (HRRN) scheduler.


The SJF algorithm prioritizes tasks based on their execution time, aiming to execute the shortest tasks first. This approach minimizes the average waiting time across all tasks, as shorter tasks are less likely to be del ayed behind longer ones. However, its potential drawback is the risk of starvation, where longer tasks could be perpetually delayed if shorter tasks keep arriving. This makes SJF less ideal in environments with a high variance in task length or where fairness among tasks is a crucial requirement.




### Shortest Remaining Time First (SRTF)

SRTF uses the **remaining** execution time instead of the total $C_{i}$ to decide which task to run.
SRTF blueprint:

- Improve responsiveness for all tasks compared to SJF
- Starvation for long tasks
- We need to know $C_{i}$ in advance as SJF

### Highest Response Ratio Next (HRRN)

HRRN selects the task with the highest **Response Ratio**:

$$R R_{i}=\left(W_{i}+C_{i}\right) / C_{i}$$

HRRN blueprint:

- Non-preemptive
- Prevent starvation
- We need to know $C_{i}$ in advance

### Round Robin (RR)

RR is  very  popular  and  very  simple and also very adopted in modern OS.  Tasks are scheduled for a given time slice $q$ and then preempted to give time to other tasks.
RR blueprint:

- Preemptive: when the time quantum expires, the task is moved back to the ready queue.
- Computable maximum waiting time: $(n-1) * q$
- No need to know $C_{i}$ in advance
- Good to achieve the fairness and responsiveness goals
- No starvation is possible
- Turnaround time worse than SJF


Tasks in a ready queue are added based on FIFO policy. If an executing task gets pre-empted while a new task has been added in the ready queue, the new task has precedence in the queue over the pre-empted task.
In Linux, the default time quantum for the Round Robin (RR) scheduler is stored in `/proc/sys/kernel/sched_rr_timeslice_ms`, with a default value of 100ms.



The scheduler is preemptive at constant time intervals. Therefore, the scheduling algorithm is Round-Robin (RR) with a quantum of q = 3 units of time.

The total overhead depends on the number of preemptions

The overhead can be reduced by increasing the quantum q > 3. The disadvantage of choosing a larger quantum value is the increasing of the average turnaround and waiting time, especially penalizing small tasks


### CFS (Completely Fair Scheduler)

CFS attempts to balance a process's virtual runtime with a simple rule: CFS picks the process with the smallest vruntime.(This is the core part of CFS.)
CFS uses a red-black tree to manage the list of runnable processes, thus finding the task with the smallest vruntime efficiently. (The node contains the smallest vruntime is the leftmost node of the red-black tree, but it's also cached in the rb_leftmost.) If here're no runnable processes, CFS schedules the idle task. rb_leftmost should be updated when a task becomes runnable (wake up) or it's first created by fork. Since the red-black tree is for the runnable processes, so when a process terminates or blocks(becomes unrunnable), it should be removed from the tree.



In Linux, the transition from the $O(1)$ scheduler to the CFS marked a significant evolution in process scheduling, emphasizing fairness and dynamic adaptability.

The $O(1)$ scheduler (known for its constant time complexity) offered quick scheduling decisions but struggled with fair CPU time distribution, especially for long-running tasks. This was due to its reliance on fixed timeslices, which could lead to task starvation.

CFS introduces a fairness-centric approach, employing a red-black tree structure to manage tasks. This method ensures fair CPU time allocation by maintaining tasks in a sorted manner based on their virtual runtime (`vruntime`), which represents the time a task should run on the CPU.

CFS dynamically adjusts time slices in proportion to the task's priority and the overall system load, encapsulated in the formula:

$$\tau_{p} = \max \left(\frac{\lambda_{p} \tau}{\sum \lambda_{i}},\mu\right)$$

Here, $\lambda_{i} = k \times b^{-\nu_{i}}$ (with $k$ and $b$ constants chosen by the scheduler) signifies the weight derived from the task's nice value ($\nu_{i}$), influencing its share of CPU time. The targeted latency ($\tau$) reflects the desired period within which all runnable tasks receive CPU time, while the minimum granularity ($\mu$) ensures a lower bound on the timeslice, preventing excessive preemption.

During each timer interrupt at time $t$, CFS updates `vruntime` ($\rho_{i}$) for each task, ensuring the task with the lowest $\rho_{i}$, and hence the least CPU time so far, is selected next for execution. This selection process is efficiently managed through the red-black tree, maintaining $O(1)$ complexity.



The O(1) scheduler is no longer used in current versions of the Linux kernel. It was replaced by the Completely Fair Scheduler (CFS) around the 2.6.23 release of the Linux kernel. The O(1) scheduler was named for its ability to schedule tasks in constant time, regardless of the number of tasks.


The `nice` value is only applicable to non-real-time processes, specifically those in the `SCHED_OTHER` (also known as `SCHED_NORMAL`) scheduling class. The `nice` value ranges from -20 (highest priority within this class) to +19 (lowest priority within this class). These `nice` values are used by the Completely Fair Scheduler (CFS) to adjust the share of CPU time that processes get, with lower `nice` values giving a process more priority, hence more CPU time.

Regarding process priorities, Linux uses a priority range from 0 to 139, where:
- Priorities 0 to 99 are reserved for real-time priorities (higher value, higher priority), used in scheduling classes like `SCHED_FIFO`, `SCHED_RR`, and `SCHED_DEADLINE`.
- Priorities 100 to 139 are for non-real-time tasks, with the `SCHED_OTHER` (or `SCHED_NORMAL`) and `SCHED_BATCH` classes. Within this range, the effect of the `nice` value is evident.

It might seem counterintuitive, but within the Linux kernel's scheduling system, a lower priority number means a higher priority for getting CPU time. This is particularly true for real-time tasks where a priority of 0 is the highest possible priority. This scheme allows real-time tasks (with priorities 0 to 99) to always preempt non-real-time tasks (with priorities 100 to 139) regardless of their `nice` values.

So, in summary:
- `nice` values are used to adjust priorities within the non-real-time priority range (100 to 139).
- Real-time tasks, which ignore `nice` values, have priorities in the range of 0 to 99, where a lower number means higher priority.
- The "priority" you see with commands like `ps` combines these two systems, showing the effective priority of a process within the whole 0 to 139 range, with the understanding that lower numbers have higher precedence for CPU time.



#### Cgroups 

CFS alone is not enough to guarantee optimal CPU usage, especially when there are multiple threads from different user: for example, if user `A` with 2 threads and user `B` with 98 threads, user `A` will only be given 2% of the CPU time, which is not ideal. Each user should be given an equal share of the CPU, which is then divided among their threads.

To address this issue, we can use **cgroups** (control groups) which allows to allocate CPU usage based on groups rather than individual threads. By creating separate control groups for different users, we can ensure that each user gets a fair share of CPU usage.

#### Load balancing in CFS 

Load balancing in CFS is done using a work stealing approach, where each idle core balances its workload by attempting to steal threads from the busiest core (also known as the designated core). 

## Scheduling classes 

The idea is to dispatching processes in multiple classes. Each class has a unique scheduling policy defined in a set of functions called **scheduling class**. 
Scheduling classes:

- `SCHED_DEADLINE`
- `SCHED_FIFO`
- `SCHED_RR`
- `SCHED_OTHER`
- `SCHED_BATCH`
- `SCHED_IDLE`

Following the 2007 Completely Fair Scheduler CFS policy for non-real-time processes, there is this priority:

- Real-time processes $\pi \in[0,99]$; they belong to scheduling class `SCHED_FIFO` and `SCHED_RR`
- Non real-time processes $100 \leq \pi(v) \leq 139$ which depend on a nice value $v \in$ $[-20,+19]$ : $\pi(v)=120+v$.

![](images/540fbd25d7b4a41066ecc2310c790b06.png)


| Name | Target (Goal) | Where |
| :--- | :--- | :--- |
| FIFO | turnaround | Linux SCHED_FIFO |
| Round robin | res. time | Linux SCHED_RR |
| CFS | CPU fair share | Linux SCHED_CFS |
| MLFQ | res. time | Solaris, Windows, MacOS, BSD |



### Task scheduling (basics)

A scheduling class is an API (set of functions) that include policy-specific code, e.g., functions to select the core on which the task must be enqueued (select_task_rq) and functions to actually put the task on that queue (enqueue_task).

This allows developers to implement thread schedulers without reimplementing generic code and also helps minimizing the number of bugs.

Which are these scheduling class?

- SCHED_DEADLINE
- SCHED_FIFO
- SCHED_RR
- SCHED_OTHER
- SCHED_BATCH
- SCHED_IDLE

If multiple policies have a runnable thread, a choice must be made by Linux to determine which policy has the highest priority. Linux chooses a simple fixed-priority list to determine this order (deadline $\rightarrow$ real-time $\rightarrow$ fair $\rightarrow$ idle).

The scheduler performs load balancing by migrating threads between cores in order to even the number of threads of all cores. Load balancing is done with a work stealing approach: each core does its own balancing and tries to steal threads from the busiest core on the system.

In 2007 has been introduced Completely Fair Scheduler for non-real-time processes. Still completely separated sets of processes with a certain normal priority $\pi$ :

- Real-time processes $\pi \in[0,99]$; they belong to scheduling class SCHED_FIFO and SCHED_RR
- Non real-time processes $100 \leq \pi(v) \leq 139$ which depend on a nice value $v \in$ $[-20,+19]$ :

$$
\pi(v)=120+v
$$

The central data structure of the core scheduler that is used to manage active processes is known as run-queue.

Multiple run-queues (one per CPU) are needed to avoid contention over task selection among processors. Practically, each class has a run-queue that contains runnable tasks. These are grouped in a general struct rqfor each core.

The main scheduler function (schedule ()) is invoked directly in many points in the kernel to allocate the CPU to a process other than the currently active one (e.g., after returning from system calls/interrupts or when a thread does some explicit blocking (mutex/semaphore/waitqueue)).


| Scheduling Class | Description | Scheduling Algorithm | Type of Target Processes |
| ---- | ---- | ---- | ---- |
| **SCHED_DEADLINE** | Deadline-based | Deadline scheduling (EDF) | Real-time |
| **SCHED_FIFO** | Soft realâ€‘time processes, continue to run until higher priority task is ready. | First-In-First-Out | Real-time |
| **SCHED_RR** | Share with timeslice | Round-Robin | Real-time |
| **SCHED_OTHER** / SCHED_NORMAL | Variable-priority | Completely Fair Scheduler (CFS) | Not real-time |
| **SCHED_BATCH** | Low-priority | CFS with idle task prioritization | Not real-time |

SCHED_RR: Same prio tasks. Lower priority only when no higher priority present


In the Linux kernel, a scheduling class represents a specific way of handling task scheduling based on a set of policies and algorithms. Each scheduling class provides a distinct API, which is a collection of functions tailored to implement these policies and algorithms effectively.

As a developer, by understanding and utilizing the Linux scheduling classes and their associated APIs, you can tailor the scheduling behavior to fit the specific needs of your application or module, leading to improved performance, efficiency, and responsiveness.




Class used for the conventional **time-shared** processes. In modern linux it is based on the completely fair scheduling algorithm and that uses soft priority mechanism based on the 'nice' value $\nu$ $\left(\nu\in[-20,+19],\pi\in[100-139]\right).$

 For each process $p$, its time-slice is computed as:

$$
\tau_{p}=f(\nu_{0},\ldots,\nu_{p},\ldots,\nu_{n-1},\bar{\tau},\mu)\sim max(\frac{\lambda_{p}\bar{\tau}}{\sum\lambda_{i}},\mu)
$$

where $\bar{\tau}$ is called schedule latency, $\mu$ is called minimum granularity while $\lambda_i(\nu_i)$ is the weight associated with a process, a sort of priority. It has exponential formula, i.e, $\lambda_i=k\times b^{-\nu_i}$ (current values k= 1024, $\mathbf{b}=\mathbf{1}.25)$



SCHED_DEADLINE
Higher priority with respect to FIFO/RR; it is an implementation of the Earliest Deadline First (EDF) scheduling algorithm. Eachtaskischaracterizedby3parameters: runtime, period anddeadline. Essentially, it gives runtime to a task every period making sure that this is given within deadline. Typically deadline = period. In newer versions, If a task uses more time than needed, it is throttled or decreased in priority. Also, if tasks occasionally need more bandwidth then they are given some.





### Run queues

In CFS there exists a `cfs_rq` which is a [red-black tree](../../../BSc(italian)/Algoritmi%20e%20Principi%20dell'Informatica/src/12.Alberi%20e%20grafi.md##Alberi%20rosso-neri) .

... Explanation of CFS computation based on $\rho$ , $\epsilon$ , $\lambda$ . 


Also Cgroup for control group a solution for guarantee fairness and optimal cpu usage when there are multiple users. 

## Priority-based scheduling

A priority-based scheduler is a type of scheduling algorithm that uses multiple queues divided by priority. Each queue can use a different scheduling algorithm. For example, we can use Round Robin (RR) scheduling for each queue, but increase the time quantum for lower priority tasks to account for their longer wait times. It's also possible to implement time slicing between the queues: a 100ms time window is divided into different portions for each queue.

| Queue | Quota | Period |
| :--- | ---: | ---: |
| Queue 1 | 80 ms | 100 ms |
| Queue 2 | 15 ms | 100 ms |
| Queue 3 | 5 ms |100 ms |

Priority is selected depending on the workload type:

- CPU-bound tasks have low priority (high quantum value)
- I/O-bound tasks have high priority (low quantum value)

How to know if a task is CPU-bound or I/O-bound?

- A **run-time feedback mechanism**: a a new task is always placed in the highest priority queue with the lowest quantum value. If the quantum expires, the task is progressively moved in queues with longer time quantum.
- Or manually set by the user 

Priority levels in Windows:

- `IDLE_PRIORITY_CLASS`  
- `BELOW_NORMAL_PRIORITY_CLASS`  
- `NORMAL_PRIORITY_CLASS`  
- `ABOVE_NORMAL_PRIORITY_CLASS`  
- `HIGH_PRIORITY_CLASS`  
- `REALTIME_PRIORITY_CLASS`

Where `REALTIME_PRIORITY_CLASS` interrupts system threads that manage mouse input, keyboard input, and background disk flushing and it's generally used by tasks which should have limited interruptions.

## Multi-Processor Scheduling

In a multi-processor system, the scheduler must decide not only which task to execute but also on which processor to assign. 
This can be a challenging decision due to various factors such as the occurrence of task synchronization across parallel executions and the difficulty of achieving high utilization of all processors or CPU cores. Additionally, managing correctly cache memory which can significantly enhance overall performance by enabling faster access to frequently used data, reducing the reliance on slower main memory access.

- **Load balancing**: evenly distributing tasks across different queues to positively impact power consumption, energy efficiency, and system reliability. It's typically performed via **task migration** which can be implemented mainly in 2 ways: 
	- **push model**: a dedicated task periodically checks the lengths of the queues and moves tasks if balancing is required.
	- **pull model**: each processor notifying an empty queue condition and picking tasks from other queues. 
- **Hierarchical queues**: a hierarchy of schedulers can be implemented to manage task dispatching in a global queue and local ready queues. Improved scalability with maybe more complex.

