
# Concurrency 

Concurrency is when a program consists of activities that can overlap in execution. This can be achieved through threading, but there are other methods as well. Threading may incur context switch costs and might not be suitable for all applications.


We characterize a program with two properties (which might be true or false):

- Safety (correctness), nothing bad happens. If your program is a state machine, we don't reach an error state or work on invalid data.
- Liveness (progress), eventually something good happens. In an FSM, we eventually reach a final state.

Data race is an issue that occurs when two instructions, one being a write, occur in different threads without any enforced order between them.



## Deadlocks

Deadlock happens when tasks are unable to proceed because they are waiting for each other. It occurs due to mutual exclusion, hold-and-wait, no preemption, and circular wait conditions. Deadlocks can be prevented by designing data structures without locks or by releasing resources if further acquisition fails.

## Priority inversion 

Priority inversion is a scheduling scenario where a high priority task is delayed by a lower priority task due to resource contention. This violates the priority model. Priority inheritance solves this issue by increasing the priority of the lower priority task with the critical resource. Another solution is using highest locker priority or priority ceiling mechanisms.

To avoid deadlocks and prioritize safety and liveness in concurrent programs:
- Use resource allocation policies that ensure safe acquisition of resources
- Detect and recover from deadlocks
- Use resource allocation policies that prevent deadlocks from occurring
- Implement prevention and detection strategies


There three techniques to solve this issue:

1) Highest Locker Priority (HLP)
2) Priority Inheritance (PIP)
3) Priority Ceiling (PCP)


## Kernel concurrency 

Deep knowledge of kernel concurrency also enables module developers to take advantage of advanced features like lockless algorithms, fine-grained locking, or utilizing specific synchronization primitives provided by the kernel. This expertise allows for optimizing performance and scalability in critical areas of the Linux kernel module development process.

### Kernel preemption 

in a preemptive kernel at the end of every interrupt the scheduler is called for context switch, multiple threads running in kernel mode access shared structures

Preemption points
- From 2.6, Linux kernel became optionally preemptive. The preemption points are:
- At the end of interrupt/exception handling, when `TIF_NEED_RESCHED` flag in the thread descriptor has been set (forced process switch). It forces the process to switch. 
- If a task in the kernel explicitly blocks and calls `schedule()` (planned process switch). It is however always assumed that the code that explicitly calls `schedule()` knows it is safe to reschedule.

Atomic context refers to the places where it is not sate to call the scheduler and switch to a different thread. Atomic context examples are:

- the kernel is running an interrupt or trap handler
- the kernel is holding a spinning lock
- all programmers' defined places where it is not safe preeto mpt, e.g., because you are modifying the per-cpu structures or kernel state not restored with a context switch (e.g., floating point registers)

Spinning locks 

- If you had a uniprocessor machine, atomic context and interrupt enable/disable facilities would be enough to ensure mutual exclusion among activities in the kernel.
- On SMP machines however you need explicit mechanisms to lock. The spinlock is the basic ingredient.
- A spinlock continuously polls the lock until it becomes available whereas a sleeping lock waits until it is notified that the lock is available. A spinlock is generally useful when contention rates and critical section length are small and/or you can't afford the overhead of a sleeping lock.

### Interrupts 

can occur asynchronously at almost any time, interrupting the currently executing code in kernel mode.


### Multiple processors 

kernel code must simultaneously run on two or more processors, they can simultaneously access shared data at exactly the same time

