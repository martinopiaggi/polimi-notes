# Concurrency 

> "Linux is probably the most complex multi-process program among all the ones that exist now." 

Concurrency is when a program consists of activities that can overlap in execution. A program can be characterized by two properties:

- **Safety**: nothing bad happens.
- **Liveness**: makes progress and is not just stuck. 

Regarding concurrency we can highlights these aspects:

- **Deadlock** between tasks: due to mutual exclusion, hold-and-wait, no preemption, and circular wait conditions.
- **Priority inversion** is a scheduling scenario where a high priority task is delayed by a lower priority task due to locking. The bad concurrency model inverts the priority model

**Kernel concurrency** is a critical aspect of Linux kernel development, distinct from user space concurrency. It involves managing and synchronizing multiple threads within the **kernel space**. This includes:

- **Interrupts**: Handling asynchronous events that can occur at almost any time.
- **Kernel Preemption**: Allowing for the interruption of kernel tasks to enhance system responsiveness and multitasking capabilities.
- **Multiple Processors**: Ensuring the kernel's ability to run on multi-processor systems, which introduces complexities in resource sharing and synchronization.

## Preemption

From Linux kernel version 2.6 onwards, the kernel became optionally preemptive. **Preemption points** in the kernel include:

- **End of Interrupt/Exception Handling**: When the `TIF_NEED_RESCHED` flag in the thread descriptor is set, forcing a process switch.
- **Explicit Blocking**: When a task in the kernel blocks and calls `schedule()`, assuming it's safe to reschedule at that point.

How this works actually? `preempt_count`variable is used to ensure a safe context switch , called , that keeps track of the preemptions: the value is initially set at 0 and increases by 1 in two scenarios: 
	1. When a process acquires a lock in its critical section.
	2. When an interrupt occurs.

As long as `preempt_count > 0` the kernel cannot switch.

### Controlling pre-emption: Atomic context

**Atomic context** in the kernel refers to scenarios where it's technically possible but **unsafe** to call the scheduler and switch to a different thread. This includes situations like:

- Running an interrupt or trap handler.
- Holding a spinning lock.
- Programmer-defined non-preemptible contexts. One common example is when a task is preempted while accessing a per-CPU variable (like `tux[smp_processor_id()]`), the value of `smp_processor_id()` may change when the task is rescheduled. 


### Preemption points 





`TIF_NEED_RESCHED` is a flag used in the thread information structure to indicate that the current task should be rescheduled. When set, it tells the kernel to run the scheduler at the next safe point (preemption point).

#### Workflow of a Preemption Point

1. **Triggering an Event**: An event occurs that can trigger a preemption. This could be an interrupt, exception, or an explicit call to `schedule()`.
2. **Set TIF_NEED_RESCHED Flag**: The kernel sets the `TIF_NEED_RESCHED` flag in the current thread's descriptor to indicate that the scheduler needs to run.
3. **Check Preempt Count**: If the `preempt_count` is zero (indicating no nested critical sections or interrupts), the kernel can preempt the current task.
4. **Invoke Scheduler**: The kernel calls the `schedule()` function to pick a new task to run.
5. **Context Switch**: The current task's state is saved, and the context of the new task is loaded.
6. **Resume Execution**: The new task starts executing.



If the flag is set and `preempt_count` is zero, the kernel calls `schedule()`.



In short, kernel preemption can occur:
i)   When an interrupt handler exits, before returning to kernel-space;
ii)  When kernel code becomes preemptible again; (preempt_count -> 0)
iii) If a task in the kernel explicitly calls schedule();
iv)  If a task in the kernel blocks (which results in a call to schedule()).


**("Provide Mechanism, Not Policy")**: Syscalls should provide basic operations (mechanisms) that user-space programs can use to implement their policies. This keeps the kernel simple and versatile.


Preemption Points in Linux Kernel:

1. Forced process switch: From version 2.6, the Linux kernel became optionally preemptive. Preemption points are as follows: • At the end of interrupt/exception handling, when the TIF_NEED_RESCHED flag in the thread descriptor has been set. This is a forced process switch. • If a task in the kernel explicitly blocks and calls schedule(). It is assumed that the code that explicitly calls schedule() knows it's safe to reschedule.
    
2. Preempt_count: The kernel keeps track of nested atomic contexts using a process-local preempt_count variable. The kernel increments this value whenever it kicks off an interrupt or grabs a spinlock. Preemption points decrease the variable and check if it's not zero. If it's not zero, a context switch isn't possible right now. Preemption gets the green light again only once all interrupts have been dealt with and all spinlocks have been released.
    
3. The ABA problem: In concurrent programming, the ABA (All-But-Aborted) problem occurs when a memory location is read twice, and both reads return the same value, but the value could have changed in between the two reads. This issue is particularly relevant when using compare-and-swap (CAS) operations. A thread might read a value A, get preempted, and then another thread might change the value from A to B and back to A. When the first thread resumes, its CAS operation will succeed because the value it checks against (A) matches the current value, even though the value has changed in the meantime. This can lead to incorrect or unpredictable behavior because the first thread is unaware of the changes that occurred while it was preempted. An example in our course demonstrated how a whole list was modified during preemption.

**ABA Problem** Occurs in concurrent programming, where a value read twice might seem unchanged but has actually been altered between reads, causing unpredictable behavior.



### Preemptive vs Non-Preemptive Kernel
#### **Preemptive Kernel**

- **Definition**: In a preemptive kernel, the kernel allows a process to be preempted while it is running in kernel mode. This means that the kernel can interrupt a process to give CPU time to another process.
- **Characteristics**:
    - **Task Switching**: Enables higher responsiveness as the system can switch tasks even when a process is executing in kernel mode.
Widely used in real-time operating systems where response time is critical.


## Synchronization

Developers with a thorough grasp of kernel concurrency can exploit advanced features like:

- **Fine-Grained Locking**: This involves locking at a more granular level, improving system performance and reducing contention.
- **Specialized Synchronization Primitives**: The kernel provides various primitives tailored for specific synchronization
- **Lockless Algorithms**: These allow for efficient data handling without traditional locking mechanisms.

### Locking


The key point of spin lock is that holding the lock for a short period of time. It's wise to hold the spin locks for less than the duration of 2 context switches, or just try to hold the spin locks for as little time as possible.

Spin locks can be used in interrupt handlers, whereas semaphores cannot be used because they sleep.



**Spinning locks** continuously poll the lock until it becomes available while a sleeping lock (semaphores in Linux) (more overhead) waits until it is notified that the lock is available. A spinlock is generally useful when critical section length are small  and the waiting time is short, otherwise it becomes inefficient.
On SMP (Symmetric Multi-Processing) machines the spinlock is the basic ingredient.
Variants of spinning locks are: 

- **Readwrite locks**: distinguish between readers and writers, where multiple readers can access an object simultaneously, whereas only one writer is allowed at a time. 
- **Seqlocks**: to prevent starvation of writers, a counter starting from 0 is used to track the number of writers holding the lock. Each writer increments the counter both at locking/unlocking phase. The counter permits to determine if any writes are currently in progress: if the counter is even, it means that no writes are taking place. Conversely, if the counter is odd, it indicates that a write is currently holding the lock. Similarly readers check the counter when trying to lock: if the counter is odd, it means busy wait. If even the reader does the work but before releasing, it checks if the counter changed (in case it does again the work).
- **Lockdep**: it enables to report deadlocks before they actually occur in the kernel, which is equally impacted by the problem of deadlocks. When `CONFIG_PROVE_LOCKING`, **lockdep** detects violations of locking rules keeping track of locking sequences, when spinlocks are acquired in interrupt handlers and also in process context and stuff like this.

#### Multi-CPU concurrency: MCS lock

**MCS (Mellor-Crummey and Scott) Locks** are a type of synchronization mechanism particularly noted for being **cache-aware spinlocks**: MCS locks solve the cache ping-pong problem by using a **queue system**. 

1. **Queue-Based Locking Mechanism**: MCS locks maintain a queue of waiting threads. Each thread in the queue spins on a **local** variable in the **local cache**, reducing the need for frequent memory access, which is common in traditional spinlocks.
2. **Localized Spinning**: In an MCS lock, a thread waits for a signal from its predecessor in the queue. This means that **it only needs to monitor a local variable**, which is likely to be in its cache line. 

![](images/d60763f841205475f264254d6fe1a4c3.png)

### Lock-free

This part delves into the realm of lock-free algorithms, which are essential for high-performance concurrent systems. 
#### Primitives and basic problems

Let's break down each part for a clearer understanding:

1. **Per-CPU Variables**: In a multi-core or multi-processor environment, the Linux kernel often needs to maintain data specific to each CPU. Per-CPU variables serve this purpose. They are used to store data that is not shared between CPUs, thereby eliminating the need for synchronization mechanisms like locks. This approach increases efficiency by reducing contention and cache coherence traffic. Each CPU accesses its own copy of the variable, ensuring faster and safer operations in a concurrent setting.
2. **The Atomic API**: Atomic operations are fundamental in concurrent programming, especially in kernel development. An `atomic` operation is executed entirely as a single, indivisible step, which is crucial in multi-threaded contexts. The atomic API in the Linux kernel provides a set of functions and macros that perform atomic operations, such as adding to, subtracting from, or setting a value. These operations are used to manipulate shared data safely between different threads or interrupt handlers without causing race conditions. 
3. **CAS (Compare-and-Swap) Primitive**: Compare-and-Swap is a powerful synchronization primitive used in multi-threaded programming. It is a single atomic operation that compares the contents of a memory location to a given value and, only if they are the same, modifies the contents of that memory location to a new given value. This atomicity is crucial to ensure that no other thread has altered the data between the steps of reading and writing. CAS is widely used in the implementation of lock-free data structures and algorithms. 
	- **The ABA Problem**: The ABA problem is a classical issue encountered in concurrent programming when using CAS operations. It occurs when a memory location is read (A), changed to another value (B), and then changed back to the original value (A) before a CAS operation checks it. The CAS operation sees that the value at the memory location hasn’t changed (still A) and proceeds, unaware that it was modified in between. This can lead to incorrect behaviour in some algorithms, as the change-and-reversion can have side effects or meanings that the CAS operation fails to recognize.

#### Readers/writers

The readers/writers problem is a common challenge in managing access to shared data by multiple threads where some threads read data (readers) and others modify it (writers).  
One synchronization technique used in the Linux kernel is the **Read Copy Update** (RCU) mechanism. 

The core idea of RCU is to allow readers to access data without any locks. In this way readers don't need to acquire locks, they can access shared data with minimal overhead, which is particularly advantageous in high-concurrency environments. 

When a writer needs to update a data structure, instead of modifying the structure in-place and potentially disrupting readers, RCU creates a copy of the structure and applies the changes to this copy. Once the updated copy is ready, the system switches pointers so that subsequent readers see the updated structure. The old data structure is not immediately freed; instead, it is marked for later reclamation once all pre-existing readers that might be accessing it have finished.

In the Linux kernel, RCU is widely used for various purposes, such as managing network routes, file system mounts, and other data structures where the pattern of frequent reads and infrequent updates is common.

#### Memory models

Memory models define how memory operations in one thread are seen by others in a multiprocessor environment. Different models have varying levels of order enforcement and visibility, influencing how multi-threaded programs behave and are synchronized. 

- **Sequential Model**: Theoretically the strongest but impractical, ensuring program and memory order are the same.
	- Defined by Leslie Lamport's in the 1979 paper: How to Make a Multiprocessor Computer That Correctly Executes Multiprocess Programs"
	- Define $<_p$ the program order of the instructions in a single thread and $<_m$ as the order in which these are visibile in the shared memory (also referred to as the happens-before relation).
	- A multiprocessor is called sequentially consistent iff for all pairs of instructions $\left(I_{p, i}, I_{p, j}\right)$ you have that $I_{p, i}<_p I_{p, j} \rightarrow I_{p, i}<_m I_{p, j}$
	- In practice the operations of each individual processor appear in this sequence in the order specified by its program.
- **Total Store Order (TSO)**: Used in x86 intel processors, involves FIFO store buffers allowing delayed writes. 
	- Processors use a local write queue, or **store buffer**, to mitigate the delays caused by memory latency when they issue a store command. Once a write operation is completed and the data reaches the shared memory, it becomes visible to all processors. 
	- Despite the sequence of operations, loads may appear to be reordered in relation to stores due to the buffering of write operations. This means that reads can sometimes retrieve updated values before the writes are fully propagated through the system.
	- To ensure sequentially consistent behavior, especially at crucial points within a program, processors can use specific instructions known as "memory barriers" like the `mfence` instruction. The `mfence` instruction acts as a full memory barrier, ensuring that each thread completes its pending write operations to memory before initiating any read operations.
- **Partial Store Order (PSO)**: Used in ARM, treats memory as if each processor has its own copy. Writes can be reordered, lacking a mechanism for simultaneous write visibility across processors.
	- The PSO model can allow data races due to the absence of a forced happens-before relation. Synchronization instructions like _release_ and _acquire_ are necessary to commit writes to shared memory and avoid data races. Release and acquire instructions are more efficient thatn fences 

#### Data races

Compilers can reorder instructions, affecting perceived memory models. 
High-level languages provide mechanisms to enforce certain memory orderings:

- **Fences/Barriers**: These are special instructions that prevent certain types of reordering of memory operations, ensuring that operations are completed as perceived in the program's order.

Fences help in avoiding incorrect read operations that might arise due to data races in concurrent environments.
##### Great powers, great responsibilities 

- **Performance Consideration**: Overuse of fences can lead to performance degradation as they restrict the processor's ability to reorder instructions for efficient execution.
- **Strategic Placement**: It's crucial to strategically place fences where necessary, rather than using them indiscriminately, to balance correctness and performance.

#### LKMM

The linux memory model is the least common denominator all CPU families that run the Linux kernel. At its core,  it's **essentially, PSO.**

The LKMM employs specific C language instructions, known as primitives, to manage memory ordering **across different threads**. Two such critical primitives are `smp_store_release` (notated as `W[release]`) and `smp_load_acquire` (notated as `R[acquire]`). 

These primitives play a pivotal role in establishing **happens-before** "relationship arcs" between concurrent threads. 



