# Convolution

[Convolutional Neural Networks](projects/polimi-notes/MSc(english)%20(WIP)/Artificial%20Neural%20Networks%20and%20Deep%20Learning%20(WIP)/src/03.Image%20Classification.md#Convolutional%20Neural%20Networks) are based on the convolution operation: multiply the kernel with the input matrix elements, and sum them up to write to the output matrix. 

```cpp
void convolution_cpu(input_type *input, const input_type *filter, input_type *output, int width, int height, int filter_size, int filter_radius) {

    // Iterate over each output pixel
    for (int outRow = 0; outRow < height; outRow++) {
        for (int outCol = 0; outCol < width; outCol++) {
            input_type value = 0.0f;  
            
            // Apply the filter 
            for (int row = 0; row < filter_size; row++) {
                for (int col = 0; col < filter_size; col++) {
                    
                    int inRow = outRow - filter_radius + row; 
                    int inCol = outCol - filter_radius + col;
                    
                    if (inRow >= 0 
	                && inRow < height 
	                && inCol >= 0 
	                && inCol < width) {
	                    value += 
                        filter[row * filter_size + col] * 
                        input[inRow * width + inCol];
                    }
                }
            }
            output[outRow * width + outCol] = value;
        }
    }
}
```

### CONV2D Computing

The convolution kernel benefits from using constant memory, which is optimized for situations where all threads read the same value, such as filter coefficients in convolution operations.

- **Serial Access:** Accessing the same constant memory location on each clock cycle significantly speeds up the operation since it avoids the latency of global memory access.
- **Memory Initialization:** The `__constant__` memory space is declared at the global level and can be accessed across multiple kernel launches without reinitialization.

Define symbols in the same file as the kernel to ensure they are recognized by the linker. Use the `extern` keyword for symbols declared in other compilation units.

```cpp
// Copy filter to constant memory
cudaMemcpyToSymbol(filter, hostFilter, sizeof(float) * KERNEL_SIZE);
//or more explicitly 
cudaMemcpyToSymbol(constant_filter, filter, FILTER_SIZE * FILTER_SIZE * sizeof(filter_type))
```

- **Symbolic Filter:** By treating the filter as a symbol rather than a variable, the kernel call simplifies as the filter does not need to be passed as a parameter.
- **Loop Unrolling:** With a fixed filter size, the compiler can optimize the convolution operation by unrolling loops, enhancing execution efficiency.


```cpp
// GPU filter for convolution CONSTANT MEMORY
__global__ void convolution_constant_mem_kernel(const input_type *__restrict__ input, input_type *__restrict__ output, const int width, const int height) {
  
  const int outCol = blockIdx.x * blockDim.x + threadIdx.x;
  const int outRow = blockIdx.y * blockDim.y + threadIdx.y;
  input_type value{0.0f};

#pragma unroll
  for (int row = 0; row < FILTER_SIZE; row++)

#pragma unroll
    for (int col = 0; col < FILTER_SIZE; col++) {
      const int inRow = outRow - FILTER_RADIUS + row;
      const int inCol = outCol - FILTER_RADIUS + col;
		if (inRow >= 0 && inRow < height && inCol >= 0 && inCol < width) {
	        value += constant_filter[row][col] * input[inRow * width + inCol];
	    }
    }
	output[outRow * width + outCol] = value;
}
```

### Coarsening 


Coarsening: each thread -> perform more work . 

1. **Increased Arithmetic Intensity:** By allowing each thread to perform more operations, the ratio of compute operations to memory operations increases, improving the overall performance on compute-bound tasks.
2. **Reduced Launch Overhead:** Fewer threads mean less overhead in thread scheduling and management on the GPU.
3. **Improved Memory Access Patterns:** Coarsening can lead to more coalesced memory accesses if the threads access adjacent memory locations, which is optimal for GPUs.
4. **Better Resource Utilization:** Maximizing the work each thread does can lead to more efficient use of the GPU's cores, which might otherwise be underutilized in a finely-grained parallel scheme.

In the context of convolution operations, coarsening can be applied by having each thread compute multiple output elements instead of one. Below is a simplified example of how you might implement a coarsened approach in a GPU kernel for a convolution operation:

```cpp
__global__ void convolution_constant_mem_coarsening_kernel(const input_type *__restrict__ input,input_type *__restrict__ output,const int width,const int height) {

  // Calculate the starting column and row for the current thread
  const int outCol = blockIdx.x * blockDim.x + threadIdx.x;
  const int outRow = blockIdx.y * blockDim.y + threadIdx.y;

  // Calculate the strides for columns and rows based on the grid dimensions
  const int stride_col = gridDim.x * blockDim.x;
  const int stride_row = gridDim.y * blockDim.y;

  // Loop over assigned columns and rows using stride values
  for (int c = outCol; c < width; c += stride_col) {
    for (int r = outRow; r < height; r += stride_row) {
      input_type value = 0.0f; 
      #pragma unroll
      for (int row = 0; row < FILTER_SIZE; row++) {
        #pragma unroll
        for (int col = 0; col < FILTER_SIZE; col++) {
          const int inRow = r - FILTER_RADIUS + row; // Compute input row index
          const int inCol = c - FILTER_RADIUS + col; // Compute input column index

          // Check boundary conditions and perform convolution
          if (inRow >= 0 && inRow < height && inCol >= 0 && inCol < width) {
            value += constant_filter[row][col] * input[inRow * width + inCol];
          }
        }
      }
      output[r * width + c] = value; // Store the computed value in the output image
    }
  }
}

```

- **Coarsening Factor:** Each thread computes a 2x2 block of the output image, effectively coarsening the thread's workload by a factor of four.
- **Boundary Conditions:** The code checks whether each output pixel location falls within the image boundaries before performing the convolution.
- **Nested Loops for Convolution:** The actual convolution computation involves nested loops over the filter dimensions and applies the filter to the relevant part of the input image.


`stride_col` and `stride_row` to skip across the image based on the grid size. The stride depends on the number of spawned threads. 


**Shared Memory Optimization Techniques**

Using shared memory in this way minimizes the reliance on global memory and speeds up the convolution by allowing faster access to input data.

Using a shared memory approach with tiling significantly alters how data is accessed and processed on a GPU, emphasizing efficiency and reduced global memory dependency. This method is particularly effective for operations like convolution where data reuse is high. The approach minimizes the latency of data access and maximizes the throughput by leveraging the fast shared memory on the GPU.

**First Load the Tiles:** Each thread in a block loads an element of the input image into shared memory, including necessary padding for the convolution operation. This reduces global memory accesses. After loading the data into shared memory, threads are synchronized to ensure all data is properly loaded before computation begins.
   ```cpp
   __shared__ input_type input_shared[IN_TILE_DIM][IN_TILE_DIM];
   if (inRow >= 0 && inRow < height && inCol >= 0 && inCol < width) {
     input_shared[tidy][tidx] = input[inRow * width + inCol];
   } else {
     input_shared[tidy][tidx] = 0.0;
   }
   __syncthreads();
   ```

**Compute Output Elements:** Each thread computes an element of the output using the data in shared memory. This computation is localized to the data loaded by the block, significantly reducing the latency associated with memory access.

   ```cpp
   const int tileCol = tidx - FILTER_RADIUS
   const int tileRow = tidy - FILTER_RADIUS
   
   if (tileCol >= 0 && tileCol < OUT_TILE_DIM && tileRow >= 0 && tileRow < OUT_TILE_DIM) {
     input_type output_value{0.0f};
     #pragma unroll
     for (int row = 0; row < FILTER_SIZE; row++)
     #pragma unroll
       for (int col = 0; col < FILTER_SIZE; col++)
         output_value += constant_filter[row][col] * input_shared[tileRow + row][tileCol + col];
     output[inRow * width + inCol] = output_value;
   }
   ```



### Caching Halo cells

![](images/Pasted%20image%2020240423190143.png)

During execution, only the internal part of the tile is loaded into shared memory. The assumption is that halo cells are potentially available in L2 cache, having been loaded by adjacent blocks in previous operations.

In the inner loop:

```cpp
if (sharedRow >= 0 && sharedRow < TILE_DIM 
	&& sharedCol >= 0 && sharedCol < TILE_DIM) {
		//shared part
		PValue += constant_filter[fRow][fCol] * input_shared[sharedRow][sharedCol];
} else {
	// Global memory
	int globalRow = row - FILTER_RADIUS + fRow;
	int globalCol = col - FILTER_RADIUS + fCol;
	if (globalRow >= 0 && globalRow < height 
		&& globalCol >= 0 && globalCol < width) {
		PValue += constant_filter[fRow][fCol] 
			* input[globalRow * width + globalCol];
	}
}
```


Due to conditional statements handling halo and boundary conditions there is a **a lot of Branch Divergence**, which increase instruction count and reduce performance.
**Compiler's Role:** the compiler generates a substantial number of instructions to manage this complexity, which can aggravate thread divergence.