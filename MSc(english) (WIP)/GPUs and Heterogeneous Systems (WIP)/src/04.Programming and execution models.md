# Programming and execution models

The overview of an execution of a CUDA program is basically this:

1. **Data Transfer**: Input data is copied from CPU to GPU memory.
2. **Kernel Execution**: The GPU program (kernel) is loaded and executed.
3. **Result Retrieval**: Computed results are transferred back from GPU to CPU memory.


In the CUDA program, we define a new kernel function and include the CUDA header. After declaring variables and arrays on the CPU side, we allocate memory on the device, transmit data, launch the kernel, wait for its completion, and send back results. The kernel function works on a single data element and has some CUDA-specific restrictions.


- **Device Memory Allocation**: Separate from host memory, requiring explicit allocation and deallocation.
- **Data Transfer**: Explicitly managed, requiring data to be moved between host and device memory for processing and retrieval.



Memory transfer is essential for running CUDA programs on GPUs. Here's a summary of the process:

1. Write a CUDA program with a similar structure to CPU code, including defining a new kernel function and using CUDA APIs.
2. The main program runs on the CPU, loading data, transmitting it to the device, launching the kernel, waiting for its completion, and sending results back to the host memory.
3. The kernel function, running on the GPU, cannot access host memory directly and is limited to void return type, asynchronous behavior, and working on a single data element.
4. The program is integrated in a single file with both device and host code using the .CU extension.


## CUDA Thread Hierarchy

Kernels are launched with specific execution configurations, defining grid and block dimensions. These configurations are crucial for optimizing performance and resource utilization on the GPU.

In CUDA, kernels execute across a set of parallel threads, which are organized into thread blocks. These blocks are then structured into a grid that can be up to three-dimensional, reflecting the problem's data structure and computational requirements.


The code is executed by each thread within a grid, which is composed of blocks. The giga thread scheduler assigns blocks to different streaming multi-cores or streaming multi-processors, each using the same instruction stream for their warps. 

Each thread is identified by a block number and thread number, and works on a single scalar sum (for example) with a specific data element. 

We map each thread to a single data element and its corresponding input elements, creating a flattened index for accessing the input and output arrays. 

blockldx: id of the block in the grid
blockDim: size of the block (#threads)
threadldx: id of the thread in the block
gridDim: size of the grid (#blocks)



kernel -> grid of blocks -> each blocks contains a grid (2d or 3d) of threads.


Threads are organized in a grid with blocks and threads, with possible 2D, 3D structures depending on the problem. 



## First example CUDA code


One or many independent functions (called **kernels**) parallelized on the device (i.e., the GPU) on a set of multiple threads.

We use a single thread on the CPU to control offloading computation to multiple concurrent threads on the GPU. 

Input data is decomposed into separate elements, and each thread performs the kernel function on its corresponding data element. For example, summing two vectors, we reshape the code to have each thread sum a single position of the vectors. 

The CPU, or host, copies input data to the GPU, or device, explicitly via API, loads and compiles the GPU program, launches the kernel, and then receives results back synchronously.

three pointers refer to device memory, not host memory. To invoke a function on the GPU, we specify the address of three vectors declared in the stack of a C program in the host memory, and add the `___global___` qualifier for GPU execution. Functions with "device" qualifier are executable on the device, while "host" functions run on the host. Some functions can be both host and device.

| Function Qualifier | Executed on | Only Callable From |
|-------------------|-------------|--------------------|
| `__device__`      | Device      | Device             |
| `__global__`      | Host        | Device             |
| `__host__`        | Host        | Host               |

The original three arrays are located in the CPU stack, and we need to add three new pointers for the GPU memory, which cannot be accessed directly by the main function. 


We replicate structures on both CPU and GPU, allocate memory on the device side using CUDA malloc, and copy data from the CPU to the GPU using CUDA memcopy.


We spawn 1024 threads, each computing a single sum, replacing the for loop with a grid of threads using block ID, thread ID, and grid dimensions as variables. Each thread receives unique values based on its context. We have one-dimensional grids with X, Y, and Z attributes, and use the X field for our mapping function. Within each block, there are 256 threads mapped to the same streaming multiprocessor, which are further divided into warps. The I variable is a local variable for each thread. We have 1024 I variables, each private for the specific thread context.


In CUDA programming, when transferring data from the host to the device for arithmetic instructions on the CPU side, we use the `cudaMemcpy()` function with the transmission direction set to "from host to device." The function is a blocking function, meaning control is returned to the main program once the transmission is completed.


```c++
cudaMalloc (&dev_va, N*sizeof (int));
// CPU—>GPU data transmission
cudaMemcpy(dev_va, host_va, N*sizeof (int), cudaMemcpyHostToDevice);

// kernel launch
dim3 blocksPerGrid (N/ 256, 1,1);
dim3 threadsPerBIock (256, 1,1);

//actual function
vsumKerne1<<<b10cksPerGrid, threadsPerBIock>>> (d_va, d_vb, d_vc); 

// GPU—>CPU data transmission * /
cudaMemcpy(host_vc, dev_vc, N*sizeof (int), cudaMemcpyDeviceToHost) ;

// device memory freeing
cudaFree (d_va) ;
cudaFree (d_vb) ;
cudaFree (d_vc) ;
```

## Compilations, debugging and profiling

- CUDA programs are compiled using the `nvcc` compiler, which translates CUDA code into PTX (Parallel Thread Execution) intermediate assembly, further compiled just-in-time by the GPU driver to optimize for the specific GPU hardware.
- Developers can control compilation for specific compute capabilities, allowing for design-time optimizations and runtime adaptability to leverage the full capabilities of the hardware.


To compile: `nvcc vector_sum.cu -o vector_sum` 

To execute: `./vector_sum`


### Error Handling and Debugging

Every CUDA function returns a flag • The flag can be used for error handling

- CUDA API calls return error codes that must be checked to ensure correct execution. Special macros can be defined for error checking to simplify code and improve readability.
- Kernel launches are asynchronous, and their execution status needs to be checked using specific CUDA API calls to handle errors within kernel execution.


CUDA functions return flags for error handling, and specific macros can be defined to check these flags conveniently. Kernel launches require separate error checking mechanisms due to their asynchronous nature.

### Profiling and Tuning

To achieve optimal performance, CUDA programs often require profiling and tuning to identify and address bottlenecks. This process involves iterative adjustments to grid and block sizes, memory access patterns, and other critical parameters, guided by profiling tools and best practices.


To query GPU information and understand the capabilities of the available GPU device(s) in your system using CUDA, you can follow these code snippets:

1. **Include CUDA Runtime Header**
   ```c
   #include <cuda_runtime.h>
   ```

2. **Declare Device Properties Structure and Device ID Variable**
   ```c
   cudaDeviceProp devProp;
   int devId;
   ```

3. **Get Current Device ID**
   ```c
   cudaGetDevice(&devId);
   ```

4. **Retrieve Device Properties**
   ```c
   cudaGetDeviceProperties(&devProp, devId);
   ```

5. **Print Relevant Information**
   ```c
   printf("Device Name: %s\n", devProp.name);
   printf("Compute Capability: %d.%d\n", devProp.major, devProp.minor);
   printf("Total Global Memory: %lu\n", devProp.totalGlobalMem);
   printf("Max Threads per Block: %d\n", devProp.maxThreadsPerBlock);
   ```

This code snippet will help you quickly identify key characteristics of the GPU, such as its name, compute capability, total global memory, and maximum threads per block, which are crucial for optimizing CUDA applications.


#### Thread and Data Mapping in CUDA

In CUDA, the concept of mapping threads to data is crucial. Data parallel functions on the GPU decompose data into single items processed by threads. For output data decomposition, threads are mapped to single output data items, suitable for transformations like matrix multiplication and image color conversion. Input data decomposition involves mapping threads to input data items, useful for functions like frequency count in text analysis.

#### Grid Dimensionality and Data Linearization

The thread grid in CUDA can have up to three dimensions, selected based on the problem's nature: 

- 1D problems involve vector operations
- 2D for matrices or images
- 3D for volumetric data

Since C/CUDA supports only 1D dynamic memory allocation, multidimensional data are also linearized in the device memory

Even if the grid can be defined with up to 3 dimensions, the grid is internally linearized with a row major layout. 

This slide simplifies the discussion by not mentioning blocks • Linearization is applied 1) per blocks and 2) per block threads


#### CUDA Thread Hierarchy and Execution

CUDA organizes threads hierarchically in grids, grouped into blocks, with each block assigned to a single streaming multiprocessor (SM). Blocks can be executed in any order, promoting a scalable architecture. The SM schedules **warps** (32 threads each) for execution, with the execution model allowing for interleaved execution of warps to hide memory access latency.

The SM divides the block in active warps (32 threads each) • A warp is executed with a SIMT approach • Execution of several warps is interleaved

Selected warp – if currently executed – Stalled warp – if not ready to be executed – Eligible warp – if ready to be executed

Warp schedulers will select a warp to execute from eligible ones


Blocks may be dispatched to SMs at different instants of time • Warps in each block may be scheduled in any order in the SMs

Key to performance is ensuring that the number of active warps is maximized, with systematic profiling used to tune block size and grid size.

#### Handling Non-Multiple Data Sizes

CUDA programming often requires handling data sizes that are not multiples of the preferred thread grid dimensions. Techniques include rounding up the number of blocks and ensuring the block size is a multiple of 32 for performance reasons, even though this may lead to some threads in the last warp doing no work.

GPU Performance Optimization

GPU is designed with throughput in mind, aiming for the best performance through optimal resource utilization. Achieving high resource utilization involves interleaving many warps within a Streaming Multiprocessor (SM). This strategy allows for latency hiding, where the SM can switch to executing another warp when one stalls.

Maximizing SM Occupancy

The initial step towards minimizing kernel execution time involves maximizing SM occupancy, calculated as the ratio of active warps to the maximum warps an SM can support. Following this, a systematic profiling of kernel execution is essential, focusing on aspects like resource utilization and memory accesses to achieve optimal performance.

Occupancy Maximization Constraints

The maximization of occupancy is subject to constraints related to block resource requirements:

- **Registers**
  - To retrieve the number of registers: `--ptxas-options=-v`
  - To set the number of registers: `-maxregistercount=NUM`
- **Shared Memory**
  - The amount of shared memory is determined directly from the source code.

Manual Block Size Tuning Guidelines

When manually tuning the block size, consider the following guidelines:

- Ensure the block size is a multiple of the warp size.
- Avoid using small block sizes.
- Adjust the block size according to the kernel's resource requirements.
- The total number of blocks should significantly exceed the number of SMs.
- Engage in systematic experiments to find the best configuration.

Note that the tuning process becomes more complex when dealing with 2D or 3D blocks and grids, requiring careful consideration and adjustments.



#### Warp Divergence and Kernel Execution

Warp divergence is another factor of performance degradation – It happens when threads in the same warp takes different directions during the execution of a branch or loop statement


Strategies to minimize divergence include organizing data to ensure threads in the same warp perform similar operations. The flow of CUDA calls involves enqueuing commands in a stream, with synchronization points used to manage execution order.



#### Synchronization and Performance Measurement

CUDA provides mechanisms for synchronizing threads within blocks and across the grid, though grid-level synchronization requires splitting the kernel into multiple parts. 

CUDA calls may be: – Blocking • The host program is blocked until the function returns • E.g.: cudaMemcpy – Non-blocking • The host program continues asynchronously its execution • E.g.: kernel launch

A barrier can be used to force the host to wait the termination of the kernel execution

Performance measurement can be achieved using both CPU and GPU timers, with GPU timers offering finer granularity and the ability to measure time directly on the device.



### Sync 


Threads cannot share data without synchronization; usually, a barrier is used. Synchronization in CUDA is explicitly invoked by the programmer as the GPU is designed for high performance with no automatic synchronization mechanism. 

Logically, threads are concurrent, but the execution order is unpredictable.

Synchronization is performed using barriers that synchronize threads in the same block. 

`__syncthreads()` is the barrier call. 

There is no barrier call applicable among threads of different blocks at grid level; they can only be invoked at the grid level. 

This for semplicity in the design of the GPU: the GPU architecture does not feature inter-SM synchronization mechanisms. 

To measure kernel performance, we can use CPU timers or a GPU timer to record the execution time of a kernel, including the launch time and execution time on the GPU.

