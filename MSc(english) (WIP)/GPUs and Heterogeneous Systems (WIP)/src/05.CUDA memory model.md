
# CUDA memory model

The CUDA **memory model** is explicit to the programmer: caches are not entirely transparent, demanding a deeper understanding of memory hierarchy and access patterns.
The device memories are mainly:

- **registers**: registers are fast, private, and limited
- **caches**: non-programmable memories transparent to the programmer:
	- L1 private to each streaming multi-processor
	- L2 private per SM
- **device memory**: (aka global or video memory) is slower and accessible by all threads and the host. 

![](images/Pasted%20image%2020240316171201.png)


Also CUDA introduces the concept of **memory spaces** which help the programmer explicitly manage the placement and accessibility of variables.
The three most important keywords are:

- `__global__`: Variables declared with the `__global__` qualifier reside in global memory and can be accessed by all threads and the host.
- `__shared__`: Variables declared with the `__shared__` qualifier reside in shared memory and can be accessed by all threads within the same block.
- `__constant__`: Variables declared with the `__constant__` qualifier reside in constant memory and can be accessed by all threads and the host.


In addition to the memory types and spaces, CUDA also provides **memory management functions** for allocating, copying, and freeing memory on the device. These functions include:

- `cudaMalloc()`: Allocates memory on the device.
- `cudaFree()`: Frees memory allocated on the device.
- `cudaMemcpy()`: Copies data between host and device memory, or between different memory spaces on the device.
- `cudaMemcpyAsync()`: Performs an asynchronous memory copy, allowing overlap of memory transfers with computation.

## Shared memory

**Shared memory** is a fast, on-chip memory that is shared among all threads within a block. It is a programmer-managed memory and can be used to efficiently share data between threads in the same block. 

Shared memory is divided into equally-sized memory banks: it's crucial to avoid bank conflicts, which occur when multiple threads access different addresses within the same bank. 

Shared memory supports broadcasting: in the case of a single address read by threads in the same block (1 transaction). 

Proper usage of shared memory involves carefully designing access patterns to minimize bank conflicts and maximize memory bandwidth utilization.

An example of the use of shared memory is the **matrix-matrix Multiplication**:

- Compute each element of the output matrix as a product of a row from the first matrix and a column from the second matrix.
- Typically implemented using three nested for loops:
    - Two outer loops scan the entire output matrix.
    - One inner loop performs the dot product.
- Using Shared Memory:
  - Transfer the first strip of matrix $M$ to shared memory once.
  - This allows multiple accesses by threads, which is more efficient than accessing global memory.
  - Shared memory can handle broadcast access better than global memory can handle transactions.
- Optimizing with Sub-Regions:
  - Although shared memory is limited, it can be divided into sub-regions for computation.
  - Computing the first sub-region against the second can optimize memory usage and computational efficiency.


```mermaid
graph TB
    A[Matrix Multiplication] --> B[Row from first matrix Ã— Column from second matrix]
    B --> D[Three nested for loops]
    D --> E[Two outer loops: Scan output matrix]
    D --> F[Inner loop: Compute dot product]
    B --> G[Use shared memory]
    G --> H[Transfer first strip of matrix to shared memory]
    H --> I[Efficient multiple accesses by threads]
    I --> J[Divide shared memory into sub-regions]
    J --> K[Optimize memory usage and computation]

```


## Constant memory

**Constant memory** is a read-only memory that is cached and optimized for broadcast access patterns, where all threads in a warp read the same memory location. It is ideal for storing constants or lookup tables that do not change during kernel execution. 

Constant memory is limited in size (typically 64KB) and is accessible by all threads and the host. To use constant memory, variables must be declared with the `__constant__` qualifier and initialized on the host side using `cudaMemcpyToSymbol()`.

## Texture memory

**Texture memory** is a read-only memory that is optimized for spatial locality and can be used to accelerate access to data with 2D or 3D locality.
Texture memory provides caching, addressing, and filtering capabilities that can improve performance for certain access patterns. Textures are bound to a specific memory region and can be accessed using special texture fetching functions within the kernel. 

Texture memory is particularly useful for image processing, computer vision, and applications that exhibit spatial locality in their data access patterns.

## Global memory 

Global memory usage in CUDA is used ensuring aligned memory accesses:

- GPUs read an entire line or sector when a single value is requested.
- In newer architectures, lines are divided into 32-byte sectors.
- Transactions must be aligned with the base address of the data (multiple of 32 bytes)
- Multiple accesses to the same cache line are combined into a single transaction.
- So, the optimal access strategy is to access 32 consecutive addresses (32-byte aligned) to minimize transactions.

When all accesses request values from the same cache line, efficiency is maximized. The worst-case scenario is when threads access random addresses in global memory: it may result in a separate transaction for each value. 

### Efficiency of Array of Structures vs. Struct of Arrays

Let's compare Array of Structures (AoS) with Structure of Arrays (SoA):

![](images/Pasted%20image%2020240418111519.png)

```cpp
typedef struct {
    float x;
    float y;
} innerStruct_t;

innerStruct_t myAoS[N];
```

```cpp
typedef struct {
    float x[N];
    float y[N];
} innerArray_t;

innerArray_t mySoA;
```

SoA in the example of RGB to grayscale conversion: data into separate arrays for red, green, and blue yields higher efficiency. SoA achieves 100% efficiency compared to only 33% with AoS.

The reason about that is that in SoA all the elements in the array are consecutive in memory. 

### Instructions for synchronization

**Atomic instructions** are used to perform read-modify-write operations on global memory or shared memory, ensuring that no other thread can access the memory location simultaneously. These instructions are commonly used for synchronization and coordination among threads in a CUDA program.

| OPERATION          | FUNCTION     | SUPPORTED TYPES                                  |
| ------------------ | ------------ | ------------------------------------------------ |
| Addition           | `atomicAdd`  | int, unsigned int, unsigned long long int, float |
| Subtraction        | `atomicSub`  | int, unsigned int                                |
| Unconditional Swap | `atomicExch` | int, unsigned int, unsigned long long int, float |
| Minimum            | `atomicMin`  | int, unsigned int, unsigned long long int        |
| Maximum            | `atomicMax`  | int, unsigned int, unsigned long long int        |
| Increment          | `atomicInc`  | unsigned int                                     |
| Decrement          | `atomicDec`  | unsigned int                                     |
| Compare-And-Swap   | `atomicCAS`  | int, unsigned int, unsigned long long int        |
| And                | `atomicAnd`  | int, unsigned int, unsigned long long int        |
| Or                 | `atomicOr`   | int, unsigned int, unsigned long long int        |
| Xor                | `atomicXor`  | int, unsigned int, unsigned long long int        |

The **shuffle instruction** enables threads within the same warp to exchange data by directly accessing each other's registers without relying on shared or global memory. This allows for efficient data sharing and communication within a warp. The shuffle instruction is particularly useful when threads need to exchange data with other threads in the same warp without the overhead of using shared memory or global memory.

```cpp
int __shfl_sync(unsigned mask, int var, int srcLane, int width=warpSize);
```

with:

- `mask`: A 32-bit integer representing the threads participating in the shuffle operation. Each bit corresponds to a thread in the warp, and only threads with their corresponding bit set will participate in the shuffle.
- `var`: The variable or register containing the data to be exchanged among threads.
- `srcLane`: The lane (thread) from which the data will be read. Each thread can specify the lane it wants to read data from.
- `width`: The number of threads participating in the shuffle operation. By default, it is set to the warp size (32).

Threads can perform data exchange, reduction operations, or other collaborative computations within a warp.

## Memory optimizations

It's important to consider strategies to accelerate parallel patterns studied in TA classes.

- **Maximizing occupancy**. The goal is to hide memory latencies by occupying all the resources, including context, words, and threads. This allows the GPU scheduler to continue scheduling and executing words, hiding the latency of memory operations from the compute and memory sides.
- **enabling coalesced global memory accesses**. This involves avoiding wasted memory accesses, which are long operations. By enabling course coalesced global memory accesses, fewer stores are required in the computation, making the memory accesses more efficient. Data from global -> shared only once. 
- **Tiling** is useful for matrix multiplication but can also be used for other parallel patterns across all the data. Tiling involves dividing a large data set into smaller chunks, or tiles, and processing them in parallel. This can help to hide memory latencies and improve occupancy.
- **Privatization** reduces the need for resources and minimizes contention and serialization of atomic operations. In the example of computing a histogram, privatization can be applied by having each thread or warp compute **partial** histograms in their own private space in shared or local memory. This approach reduces the frequency of atomic updates to global memory, subsequently reducing the number of atomic conflicts. These local histograms are then combined, enhancing performance by decreasing atomic operations/conflicts.
- **Coarsening**, reducing the size of the problem, is also discussed as a method for improving performance. It refers to increasing the amount of work done by each thread in a kernel. Instead of having many threads doing small amounts of work, fewer threads do more work each. For instance, in the context of processing an image, instead of assigning one thread per pixel, thread coarsening would assign one thread to handle a block of pixels, reducing the total number of threads and potentially making better use of the GPU's resources.


| Optimization                              | Strategy                                                                                                                                                                                                    | Benefit to compute cores                                      | Benefit to memory                                                       |
| ----------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------- | ----------------------------------------------------------------------- |
| Maximizing occupancy                      | Tuning usage of SM resources such as threads per block, shared memory per block, and registers per thread                                                                                                   | More work to hide pipeline latency                            | More parallel memory accesses to hide DRAM latency                      |
| Enabling coalesced global memory accesses | Transfer between global memory and shared memory in a coalesced manner and performing uncoalesced accesses in shared memory; Rearranging the mapping of threads to data; Rearranging the layout of the data | Fewer pipeline stalls waiting for global memory accesses      | Less global memory traffic and better utilization of bursts/cache lines |
| Minimizing control divergence             | Rearranging the mapping of threads to work and/or data; Rearranging the layout of the data                                                                                                                  | High SIMD efficiency (fewer idle cores during SIMD execution) | -                                                                       |
| Tiling of reused data                     | Placing data that is reused within a block in shared memory or registers so that it is transferred between global memory and the SM only once                                                               | Fewer pipeline stalls waiting for global memory accesses      | Less global memory traffic                                              |
| Privatization                             | Applying partial updates to a private copy of the data and then updating the universal copy when done                                                                                                       | Fewer pipeline stalls waiting for atomic updates              | Less contention and serialization of atomic updates                     |
| Thread coarsening                         | Assigning multiple units of parallelism to each thread to reduce the price of parallelism when it is incurred unnecessarily                                                                                 | Less redundant work, divergence, or synchronization           | Less redundant global memory traffic                                    |
