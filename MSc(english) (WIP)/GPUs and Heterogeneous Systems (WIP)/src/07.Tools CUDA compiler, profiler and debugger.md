---
tags:
  - pragma
---
# Tools CUDA compiler, profiler and debugger


Nvidia's documentation is comprehensive and well-maintained: 

- Use the [CUDA Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html) extensively for detailed information.
- For specific CUDA API references, consult the [Runtime API Documentation](https://docs.nvidia.com/cuda/cuda-runtime-api/index.html).
- Access the [full CUDA Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html) for an in-depth understanding.

**CUDA API** exposes both the Runtime and the Device API.
  - We will focus on the **CUDA Runtime API**:
    - The Runtime API simplifies device code management by providing implicit initialization, context management, and module management, leading to simpler code but less control than the Driver API.
    - The Device API allows for more fine-grained control, particularly over contexts and module loading, making kernel launches more complex. It's more bare metal. 


The important difference is that you use the device API to build all the abstraction you will need. 
### Best Practices - 1

- Use the `__restrict__` keyword to suggest that pointers point to unique memory.
- If something is constant, use `const`.
- Avoid global variables:
  - They can cause issues with multiple compilation units.
- Prefer compile-time computations:
  - Utilize `macros` and `constexpr` for efficiency.
- Utilize CHECK functions to verify execution status:
  - Remember that CUDA focuses on performance, not safety.
  - Understand the differences between synchronous vs asynchronous errors.
  - **Tip**: Enable checks only in debug mode for better performance.
- Use `inline` cautiously:
  - Compilers treat it as a hint rather than a guarantee.
  - For force inlining, use `__attribute__((always_inline))`.
- Estimating grid's dimensions can be done at runtime:
  - Use the API `cudaOccupancyMaxActiveBlocksPerMultiprocessor`.
  - An example will be provided later.
- Be mindful of `#pragma unroll`:
  - It can boost performance but also increase register usage.
- Pay attention to synchronization functions:
  - Use `__syncthreads` or `__syncwarp` where necessary.
  - Fences (`__threadfence` and its variants) might also be options.



Based on the contents of the PDF, here is the organized information under the specified headings in markdown format:

## Compiling

### NVCC (NVIDIA C Compiler)

**NVCC Compiler:** handles both host (CPU) and device (GPU) code, generating a Fat Binary output containing PTX (Intermediate Representation) and Cubin (GPU executable code).
The compilation output

- CUBIN: Contains executable binary code for a real architecture. Cubin is used for specific architectures, optimizing for that device.
- PTX: Intermediate representation for a virtual architecture.
- FATBIN: Embeds PTX and CUBIN in the executable.


 Compile for virtual and specific architectures using `nvcc` with flags like `-arch=compute_50` or `-arch=sm_50`.




Compilation Types:
    - **Just-in-Time (JIT):** PTX is included, runtime Cubin generation introduces overhead.
    - **Ahead-of-Time (AOT):** Cubin is directly in the executable, avoiding runtime compilation.


## Debugging

**CUDA-GDB:** Interactive debugger for CUDA code.
    - Requires `-g` and `-G` flags during compilation for debugging kernel code.
    - Supports breakpoints, switching execution context, and examining variables within kernels.


When you execute NVCC, your code is divided into device code and host code. The device code goes to the device compiler, and you get what's so-called Fat binary, of code FAT binary output all together, and you get your output file, which is the one you execute.

CUDA Binary Utilities

CUDA binary files are ELF-formatted and consist of executable code sections.

NVCC embeds cubin files into the host executable.


**CUDA objdump, cuobjdump, nvdisasm:** Tools for analyzing CUDA binaries, examining PTX or Cubin code for optimization purposes.

### CUOBJDUMP & NVDISASM

- CUOBJDUMP: Extracts and formats information from CUDA binary files.
- NVDISASM: Similar to CUOBJDUMP but for standalone cubin files.

## Profiling and Metrics

### The old way: Visual Profiler and NVPROF

- NVPROF command-line tool for profiling data.
- NVIDIA Visual Profiler (NVVP) for visualization and optimization.
- Support ended with the Volta architecture.
- 
- **Profiling Tools:**
    - **NVIDIA System Management Interface (NSMI):** Legacy tool, still in use.
    - **NVIDIA Visual Profiler (NVVP):** Older, graphical tool.
    - **Nsight Compute (NCU):** Modern, more comprehensive profiler. Provides both command-line and GUI interfaces.

NSIGHT Profiling Tools

- NSIGHT SYSTEMS for system-wide profiling.
- NSIGHT COMPUTE for interactive kernel profiling.
- Inclusion in CUDA toolkit with references [here](https://docs.nvidia.com/nsight-systems/UserGuide/index.html) and [here](https://docs.nvidia.com/nsight-compute/2023.3/).

![](images/Pasted%20image%2020240418131204.png)



**CUPIT and NVTX** also exist CUPTI & NVTX.

## Roofline

The Roofline Performance Model is a visual tool for analyzing performance bottlenecks and potential optimizations:

- **Computational Rooflines:** Represent limits based on operation type (floating-point, integer, fused multiply-add).
- **Memory Rooflines:** Indicate bandwidth limitations for different memory hierarchies (DRAM, L1/L2 caches, shared memory).


Example:

![](images/Pasted%20image%2020240418131915.png)

Provides performance estimates based on computational and bandwidth peak performance.

Evaluates kernels or applications in terms of Arithmetic Intensity (AI).


A roofline is basically a visual model that you can use to identify which are the bottlenecks in your program. 

You want both to reach the upper limit of the performance you've got and also to shift on the right in order to increase the operational/arithmetic intensity. 

two floating-point operations (FLOP)
And you will end performing two operations, multiplication and summation. 

The roofline model is a visual representation used to understand the performance limits of computer systems, particularly for high-performance computing (HPC) applications. It illustrates the trade-off between computational capabilities and memory bandwidth.

The speaker also discusses the hierarchy of GPU memory, suggesting that to maximize performance, one should strive to bring data as close to the computation as possible, utilizing caches more effectively to alleviate the bottleneck caused by slower global memory accesses. The superposition of rooflines for different memory hierarchies is shown in the image, indicating the performance one can achieve based on the arithmetic intensity of the application and the hierarchy of memory being used.



We can use Nsight Compute to get a roofline of any kernel. 


Maximize compute performance: 

Multithreading
Vectorization
Increase SM occupancy
Utilize FMA instructions Fused Multiply-Add. It is a type of CPU or GPU instruction that combines multiplication and addition operations into a single instruction ``result = (a * b) + c``
Minimize thread divergence


In CUDA programming, the memory access pattern is crucial for achieving optimal performance. When threads access memory, it's beneficial for their accesses to be "coalesced," meaning that consecutive threads access consecutive memory addresses. This allows the memory transactions to be combined into fewer operations, which maximizes bandwidth utilization and can significantly improve the performance of the GPU.

Here's a summary of the provided code snippets, translated into a markdown note for better readability:

```markdown
# CUDA Memory Access Patterns

Understanding how threads access memory in CUDA is essential for writing efficient GPU kernels. There are two primary ways these memory accesses can be handled:

## Non-Coalesced Access

In the naive version of the SGEMM (simple general matrix multiply) kernel, we see memory access patterns that can lead to non-coalesced access:

```cpp
__global__ void sgemm_naive(int M, int N, const float *A, const float *B, float beta, float *C) {
    const uint x = blockIdx.x * blockDim.x + threadIdx.x;
    const uint y = blockIdx.y * blockDim.y + threadIdx.y;
    // ... rest of the kernel ...
}
```

The variables `x` and `y` calculate global thread indices in two dimensions. However, this may result in non-coalesced memory access if `blockDim.x` is not a multiple of the memory coalescing granularity (e.g., 32 for current CUDA architectures).

## Coalesced Access

To ensure that consecutive threads access consecutive memory locations, we can adjust the indexing strategy:

```cpp
__global__ void sgemm_global_mem_coalesce(int M, int N, const float *A, const float *B, float beta, float *C) {
    const int cRow = blockIdx.x * BLOCKSIZE;
    const int cCol = blockIdx.y * BLOCKSIZE;
    const int x = cRow + (threadIdx.x / BLOCKSIZE);
    const int y = cCol + (threadIdx.x % BLOCKSIZE);
    // ... rest of the kernel ...
}
```

In this optimized version, we assume a 2 D grid of threads where `BLOCKSIZE` is tuned to the architecture's coalescing requirements. Each thread calculates its unique `x` and `y` coordinates that correspond to matrix elements in `C`. The division and modulo operations ensure that threads within the same warp access contiguous memory locations, thus ensuring memory coalescence.

**Key Takeaway:**

Memory coalescence is a performance-critical aspect in CUDA programming. Ensuring that consecutive threads access consecutive memory locations can significantly improve the performance of memory-bound kernels, like those used in matrix multiplication. The correct calculation of thread indices is essential for achieving coalesced memory access.


  
In the context of CUDA programming, bank conflicts refer to a situation that occurs when multiple threads in the same warp (a group of 32 threads that execute instructions in lockstep) attempt to access data from the same memory bank of the shared memory simultaneously. Shared memory on NVIDIA GPUs is divided into equally sized memory modules called banks.

In CUDA, the `__restrict__` keyword is used in pointer declarations within kernel function parameters to indicate to the compiler that the object pointed to by the pointer is accessed only via that pointer (and pointers copied from it). It is an assurance that there will be no overlapping reads and writes to memory, which allows for more aggressive optimization of the code.
Certainly! Below is the markdown note with the code snippets included within each section:




- In the case of matrix operations, like matrix transpose, `TILE_DIM` helps to organize the data into square blocks, which are then loaded into shared memory for efficient processing.
    
- **Parameter Tuning**: The value of `TILE_DIM` is typically chosen based on the GPU's architecture, such as the size of the shared memory and the number of threads that can be run in parallel. It's often set to values like 16, 32, etc., to match these architectural features.

Here’s a breakdown of `blockIdx` and related concepts:

- **Grid**: A grid is the total collection of blocks that execute a kernel. It can be one, two, or three-dimensional.
    
- **Block**: A block is a group of threads that execute together and can share data through shared memory. Blocks are scheduled to run on the Streaming Multiprocessors (SMs) of the GPU.
    
- **Thread**: Threads are the smallest execution units in CUDA. Each thread runs an instance of the kernel and has an ID that can be used to control which data elements it processes.
    
- **blockIdx.x, blockIdx.y, blockIdx.z**: These are components of `blockIdx` that provide the position of the block within the grid. The `.x`, `.y`, and `.z` correspond to the dimensions of the grid. For a 2D grid, `blockIdx.x` and `blockIdx.y` are used.
    
- **Usage**: `blockIdx` is used within a kernel to calculate the global index of a thread. This global index determines which part of the data the thread should work on, ensuring that each thread works on a unique piece of data.

 Let's start with a Simple Copy Kernel Measures the best effective bandwidth without shared memory. It directly copies data from input to output array.

```cpp
__global__ void copy(float * __restrict__ odata, const float * __restrict__ idata) {
  const int x = blockIdx.x * TILE_DIM + threadIdx.x;
  const int y = blockIdx.y * TILE_DIM + threadIdx.y;
  int width = gridDim.x * TILE_DIM;

  for (int j = 0; j < TILE_DIM; j+= BLOCK_ROWS)
    odata[(y+j)*width + x] = idata[(y+j)*width + x];
}
```

In this code:

```cpp
const int x = blockIdx.x * TILE_DIM + threadIdx.x; 
```

- `blockIdx.x` identifies the block’s position along the x-axis.
- `TILE_DIM` represents the number of columns that each block is responsible for processing.
- Multiplying them together gives you the starting column for that block.
- `threadIdx.x` then gives you the offset within the block, so adding it to the starting column gives you the specific column that the thread should process.

While regarding 

```cpp
int width = gridDim.x * TILE_DIM;
```

- `gridDim.x`: This is the number of blocks along the x-axis of the grid. In CUDA, when you launch a kernel, you specify the grid dimensions, which tells you how many blocks there are in each dimension of the grid.
- `TILE_DIM`: This represents the number of columns (or the width) that each block will process. It's called `TILE_DIM` because you can think of each block processing a "tile" of the larger dataset.    
- `width = gridDim.x * TILE_DIM`: This calculates the total number of columns in the entire dataset. It multiplies the number of blocks along the x-axis by the number of columns each block is responsible for. The result is the width of the two-dimensional structure that the blocks collectively cover.



Copy Kernel Using Shared Memory

- **Purpose**: Demonstrates the use of shared memory to potentially increase bandwidth.
- **Function**: Copies data via shared memory, which may reduce global memory bandwidth usage.

```cpp
__global__ void copySharedMem(float * __restrict__ odata, const float * __restrict__ idata) {
  __shared__ float tile[TILE_DIM * TILE_DIM];
  
  const int x = blockIdx.x * TILE_DIM + threadIdx.x;
  const int y = blockIdx.y * TILE_DIM + threadIdx.y;
  const int width = gridDim.x * TILE_DIM;

  for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS)
    tile[(threadIdx.y+j)*TILE_DIM + threadIdx.x] = idata[(y+j)*width + x];

  __syncthreads();

  for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS)
    odata[(y+j)*width + x] = tile[(threadIdx.y+j)*TILE_DIM + threadIdx.x];          
}
```


`TILE_DIM` dictates the size of the matrix tile each block handles, while `BLOCK_ROWS` determines the stride between rows each thread processes, allowing for efficient distribution of work across threads within a block.

```cpp
Tile[threadIdx. Y+j][threadIdx. X] = idata[(y+j)*width + x];
```

This setup ensures that each thread works on different parts of the tile, filling it **row by row**

Naive Transpose

- **Purpose**: Provides a basic matrix transpose without using shared memory.
- **Characteristics**: Coalesces global memory reads, but writes are not coalesced.

```cpp
__global__ void transposeNaive(float * __restrict__ odata, const float * __restrict__ idata) {
  const int x = blockIdx.x * TILE_DIM + threadIdx.x;
  const int y = blockIdx.y * TILE_DIM + threadIdx.y;
  const int width = gridDim.x * TILE_DIM;

  for (int j = 0; j < TILE_DIM; j+= BLOCK_ROWS)
    odata[x*width + (y+j)] = idata[(y+j)*width + x];
}
```



Coalesced Transpose

- **Purpose**: Achieves coalesced reads and writes using shared memory.
- **Note**: May cause bank conflicts due to tile width matching the number of memory banks.

```cpp
__global__ void transposeCoalesced(float *odata, const float *idata) {
  __shared__ float tile[TILE_DIM][TILE_DIM];
    
  int x = blockIdx.x * TILE_DIM + threadIdx.x;
  int y = blockIdx.y * TILE_DIM + threadIdx.y;
  int width = gridDim.x * TILE_DIM;

  for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS)
    tile[threadIdx.y+j][threadIdx.x] = idata[(y+j)*width + x];

  __syncthreads();

  x = blockIdx.y * TILE_DIM + threadIdx.x;  // transpose block offset
  y = blockIdx.x * TILE_DIM + threadIdx.y;

  for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS)
    odata[(y+j)*width + x] = tile[threadIdx.x][threadIdx.y + j];
}
```



> "scrivo e leggo in righe dalla global memory"

- **Row-major Order**: Languages like C and C++ store multi-dimensional arrays in row-major order, meaning that the consecutive elements of a row reside next to each other in memory. When programming CUDA kernels for row-major storage, you'll typically want threads within a warp to access memory elements that are next to each other horizontally.

1. **Align Access Patterns with Memory Layout**: Always try to match your thread access patterns to the memory layout. For row-major storage, ensure that threads in a warp access consecutive elements in a row. For column-major storage, they should access consecutive elements in a column.
    
2. **Use Shared Memory to Transpose**: If you need to transpose data or convert from row-major to column-major access patterns (or vice versa), using shared memory as an intermediate storage can help. Load the data into shared memory in a way that matches the original layout and then write it out to global memory in the desired transposed layout. This strategy is exactly what your `transposeCoalesced` kernel does.


Yes, that's exactly right. Each thread in the `transposeCoalesced` kernel is primarily responsible for a specific column of the input matrix, but it loads data row by row, advancing according to the `BLOCK_ROWS` setting. This approach efficiently utilizes shared memory and ensures that the data is prepared for a coalesced write-back in the transposed format.

- **Write Operation**: The data is written from shared memory back to global memory. The way data was loaded into `tile` and now read out ensures that the writes are coalesced. Since `threadIdx.x` and `threadIdx.y` are switched when writing back, data that was contiguous along a row in shared memory is now written contiguously into a column in global memory, aligning with how data is ideally accessed in CUDA for performance (coalesced access pattern).

- **Data Re-orientation in Shared Memory**: Originally vertical slices of the matrix are loaded horizontally into shared memory. (`tile[threadIdx.y+j][threadIdx.x] = idata[(y+j)*width + x];` )
- **Efficient Writing**: When writing back, threads access shared memory horizontally (which was vertical in the original matrix), and write back vertically, thus maintaining coalesced writing patterns.

No Bank-Conflict Transpose

- **Purpose**: Optimizes the coalesced transpose to eliminate shared memory bank conflicts.
- **Improvement**: Adds padding to the shared memory tile to prevent bank conflicts.

```cpp
__global__ void transposeNoBankConflicts(float *odata, const float *idata) {
  __shared__ float tile[TILE_DIM][TILE_DIM+1];
    
  int x = blockIdx.x * TILE_DIM + threadIdx.x;
  int y = blockIdx.y * TILE_DIM + threadIdx.y;
  int width = gridDim.x * TILE_DIM;

  for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS)
    tile[threadIdx.y+j][threadIdx.x] = idata[(y+j)*width + x];

  __syncthreads();

  x = blockIdx.y * TILE_DIM + threadIdx.x;  // transpose block offset
  y = blockIdx.x * TILE_DIM + threadIdx.y;

  for (int j = 0; j < TILE_DIM; j += BLOCK_ROWS)
    odata[(y+j)*width + x] = tile[threadIdx.x][threadIdx.y + j];
}
```

Each kernel employs `TILE_DIM` and `BLOCK_ROWS` as macros to define the tile size and row processing strategy, respectively. The `__restrict__` qualifier helps the compiler optimize memory accesses by asserting that memory regions pointed to by the pointers do not overlap.



The introduction of an extra column `[TILE_DIM+1]` changes the indexing pattern such that:

- Accesses that were previously aligned with the banks (leading to conflicts) are now staggered.
- This staggering effectively ensures that threads accessing consecutive elements along a column will likely access different banks, thus avoiding conflicts and enhancing performance.



## 05 03 

**`atomicAdd()` Function:** - `atomicAdd()` is used to atomically add a number to a variable in memory, ensuring that no two threads can interfere with each other’s operations. 

Example Usage: `atomicAdd(&counter, 1);`

This would safely increment the `counter` by 1, used typically in scenarios like counting the number of times a condition is met across multiple threads.

For the histogram example is killer the "privatization" we can privatize at different levels:

- shared memory 
- registers
- Or even committing on different region of global memory to boost performance. Through privatization, each block gets its exclusive region in global memory, reducing conflicts, especially with a large number of bins. This technique enhances efficiency, particularly in scenarios with high memory demand, like when numerous bins are involved.

Obviously the benefits of utilizing shared memory per block and optimizing memory access by committing to shared memory instead of global memory. 

As more transactions are performed with larger inputs, the impact on final performance diminishes. 


By using certain optimizations like const keyword and other compiler-specific strategies, the code efficiency can be enhanced. The discussion also touches upon verbose flag options for providing detailed information during compilation, which can be beneficial in identifying and addressing potential performance issues in advance, especially in terms of memory usage. 



Increased register usage can be beneficial and showcases an example of how the number of registers can be increased without a significant impact. Additionally, the concept of aggregating data values in contiguous regions to optimize dynamic tasks is explained. This aggregation method involves counting repeated data values to minimize memory usage on registers.




In general making assumptions about the input data to optimize performance and analyses the impact of branch divergence on processing efficiency.
Aggregation

Some datasets have a large concentration of identical data values in localized
Areas

- High contention
- Reduced throughput
