
# GPU architecture

The focus will be on NVIDIA architectures, but other vendors followed a similar path.

## GPU evolution

The main steps in GPU Evolution can be resumed in:

1. GPU as a hardware accelerator for graphics pipeline
2. GPU for general-purpose computing
3. GPU architecture evolution to the present day

### GPU as a hardware accelerator for graphics pipeline

The GPU is actual a general pathos accelerator. If you care about computer graphics, there is another course that is on computer graphics. The starting point here is to understand how the graphics pipeline works just because the has been originally defined based on this idea. 

**3D Rendering:** The process of converting a 3D model into a 2D image, performed in real-time or offline depending on the application

**Graphics Pipeline for 3D Rendering:** A conceptual model describing the necessary steps for real-time 3D rendering, standardized around OpenGL/Direct3D APIs and possibly accelerated in hardware. 

The real-time graphics pipeline implemented by OpenGL before 2007, consisting of a sequence of operations on several types of objects:

- **Vertex Generation:** The host interface receives commands from the CPU and pulls **geometry information** from system memory, outputting a stream of vertices in object space with associated information.
- **Vertex Processing:** Receives vertices from the host interface in object space and outputs them in clip space through matrix multiplications on each vertex.
- **Primitive Generation:** Groups vertices forming one primitive (triangle).
- **Primitive Processing:** Performs various elaborations, including perspective division, viewpoint transformation, clipping, and backface culling.
- **Fragment Generation:** Rasterization converts each primitive into fragments, determining which pixels a primitive overlaps and computing preliminary fragment color.
- **Fragment Processing:** Assigns final colors to fragments, simulating the interaction of light and materials.
- **Pixel Operations:** Processes fragments to compute the final color of each screen pixel, including blending, Z-buffer test, and other tests


**Parallelism in the Real-time Graphics Pipeline:** Huge amount of data to be processed, single-stage elaborations can be performed in **parallel**, mainly data-intensive based on **matrix elaborations** and other arithmetic operations, **few branch instructions and data dependencies**. 

**Quest for Accelerating Graphics Pipeline Processing:** Hardware accelerators designed to accelerate the graphics pipeline, evolution of graphics pipeline accelerators from pre-GPU era to GPU based on unified shared processors

### GPU for general-purpose computing

In the **Pre-GPU Era** graphics supercomputers in the 70s/90s mainly targeted **specific market sections** such as computer simulation, digital content creation, engineering, and research.

![](images/Pasted%20image%2020240224153032.png)

**Fixed-function GPU:** GPUs with fixed-functionality for specific tasks, such as 3dfx voodoo (1996) and NVIDIA GeForce 256 (1999)

**Programmable GPU:** GPUs with programmable stages, such as NVIDIA GeForce 3 (2001), GeForce 6 (2004), and GeForce 7800 (2005)




Videogames were actually what pushed us to move from the pre-GPU era to an era where each of us has a cheaper chip in our computers.


Goals of GPUs?

The Graphics Processing Unit (GPU) is a HW accelerator (initially designed) for the graphics pipeline • Exploit parallelism – Among the pipeline steps – In the data processing of the single step – By running different tasks on CPU and GPU in parallel • Use ad-hoc hardware accelerators for recurrent functions – Texture filtering, rasterization, MAC, sqrt, ...



--- 



**GPU Based on Unified Shared Processors:** GPUs with unified shader processors, such as NVIDIA GeForce 8 (2006) and Tesla (2006)

**General-purpose Computing on GPU (GPGPU):** The use of GPU for general-purpose computing, enabled by CUDA language and the manycore architecture of modern GPUs

**Branches in Shader Programs:** The impact of branches on GPU performance and the use of coherent and divergent execution to optimize performance

**Stalls:** The impact of stalls caused by data dependencies and the use of instruction stream multi-threading to hide stalls and improve performance

**Throughput-oriented System:** Moving from a latency-oriented architecture (CPU) to a throughput-oriented one (GPU) to improve overall task execution latency

**Basic Architecture of a Modern GPU:** Summary of the modern GPU architecture, including the use of many simplified cores, implicit SIMD execution, and avoiding latency stalls by interleaving execution of many groups of instruction streams





The second generation, GeForce 2-5-6, 9-series GPUs, marked a significant improvement as they utilized the entire GPU to elaborate the entire complex scene.

All stages implemented in hardware (the all graphics pipeline). 

Year by year Developers gained the ability to program specific steps in the graphics partner, including vertex processing, using OpenGL as a single language. This enabled customization of algorithms and skipping fixed units. 

The new version supports dynamic branches, allowing for more complex code execution, and memory ports have been increased for faster access. The pipeline architecture features a mix of fixed function and programmable stages

New features include fashion decodes units, program counters, and current selection guidance.

In one redesign, Nvidia moved from multiple multicores to a single homogeneous multicore with accelerators for each stage of the software graphics plan. This change enabled the GPU to accelerate not only graphics but also **general-purpose computing**. The interface with memory is similar, and the reason for this redesign is to address imbalances in workloads. For pixel-heavy workloads, the pixel shader is the bottleneck, while for geometry-heavy workloads, the vertex shader is saturated. 

**NVIDIA GeForce 8 (2006):** Features and architecture of the NVIDIA GeForce 8 GPU, including shared memory, constant cache, texture cache, and different latencies for different memories.


Previous designs attempted to address this with a fixed number of cores in each stage, but this new architecture performs better on both common and corner cases where one value programmable stage is saturated and others are underutilized.

Problems in balancing workload in pipeline stages : 

The GPU industry is seeing underutilization of certain stages while others are saturated. 



To address this, architects are designing a single, homogeneous core to execute all possible elaborations, enabling adaptability for various capabilities. This core, known as a General Purpose Core, also includes a Unified shader processors, optimal usage of processing resources.

How does the unified shared processor work?

Three key ideas:

1) Instantiate many shader processors
2) Replicate ALUs inside each core to enable SIMD processing
3) Interleave the execution of many groups of SIMD threads

![](images/Pasted%20image%2020240303165710.png)

Shader programming model:

- Fragments (or vertex or pixels) are processed independently
- The function is written to work on a single fragment


In GPU architecture, we only require a simple core to execute a single instruction stream on a single data chunk. Each thread operates independently, with its own fetch and code unit, and a smaller register file. We can execute the same instruction stream concurrently on different data chunks without the need for complex features like branch prediction or out-of-order execution. To make better use of processing power, we can instantiate multiple simplified cores to run in parallel. The GPU architecture allows for independent threads to process different data, each with its own context and data saved in the register file. 





Explicit SIMD processing strategy is used but has some inefficiency. 

This strategy in school is called an explicit vector approach based on vector instructions, which requires explicit parallelism in the program and restricts flexibility due to the need to consider GPU architecture specifics.

This is actual not been adopted in GPU but only in CPUs: implicit vector solution where the compiler generates binary code with scalar instructions, allowing the architecture to run multiple instances of the same program with each thread accessing unique data and performing implicit vectorization at runtime. IOn Intel AVX2, AVX512 and ARM Neon instructions examples.


Single instruction multiple thread

In GPUs a slightly different solution (a sort of implicit SIMD) has been adopted. 
This architecture is known as Single Instruction Multiple Threads (SIMT), where the same instruction is executed on different data by different threads. In this approach, there is no explicit vectorization specified at compile time or in the source code. Instead, concurrent execution in 8 or 32 threads is performed in lock-step fashion with each thread responsible for fetching its own code unit. 

The G8 architecture is a multicore design with each core having its own fetch and code unit. Nvidia terms this as a single processor being composed of multiple streaming processors. Within the streaming multicore processors, there are special functional units for math operations and a scheduler to manage thread execution on 8 streaming cores. Each streaming processor can perform integer and floating-point operations and has specialized hardware accelerators.

This GPU architecture is designed for data parallel operations, especially compute-intensive functions. It accelerates functions written in specific languages that support GPU programming. The workflow involves sending data to the GPU, running the kernel function, and returning the results back to the CPU.

However, branch divergence, where threads take different paths due to conditional statements, can create issues. Since threads are not split into subgroups, you must execute both branches sequentially, disabling elaborations on threads with a false condition. This can lead to underutilization of resources.

We are experiencing under-utilization and parameter degradation in our streaming multiplicers due to branch divergence, specifically within the same multiplicer. This issue arises because instruction sequences may differ for various data elements, leading to inefficient use of resources.

The concept of coherent execution is crucial for optimal processing, where the same instruction sequence applies to multiple data elements. In the absence of branches or with identical branch outcomes for all threads, this approach can help improve performance.

However, we will encounter another issue, known as variant execution, and memory access challenges. Memory access is significantly slower than ALU instructions, causing latency and hindering performance. Previously introduced cache pre-fetching logic has been removed, making it essential to maintain data alignment and minimize memory access.

To address memory latency


Hiding stalls with instruction stream multi-threading
Interleave processing of many streams on a single core to hide stalls
caused by latency operations

- Each thread gets a unique index to determine its data portion.
- All threads execute the same `vectorAdd` kernel.

we partition the data into groups and execute them concurrently on different groups of threads. This approach keeps the streaming multi-core busy by swapping to the next group of threads during memory latency periods.




Overall, our focus is on maximizing data parallelism to maximise throughput, enabling efficient processing across large numbers of simplified streaming cores.

The core is not stalled by memory accesses since running other concurrent instruction streams

We have a function that allows data parallel processing, enabling the same computations on different data chunks, moving from a latency-oriented system to a throughput-oriented one. CPU logic is simplified for multi-core exploitation of high data parallelism. Maximizing throughput for language changes reduces overall function latency. No stores or brands are preferred, keeping architecture intact for maximum throughput. Multi-core has numerous groups of threads scheduled based on memory stores. In a multi-stream environment, program counters differ based on scheduling. Transitioning from a coherent architecture to a throughput-oriented one increases throughput by decreasing single instruction stream latency. Zero-delay scheduling is achieved in hardware without operating system involvement or data transfer. However, avoiding context switches for less intensive applications remains a challenge. Overall, this architecture is efficient for intensive applications.



**CPU Memory Hierarchy vs. GPU Memory Hierarchy:** Differences in memory hierarchy and usage between CPUs and GPUs, including the use of caches in CPUs and high-bandwidth connections and sophisticated memory request order logic in GPUs


GPUs have introduced caches, which are organized differently than CPU caches to exploit temporal and spatial locality. Space locality is less of an issue in multi-threaded streaming applications. No L2 cache coherence is implemented, but specific caches are used for textures, which have simplified cache architectures without coherence mechanisms. Shared local memory and scratchpad memory are also introduced for fast data access within streaming multi-threaded applications. Different needs require different solutions compared to CPUs.

In the streaming multi-core architecture, different needs call for varied CPU solutions. When an application exhibits specific locality, a smaller exploit called reload temporal locality cache is useful. However, this issue isn't resolved at the hardware level; it's the programmer's responsibility. In this course, we'll need to write a program featuring a high memory instruction ratio for automatic intensity. The shared memory, directly connected to the streaming multi-core, is a high-speed solution. However, the programmer is responsible for loading and transferring data to and from the shared memory. Profiling and optimization are essential to maximize performance.


Three years later, the Tesla GPU was designed and announced, featuring 16 streaming multiprocessors with increased numbers of cores, six memory ports, a global scheduler, and a complete hardware-implemented scheduler with zero delay and immediate thread dispatching.


**NVIDIA Fermi (2010):** Architecture and features of the NVIDIA Fermi GPU, including streaming multiprocessors, streaming cores, memory partitions, and the global scheduler

Actually then L1-L2 caches have been added later 

GPU applications present high spatial locality but very Iow temporal locality
Caches are smaller w.r.t. CPU ones to exploits spatial locality


The programmer divides
threads to be spawn for a
kernel function in blocks



The global scheduler
dispatches blocks among
streaming multiprocessors



 **In this architecture**, 

A grid in GPU computing is divided into blocks. The Gigathread scheduler distributes these blocks among streaming multiprocessors. Each thread is part of a block, and multiple blocks can be allocated on the same streaming multiprocessor for interleaving. The hardware mechanisms enable zero-delay dispatching of blocks based on resource requirements.

The GPU and CPU architectures must be considered to achieve optimal performance. Resources, including memories, are partitioned and managed by the compiler for efficient utilization. The gigatread scheduler can dispatch blocks with specific resource requirements to streaming multiprocessors.

A GPU has a texture streaming multiprocessor with 32 streaming cores, each capable of integer and floating-point operations. The GPU adheres to the IEEE 754 floating-point standard for double precision computation. The streaming multiprocessor also features 16 load/store units that can be used for **parallel** memory access.

Threads are executed in groups of 32, called warps. These special units only deal with memory access and have mechanisms for automatic casts and retiring units for efficient data processing.

Each warp, composed of 32 threads, is scheduled and dispatched to available processing elements, achieving parallelism by interleaving a large number of warps in the same streaming multicore. The number of warps that can be scheduled depends on available resources, and the register file is partitioned to accommodate each thread's required registers. The number of blocks and works that can be allocated in the same streamer multicore depends on the available resources, which is managed by the giga thread scheduler.

Warps are assumed to be independent, removing the need for complex control and synchronization architecture, such as dependency checkers. Instead, a scoreboard is used to keep track of the warp status. In some architectures, multiple warp schedulers exist to allow for concurrent scheduling of different independent works. Each instruction execution time varies, with some taking longer than others due to the organization of warps into 32 threads per streaming multiprocessor. 

Concurrent executions

Nvidia implemented interleaving to execute multiple kernels simultaneously within the same streaming multicore. This allows for savings in time, but each GPU is still reserved for one application at a time. To avoid context switching delays, multiple applications can be merged into a single process, compressing their kernels, and allowing one application to monopolize the GPU. Optimization includes maximizing warp utilization and minimizing context switching, memory occupancy, and branch divergence.

The NVIDIA e application reserves the GPU and compresses the kernel for multiple applications. The Tesla GPU's memory hierarchy was restructured with the introduction of L1 cache integrated within each multi-processor, no memory coherence, and the removal of textur cache. 

Restructured cache hierarchy
— Introduction of private LI cache
and chip-level L2 cache
— Texture cache has been
removed from LI since not
efficient for general purpose
computing

Without cache coherence, the programmer uses synchronization techniques like register spilling and atomic memory operations in the L2 cache to avoid risk conditions during critical sections.

(
Register spilling occurs when a compiler runs out of physical registers to hold all of the variables that are currently in use ("live") within a program. The compiler is then forced to temporarily store some of these variables in memory (usually RAM) and load them back into registers when needed.
)


Also Unified address space on local,
shared and global memory

Considerable advantages
from the programming
point of view
Enhanced su ort to C++ -> to use pointers. 



**Streaming Multiprocessor (SM):** Detailed architecture of the streaming multiprocessor, including processing cores, load/store units, special function units, and the shader clock

**Concurrent Executions:** Support for concurrent execution of multiple compute kernels from the same application and the benefits of fast application context switch

**Memory Hierarchy:** Restructured cache hierarchy in Tesla Fermi, including the introduction of private L1 cache and chip-level L2 cache, unified address space, and enhanced support for C++

**Memory Hierarchy Configuration:** Possibility for the programmer to configure local memory, including 16KB L1 cache and 48KB shared memory, and the benefits of shared memory for cooperation between threads of the same block

**NVIDIA Kepler (2012):** Architecture and improvements of the NVIDIA Kepler GPU, including performance and power efficiency enhancements

Subsequent architectures, such as Kepler, continued to refine and improve the design for better performance and power efficiency. 

The Kepler GPU architecture made improvements by accelerating on the GPU and reorganizing the architecture.

**Streaming Multiprocessor (SMX):** New SM architecture, including 192 single-precision streaming cores, 64 double-precision streaming cores, and other features

The internal structure of streaming multicore was reorganized to integrate a larger number of cores, and single cores were modified to handle multiple instructions, which shifted the focus from a single instruction stream to a more computer-like architecture.

Researchers improved single and double precision floating point streaming cores in the GPU architecture by partitioning functional units and implementing out-of-order execution.

**New Scheduler:** Improvements to the scheduler in the SMX architecture, including 4 warp schedulers, 2 dispatchers per warp, and other features

They introduced multiple schedulers and dispatchers to parallelize instruction execution and removed the dependency between single and double precision instructions. The out-of-order execution uses a simpler scoreboard and enables the execution of multiple instructions concurrently, increasing parallelism, throughput, and optimizing overall kernel execution time. Memory hierarchy was also reshaped by introducing a read-only cache (useful for 3d rendering) and reconfiguring L1 shared memory based on common application trends and GPU usage characteristics.

They removed configurable L1 shared memory and implemented shuffle instructions for in-memory structure support in cache. Dynamic parallelism was introduced, allowing the GPU to offload work from the CPU and adapt the number of threads based on the complexity and amount of data to process. This leads to better data parallelization and efficient use of resources.



**Memory Hierarchy:** Improvements to the memory hierarchy in the Kepler architecture, including larger cache sizes, larger configurability of L1/shared memory, introduction of a read-only cache, and added shuffle instructions


**Dynamic Parallelism:** Ability for a **kernel to launch another kernel**, allowing optimization of recursive and data-dependent execution patterns

Spawning kernels based on the actual complexity of the instance of the process. We spawn a lot of threads only if necessary! Occupy resources only if needed by the instance of the problem. "split dynamically the amount of work".


Another architectural aspect


**Several Work Queues:** Hyper-Q mechanism offering 32 HW-managed work queues, allowing **multiple CPU cores to launch work on a single GPU simultaneously** and improving GPU utilization and reducing CPU idle times.

The distribution of workload among threads can be optimized at runtime to reduce load on a specific number of threads, beneficial for recursive and data-dependent execution patterns. In the case of the GPU architecture, loading a kernel involves the CPU sending a command to a software queue, which connects to an hardware queue in the GPU. With only one hardware queue in the Fermi architecture, dependencies between kernels could lead to false serial dependencies, preenting convcurrent execution. To address this issue, the Kepler architecture introduced **multiple hardware queues**, allowing dependent kernels to be executed in sequence within the same queue while enabling concurrent execution of independent kernels by the GPU scheduler. With the Kepler architecture, GPUs have since been used extensively in servers and supercomputers for parallelizing complex simulations and connecting multiple GPUs through technologies like GPU Direct, enabling direct data transmission between GPUs.


**Grid Management Unit:** Redesign of the grid management unit in the Kepler architecture

**Multi-machine Systems:** GPUDirect technology allowing direct access to GPU memory from third-party devices, optimizing MPI send/receive.

MPI was previously used to program the system. Nvidia added support for MPI and CUDA to enable multi-GPU applications with data parallelism on a single GPU and load distribution among multiple GPUs without involving the CPU and its PCIe bottleneck. 



**NVIDIA Maxwell (2014):** Similar architecture to Kepler, with new streaming multicore (SMM) and memory hierarchy improvements

In the Maxwell architecture introduced in 2014, they introduced application-level preemption with granularity down to the single structure level to schedule multiple applications and swap data between registers, share memory, and video memory. They also separated single-precision and double-precision cores and introduced FP16 (half-precision) for applications that don't require high precision, such as AI and deep learning, which mostly use values between 0 and 1 or -1 and 1. 

**Streaming Multicore:** Preemption at single instruction granularity, separate cores for single and double precision operations, introduction of half precision floating point instructions, and other improvements





**NVIDIA Pascal (2016):** More hierarchical organization, including graphics processing clusters (GPCs) and texture processing clusters (TPCs)


The Pascal architecture in 2016 reorganized memory address space to better distinguish the GPU as a discrete accelerator and improved performance by using a two-times throughput for single-precision operations.



Programmer speed is impacted by the separation between CPU and GPU memories and address spaces. To simplify programming, unified memory was implemented, allowing a single unit for both memories and shared pointers. 

**Unified Memory:** Unified virtual addressing between CPU and GPU, transparent transmission of memory pages between CPU and GPU memories, and automatic handling of page fault and global data coherence

Advanced hardware mechanisms perform automatic data transmission between CPU and GPU memories. 

Memory pages are transparently transmitted between CPU and GPU memories !


**NVLink:** New high-speed interface (160GB/s bidirectional) to enable multi-GPU architectures for high-performance computing
Multiple GPUs can be integrated using the NV link interface. 






**NVIDIA Volta (2017):** Streaming multicore partitioned into processing blocks, separate floating point and integer cores, new tensor cores for matrix multiplication and accumulation, and independent thread scheduling

To accelerate AI, a tensor core was introduced in the streaming multiprocessor as a matrix multiplier and accumulator.

**Tensor Cores:** Perform multiplication and accumulation of 4x4 matrices, mixed precision operations, and specific for deep learning applications

 The tensor core can directly accelerate matrix multiplications and convolutions. However, there are data precision issues, and the hardware implementation may have different precisions for intermediate operations and results.



**Independent Thread Scheduling:** Improvements to thread scheduling in the Volta architecture, allowing per-thread execution state and improved performance.


They change their mind: Per-warp execution state (Pre-Volta architecture) -> to Per-thread execution state. 

To improve performance, Fact architecture changed from a single program counter for an entire warp to individual program counters for each thread.  
This allows for warp splitting during **branch divergences** to execute separate results. An example is given with an if-else instruction having a branch divergence, where the warp can be split into subgroups to hide data dependencies and improve performance. The advantage is that memory access dependencies can be hidden by assigning them to different subgroups, requiring the GPU to keep track of each thread's program counter and context.


Despite reintroducing some complex mechanisms, they have been re-engineered for GPU requirements. The next class will cover multi-process server support and GPU programming with Cuda.









**Multi-process Server (MPS) Support:** Support for multiple applications to run concurrently on separate resources, with limitations in the Pascal architecture and improvements in the Volta architecture

**NVIDIA Turing (2018):** Main enhancements on graphics pipeline acceleration, including a new RT unit for ray tracing and relevant performance improvements on deep learning acceleration

**NVIDIA Ampere (2020):** New tensor cores, virtualization, and other features and improvements

**New Tensor Cores:** Acceleration for all data types, exploiting matrix sparsity, and avoiding multiplications by zero

**Multi-Instance GPU (MIG) Virtualization:** Ability to partition the GPU into 7 separate virtual GPUs, improving resource utilization and efficiency in cloud environments. This was crucial in cloud service provider's multi user node. 

![](images/Pasted%20image%2020240310182703.png)



Other GPU vendors, including AMD and ARM, and their architectures and timelines

**AMD Timeline:** Overview of the timeline and features of AMD GPUs

**AMD Radeon HD 6970 Cayman (2010):** The streaming multiprocessor is called a compute unit, threads are grouped in 64 elements (wavefront), and four clocks are used to execute an instruction for all fragments in a wavefront

**AMD Radeon R9 290X (2013):** The overall multicore architecture, including up to 44 compute units, and the memory hierarchy and communication infrastructure



**ARM Mali 628 (2014) and ARM Mali G720 (2023):** Targeted for embedded computing and considerably smaller architecture compared to NVIDIA and AMD GPUs.


[00:00:00] In this discussion, we're concluding our exploration of GPU architecture, focusing on NVIDIA's Bolt architecture. This innovation was driven by the demand for data application accelerators in server markets. With Bolt, NVIDIA introduced multiprocessing capabilities, allowing multiple applications to interleave and use the GPU simultaneously, improving overall efficiency. However, this approach had limitations, including shared memory and no virtualization, leading to performance unpredictability and inability to share the GPU among multiple users.

In the cloud sector, this limitation led to the integration of multiple GPUs per machine to serve multiple users. With the Volta GPU, cloud providers can assign a single GPU to a user at a time, ensuring a cost-effective utilization of resources. The subsequent architectures, while featuring internal stream multicore and graphics pipeline acceleration, did not significantly deviate from the Bolt architecture's design. Additionally, tensor cores were engineered to accelerate multiple data types.

[00:06:13] The NVIDIA Tensor Core has been updated to support multiple data types and different precisions, allowing for a trade-off between accuracy and performance. Previously, the Tensor Core only worked with 16 and 32-bit vectors. The new design also includes hardware support for exploiting sparse metrics in convolutions, mechanisms for CNN pruning, and improved hardware mechanisms for the server-side cloud market.

CNN pruning is a model compression technique used to remove unnecessary weights and reduce the size of deep learning models. This is especially useful for floating point matrices with large numbers of zeros. NVIDIA provides both software and hardware support for metrics compression, resulting in smaller and more efficient multiplications.

The Amper architecture also introduces hardware-software virtualization, enabling a single GPU to be divided into multiple portions, with each portion being assigned to a single user in a cloud computing scenario. This feature allows for more fine-grained resource assignments, better exploitation of GPU resources, and potential cost savings for cloud providers.

[00:12:52] The GPU market responded to changing needs with advancements in technology. NVIDIA and AMD, among others, developed GPUs for graphics pipelines, making them programmable. Later, GPUs were adapted for general-purpose computing by introducing single shader cores, dynamic parallelism, and server-side mechanisms. In recent years, the focus has been on deep learning, resulting in tensor cores and other integrations. NVIDia and AMD have followed similar paths, with AMD also prioritizing APUs to reduce costs by integrating CPUs and GPUs on the same chip and sharing memory between them. AMD's unified architecture reduces transmission costs compared to discrete GPUs with separate chips and memories.

[00:20:56] AMD GPUs allow sharing memory between CPU and GPU. Their compute units have long instruction words and consist of 16 cores, each with a single scheduler, dispatcher, load/store unit, and branch unit. AMD moved optimization from architecture to compiler. Compute units have 64 elements, called a wave front. Memory hierarchy is similar to NVIDIA GPUs, with no cache coherency. AMD Radeon GPUs can be connected to AMD CPUs via PCIe, but architecture was later reshaped to be more similar to NVIDIA's. ARM is a major vendor in the systems market, with their Mali GPU having a similar architecture to NVIDIA and AMD GPUs, but with fewer shader cores and less detailed information available.



