
# Introduction to heterogeneous systems

An **heterogeneous System** comprises multiple computing units, each with distinct characteristics. Heterogeneous systems are necessary since the need to run applications in various fields with differing performance requirements (Email, browser, videogames , cad ... etc). 

## Why heterogeneous systems

What are the motivations, challenges, and real-world examples of heterogeneous systems?

**Motivations**:

1. The need for high performance and energy efficiency in modern devices, which are **power-constrained**.
2. Many compute-intensive applications can be partitioned into parts with different characteristics, **each of which can be efficiently accelerated by a different type of computing unit**.

**Challenges**:

1. **Integration** of many different units and **communication** between them.
2. **Programmability**, as each type of unit has different programming languages and paradigms.
3. **Resource management**, including distributing a multi-programmed dynamic workload, configuring software/hardware knobs, and achieving required performance at a power-efficient level.

Real-world examples are infinite: mobile phones, laptops, desktop computers, embedded and edge devices, and supercomputers. 

## Evolution of Computing System Architectures
    
- **Single-Core Era**: This era was characterized by increasing voltage/frequency scaling until the power wall was reached in 2004.
- **Multi-Core Era**: This era saw the integration of multiple cores in the same chip. However, performance limits were encountered due to power consumption and scalability issues.
- **Heterogeneous Systems Era**: marked by the integration of heterogeneous units in the same chip, allowing for parallelization of various applications. Different parts of the applications may benefit from specialized computing units.

## Parallelism 

**Task Parallelism**: This involves the execution of many independent tasks/functions in parallel.
**Data Parallelism**: This involves operations on data composed of many items that can be processed simultaneously.

## Flynn's Taxonomy



This taxonomy classifies system architectures based on instructions and data into categories like SISD, SIMD, MISD, MIMD, SPMD, and MPMD.

**Architectural Solutions for Parallel Computing**:

Instruction-Level Parallelism (ILP) 

Instructions are executed in sequence • A new instruction is issued when the previous one is completed • Each instruction works on a single data item

Thread-Level Parallelism (TLP) and include architectures like pipelined, superscalar, VLIW, SIMD, Simultaneous Multi-Threading, and Multicore.

 Interleaved Multi-Threading: 

Multiple instruction streams (called threads) are executed together – Threads are independent from each other • The architecture stores the execution data (called context) of all the threads • The various threads are executed in time-slicing – Possibility to hide stalls of a single thread by executing the other threads

The architecture contains several cores – Cores may be of different types • Each core executes a thread (at least) • Need for a cache hierarchy to avoid memory accesses to be the bottleneck


**Data-Level Parallelism (DLP)**

The architecture contains groups of several ALUs of the same type


The program code contains vectorized instructions • Each vectorized instruction requires the same operation to be executed on different data items in parallel




1. **GPU Parallelism**:
    - GPUs exploit Data-Level Parallelism (DLP), where groups of ALUs execute the same operation on different data items simultaneously.
