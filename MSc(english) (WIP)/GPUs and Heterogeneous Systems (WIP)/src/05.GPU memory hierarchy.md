
# GPU memory hierarchy

In CUDA there are 

- **registers**:  Registers are fast, private, and limited
- **device memory** is slower and accessible by all threads. (aka global or video memory) – Off-chip memory – Accessible by • All threads on SMs • The host. 
- **Caches**, including L1 and L2, are integrated into the architecture, with L1 being private to each streaming multi-processor and L2 being per SM. Non-programmable memories like L1 and L2 caches are transparent to the programmer

![](images/Pasted%20image%2020240316171201.png)

The CUDA memory model gives a uniform and systematic abstract view on the GPU memories.

It is independent from the specific GPU generation • It specifies how to program all memory types – The programmer states where to allocate each variabl


The memory model in GPU programming allows the programmer to specify where to place variables based on different memory types and their access costs. 

**local memory**: Local memory is a part of global memory logically reserved for private data of each thread. L1 and L2 caches can be used transparently to speed up access to frequently used data, reducing latency and avoiding accessing the entire physical memory hierarchy. Spatial locality, where threads access the same memory location, is more commonly used than temporal locality, where threads access different memory locations over time, in data-intensive applications. The programmer can use strategies like variable reuse and locality to minimize the cost of accessing data. Registers are simple to use, with each thread having its own registers, and variables declared within a thread are mapped to registers. The thread's lifetime determines the variable's lifetime, and once the thread ends, the value in the register is released. Local memory is mapped as a part of global memory for private thread data, with L1 and L2 caches automatically used to speed up access. In data-intensive applications, spatial locality is more commonly used than temporal locality. Threads access the same memory location, and the probability of another thread within the same warp or block accessing the same value is low.

**Global memory**, mapped on device memory, is used for reading and writing data for all threads and the host. Accesses are organized in **transactions**, with 32 accesses per load instruction. Specific rules apply to data granularities, alignments, and access patterns. Global variables can be declared and accessed in two ways: starting declaration with a pointer or variable in OSCode, or static declaration within the code for device-side use only. The programmer is responsible for avoiding multiple threads writing to the same data location.

Host code accesses the variable by means of specific functions: • Host-> device: `cudaMemcpyToSymbol(`) • Device->host: `cudaMemcpyFromSymbol()`
Host declares the device pointer as a common C pointer – Host allocates the memory (cudaMalloc()) and release it (cudaFree()) • Access: – Host code accesses the variable by means of the cudaMemCpy() function

The speaker discusses accessing global variables in CUDA programming. To access a global variable from the outside, we need to copy it to device memory using the "cuMemcpyToSymbol" function and copy it back using "cuMemcpyFromSymbol." 

Global variables are placed in CUDA memory, specifically in device memory. We can use pointers to allocate and copy memory between the host and device sides using "cuMalloc" and "cuMemcpy."

 With the unified memory model in Kepler and later architectures, we have a single memory address space shared between the CPU and GPU. Dynamic memory allocation using the unified memory is possible using the "cudaMalloc" function, and the transmission is transparent to the programmer. We can use a single pointer to manage both the host and device memory. In summary, to access global variables in CUDA programming, we need to copy them to device memory using "cuMemcpyToSymbol" and "cuMemcpyFromSymbol," and access them in the standard C-like way. Global variables are placed in device memory, and we can use pointers to allocate and manage memory between the host and device sides using "cuMalloc" and "cuMemcpy." With the unified memory model, we can use a single pointer to manage both the host and device memory.

Constant memory:

Texture memory: 

 




