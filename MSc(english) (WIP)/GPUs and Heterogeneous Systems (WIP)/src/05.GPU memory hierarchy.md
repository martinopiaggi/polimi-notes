
# GPU memory hierarchy

**Memory Model**: More explicit to the programmer, with caches not being entirely transparent, demanding a deeper understanding of memory hierarchy and access patterns.

In CUDA there are 

- **registers**:  Registers are fast, private, and limited
- **device memory** is slower and accessible by all threads. (aka global or video memory) – Off-chip memory – Accessible by • All threads on SMs • The host. 
- **Caches**, including L1 and L2, are integrated into the architecture, with L1 being private to each streaming multi-processor and L2 being per SM. Non-programmable memories like L1 and L2 caches are transparent to the programmer

![](images/Pasted%20image%2020240316171201.png)

The CUDA memory model gives a uniform and systematic abstract view on the GPU memories.

It is independent from the specific GPU generation • It specifies how to program all memory types – The programmer states where to allocate each variabl


The memory model in GPU programming allows the programmer to specify where to place variables based on different memory types and their access costs. 

**local memory**: Local memory is a part of global memory logically reserved for private data of each thread. L1 and L2 caches can be used transparently to speed up access to frequently used data, reducing latency and avoiding accessing the entire physical memory hierarchy. Spatial locality, where threads access the same memory location, is more commonly used than temporal locality, where threads access different memory locations over time, in data-intensive applications. The programmer can use strategies like variable reuse and locality to minimize the cost of accessing data. Registers are simple to use, with each thread having its own registers, and variables declared within a thread are mapped to registers. The thread's lifetime determines the variable's lifetime, and once the thread ends, the value in the register is released. Local memory is mapped as a part of global memory for private thread data, with L1 and L2 caches automatically used to speed up access. In data-intensive applications, spatial locality is more commonly used than temporal locality. Threads access the same memory location, and the probability of another thread within the same warp or block accessing the same value is low.

**Global memory**, mapped on device memory, is used for reading and writing data for all threads and the host. Accesses are organized in **transactions**, with 32 accesses per load instruction. Specific rules apply to data granularities, alignments, and access patterns. Global variables can be declared and accessed in two ways: starting declaration with a pointer or variable in OSCode, or static declaration within the code for device-side use only. The programmer is responsible for avoiding multiple threads writing to the same data location.

Host code accesses the variable by means of specific functions: • Host-> device: `cudaMemcpyToSymbol(`) • Device->host: `cudaMemcpyFromSymbol()`
Host declares the device pointer as a common C pointer – Host allocates the memory (cudaMalloc()) and release it (cudaFree()) • Access: – Host code accesses the variable by means of the cudaMemCpy() function

The speaker discusses accessing global variables in CUDA programming. To access a global variable from the outside, we need to copy it to device memory using the "cuMemcpyToSymbol" function and copy it back using "cuMemcpyFromSymbol." 

Global variables are placed in CUDA memory, specifically in device memory. We can use pointers to allocate and copy memory between the host and device sides using "cuMalloc" and "cuMemcpy."

 With the unified memory model in Kepler and later architectures, we have a single memory address space shared between the CPU and GPU. Dynamic memory allocation using the unified memory is possible using the "cudaMalloc" function, and the transmission is transparent to the programmer. We can use a single pointer to manage both the host and device memory. In summary, to access global variables in CUDA programming, we need to copy them to device memory using "cuMemcpyToSymbol" and "cuMemcpyFromSymbol," and access them in the standard C-like way. Global variables are placed in device memory, and we can use pointers to allocate and manage memory between the host and device sides using "cuMalloc" and "cuMemcpy." With the unified memory model, we can use a single pointer to manage both the host and device memory.

Constant memory:

Texture memory: 

 
the shared memory, a scratchpad memory that programmers can use to share data within the streaming multiprocessor. Though it's useful, using it correctly can be challenging.

Apart from global and local memories, there are other memory types such as constant memory and texture memory. These memory types are read-only from the GPU side, so there is no coherency problem, making them very fast.

Constant memory is well-suited for cases where all threads read from the same memory, such as constants used in stencils or similar kinds of algorithms where threads perform linear variation and have to take the same constant value. These constant values should be placed in constant memory to achieve high performance due to its fast speed and broadcast mechanisms.

Constant memory is declared in the off-code with a read and write access from the off-code. Its scope and lifetime are throughout the application. It should not be confused with defined methods. 


When declaring variables, it's important to consider the specific memory in the CUDA memory model to be used, the scope, and the alignment. There are architectural aspects to take into account as well, such as caching and on-chip or off-chip memory.


#### Use of global memory 


For device memory accesses to global memory, it's important to note that they all go through the L1 and L2 caches. The GPU reads an entire line, or even an entire sector, when a single value is requested. This means that, for a 128-byte line, a single read can cause 128 bytes to be read and transferred to the GPU. In recent architectures, lines have been divided into 32-byte sectors, reducing the amount of data transferred per read. Write accesses are also 32 bytes, regardless of the GPU architecture.

These transactions need to be aligned, meaning the base address of the data being accessed is a multiple of 32 bytes. If multiple accesses are made to the same cache line, they will be packed into a single transaction. When a thread requests memory, it's actually 32 requests being made at the same time. Ideally, the thread should be asking for 32 consecutive addresses (also known as a "32-byte aligned") as this will result in one transaction. If the accesses are not aligned, it will cause multiple transactions and unnecessary data will be transferred from memory to cache. The goal is to reduce the number of accesses required by a single word to optimize memory usage.

- **Memory Accesses on GPU**: All device memory accesses to global memory on GPUs go through L1 and L2 caches.
- **Reading Data**:
  - GPUs read an entire line or sector when a single value is requested.
  - For a 128-byte line, a single read operation causes 128 bytes to be transferred to the GPU.
  - In newer architectures, lines are divided into 32-byte sectors, reducing data transferred per read.
- **Writing Data**:
  - Write accesses are consistently 32 bytes across various GPU architectures.
- **Alignment Requirement**:
  - Transactions must be aligned with the base address of the data, which should be a multiple of 32 bytes.
  - Multiple accesses to the same cache line are combined into a single transaction.
- **Optimal Access Strategy**:
  - Ideally, threads should access 32 consecutive addresses (32-byte aligned) to minimize transactions.
  - Misaligned accesses lead to multiple transactions and unnecessary data transfers from memory to cache.
  - The goal is to minimize the number of accesses per word to optimize memory usage.

When running a code, it's not always possible to generate this kind of access, and it's necessary to write the code accordingly.

- **Random Access and Efficiency**:
  - When all accesses request values from the same cache line, all 32 accesses are part of one transaction, achieving 100% efficiency.
  - An offset in computing the memory index can lead to misaligned access, reducing efficiency.

- **Example of Misaligned Access**:
  - If the first element's address is at 64 and the next one is at 128, two transactions are necessary, leading to 50% efficiency.

- **Using Constant Memory**:
  - For loading constant values, using constant memory is more effective.
  - The constant cache in GPUs supports efficient data broadcasting to multiple processors.

- **Worst-Case Scenario**:
  - If 32 threads access random addresses in global memory, it may result in needing a separate transaction for each value.
  - This scenario can lead to a worst-case efficiency of only 3%.


**Efficiency of Array of Structures vs. Struct of Arrays and Using Global Memory**

Data Organization for Efficiency:
The speaker compares Array of Structures (AoS) with Structure of Arrays (SoA).
SoA, such as segregating data into separate arrays for red, green, and blue in a bitmap image, yields higher efficiency.
During RGB to grayscale conversion, SoA achieves 100% efficiency compared to only 33% with AoS.

Reason about the fact that in SoA all the elements in the array are consecutive in memory. 

![](images/Pasted%20image%2020240418111519.png)

```cpp
typedef struct {
    float x;
    float y;
} innerStruct_t;

innerStruct_t myAoS[N];
```

```cpp
typedef struct {
    float x[N];
    float y[N];
} innerArray_t;

innerArray_t mySoA;
```

The profiler supports the programmer in the analysis of the efficiency of the
memory usage in terms of access alignment and coalescence

$$\text{Global\_load\_efficiency} = \frac{\text{requested global memory load throughput}}{\text{required global memory load throughput}}$$

$$\text{Global\_store\_efficiency} = \frac{\text{requested global memory store throughput}}{\text{required global memory store throughput}}$$




#### Use of Shared Memory


Important to synchronize data to ensure all relevant data is stored in shared memory before processing. Results should be sent to global memory only after proper synchronization and computation in shared memory.

- **Matrix Multiplication Example**:

  - Basic matrix multiplication involves computing each element of the output matrix as a product of a row from the first matrix and a column from the second matrix.
  - The implementation typically uses three nested for loops: two outer loops scan the entire output matrix, and one inner loop performs the dot product.

  - Suggested to transfer the first strip of matrix M to shared memory once to allow multiple accesses by threads, which is more efficient.
  - Shared memory can handle broadcast access better than global memory can handle transactions.

  - Although shared memory is limited, it can be divided into sub-regions for computation.
  - Computing the first sub-region against the second can optimize memory usage and computational efficiency.

Access in shared memory has less constraints (no transaction and possibility to broadcast (faster))
Shared memory supports broadcasting, allowing multiple threads to read a value simultaneously without performance loss.


Initial steps involve loading sub-rectangles (tiles) of the strips and performing a partial dot product.

Basically: a small section of shared memory is utilized, reused across multiple locations for efficiency.
Small matrices in shared memory, when used repeatedly for larger matrix operations, exemplify effective cache usage.

**Tile-based Computation**:


  - Two matrices are declared within shared memory for computation.
  - A register accumulates values across iterations, while a for-loop iterates over the tiles.
  - Threads individually transfer values from global memory to shared memory.
  - Synchronization is essential due to block size differing from warp size; this is managed through synchronization calls and potentially dividing larger blocks into multiple warps.


Shared memory declaration: a matrix storing a tile per each input matrix. The tile has the same size as the thread block. For the sake of simplicity, the height and the width of the matrices are assumed to be divisible by the tile width.

After dividing the problem into multiple words, they need to be synchronized. A barrier is put after which all values are available in the shared memory and are ready for computation. After synchronization, the dot product is computed for the current tile. Once the partial dot product is computed, they synchronize again to go back to the beginning of the for loop to send the next tile from global memory to the shared memory. When all the tiles are elaborated, the result is saved in the global memory.

Regarding the shared memory, it is divided into banks and rows. Each bank is 32 bytes wide in newer generations, and at any time, it can only access 32 bits granularity. Banks can be accessed concurrently on a word granularity, and the best scenario is when each thread is accessing a different bank for efficient shared memory usage. If each thread is accessing 4 consecutive positions in the shared memory, fed to different banks, then all the requests are served with a single transaction, which is the best situation.

```cpp
__global__ void tiled_matrixmult(int *M, int *N, int *P,
                                 int numMRows, int numMColumns, int numNColumns) {
    __shared__ int ds_M[BLOCK_WIDTH][BLOCK_WIDTH];
    __shared__ int ds_N[BLOCK_WIDTH][BLOCK_WIDTH];

    int bx = blockIdx.x, by = blockIdx.y;
    int tx = threadIdx.x, ty = threadIdx.y;
    int i = by * BLOCK_WIDTH + ty; // Calculate the row index
    int j = bx * BLOCK_WIDTH + tx; // Calculate the column index
    int Pvalue = 0;

    // Loop over the M and N matrices in tiles
    for (int m = 0; m < (numMColumns - 1) / BLOCK_WIDTH + 1; ++m) {
        // Load the M and N tiles into shared memory
        ds_M[ty][tx] = M[i * numMColumns + m * BLOCK_WIDTH + tx];
        ds_N[ty][tx] = N[(m * BLOCK_WIDTH + ty) * numNColumns + j];
        
        // Synchronize to make sure the tiles are loaded before starting the computation
        __syncthreads();

        // Perform the dot product for the current tile
        for (int k = 0; k < BLOCK_WIDTH; ++k) {
            Pvalue += ds_M[ty][k] * ds_N[k][tx];
        }

        // Synchronize to ensure all threads are done computing with the current tile
        __syncthreads();
    }

    // Write the computed value back to global memory
    if(i < numMRows && j < numNColumns) { // Check the bounds before saving
        P[i * numNColumns + j] = Pvalue;
    }
}
```




When multiple threads request values that refer to the same column or bank, there's an issue. For example, if several threads ask for different values but refer to the same code, this can be a problem. In such cases, the system aims to serve everything with a single transaction.

There's an alternative approach—broadcasting—where the system serves threads with a single request. This is different from the previous case. For instance, when the first thread asks for a specific value or line from bank 0, all threads will ask for the value 0. Here, threads are paired or grouped in pairs, and the threads in the same pair ask for the same value. This results in serving with a single transaction.

A different situation occurs when threads access values belonging to the same bank. In such cases, the solution involves modifying the declaration of the array in the shared memory by introducing a padding column. From an architectural perspective, this means shifting the fetch line in the memory. While the explanation refers to 32 banks, in a realistic architecture, the threads will always access the first bank. With 32 threads, the worst situation is 32 different transactions, whereas a stride of 2 results in 16 transactions.



**Using Atomic Instructions to Achieve Global Synchronization**

The atomic card increments a value by one and stores it in a specific memory position. If multiple atomic elaborations refer to the same position, they are serialized, so no risk conditions occur. 

The shuffle instruction enables threads within the same workgroup to exchange data by storing registers without relying on shared or global memory.

| OPERATION             | FUNCTION     | SUPPORTED TYPES                          |
|-----------------------|--------------|------------------------------------------|
| Addition              | atomicAdd    | int, unsigned int, unsigned long long int, float |
| Subtraction           | atomicSub    | int, unsigned int                        |
| Unconditional Swap    | atomicExch   | int, unsigned int, unsigned long long int, float |
| Minimum               | atomicMin    | int, unsigned int, unsigned long long int|
| Maximum               | atomicMax    | int, unsigned int, unsigned long long int|
| Increment             | atomicInc    | unsigned int                            |
| Decrement             | atomicDec    | unsigned int                            |
| Compare-And-Swap      | atomicCAS    | int, unsigned int, unsigned long long int|
| And                   | atomicAnd    | int, unsigned int, unsigned long long int|
| Or                    | atomicOr     | int, unsigned int, unsigned long long int|
| Xor                   | atomicXor    | int, unsigned int, unsigned long long int|

- **Basic Execution Units**: Warps are the smallest group of threads that can be scheduled for execution by the GPU. In NVIDIA's CUDA architecture, a warp consists of 32 threads.
    
- **Simultaneous Thread Execution**: All threads in a warp execute the same instruction at the same time on different data. This is known as Single Instruction, Multiple Threads (SIMT) architecture.
    
- **Implications for Efficiency**:
    
    - If all threads in a warp follow the same execution path, this leads to high efficiency.
    - When threads in a warp must execute different instructions (divergence), this causes inefficiency as the warp serially executes each distinct path.



Constant memory is a part of the CUDA memory model that is mapped to an architectural level constant cache. When multiple threads are asking for the same value, it's beneficial to use constant memory because it reduces the number of load accesses, thereby increasing efficiency.


When discussing the GPU architecture, execution model, programming model, and memory model, it's important to consider strategies to accelerate parallel patterns studied in TA classes.

- **Maximizing occupancy**. The goal is to hide memory latencies by occupying all the resources, including context, words, and threads. This allows the GPU scheduler to continue scheduling and executing words, hiding the latency of memory operations from the compute and memory sides.
- **enabling coalesced global memory accesses**. This involves avoiding wasted memory accesses, which are long operations. By enabling course coalesced global memory accesses, fewer stores are required in the computation, making the memory accesses more efficient. Data from global -> shared only once. 
- **Tiling** is useful for matrix multiplication but can also be used for other parallel patterns across all the data. Tiling involves dividing a large data set into smaller chunks, or tiles, and processing them in parallel. This can help to hide memory latencies and improve occupancy.
- **Privatization** reduces the need for resources and minimizes contention and serialization of atomic operations. In the example of computing a histogram, privatization can be applied by having each thread or warp compute **partial** histograms in their own private space in shared or local memory. This approach reduces the frequency of atomic updates to global memory, subsequently reducing the number of atomic conflicts. These local histograms are then combined, enhancing performance by decreasing atomic operations/conflicts.
- **Coarsening**, reducing the size of the problem, is also discussed as a method for improving performance. It refers to increasing the amount of work done by each thread in a kernel. Instead of having many threads doing small amounts of work, fewer threads do more work each. For instance, in the context of processing an image, instead of assigning one thread per pixel, thread coarsening would assign one thread to handle a block of pixels, reducing the total number of threads and potentially making better use of the GPU's resources.



| Optimization                              | Benefit to compute cores                                      | Benefit to memory                                                       | Strategies                                                                                                                                                                                                                         |
| ----------------------------------------- | ------------------------------------------------------------- | ----------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Maximizing occupancy                      | More work to hide pipeline latency                            | More parallel memory accesses to hide DRAM latency                      | Tuning usage of SM resources such as threads per block, shared memory per block, and registers per thread                                                                                                                          |
| Enabling coalesced global memory accesses | Fewer pipeline stalls waiting for global memory accesses      | Less global memory traffic and better utilization of bursts/cache lines | Transfer between global memory and shared memory in a coalesced manner and performing uncoalesced accesses in shared memory (e.g., corner turning); Rearranging the mapping of threads to data; Rearranging the layout of the data |
| Minimizing control divergence             | High SIMD efficiency (fewer idle cores during SIMD execution) | -                                                                       | Rearranging the mapping of threads to work and/or data; Rearranging the layout of the data                                                                                                                                         |
| Tiling of reused data                     | Fewer pipeline stalls waiting for global memory accesses      | Less global memory traffic                                              | Placing data that is reused within a block in shared memory or registers so that it is transferred between global memory and the SM only once                                                                                      |
| Privatization                             | Fewer pipeline stalls waiting for atomic updates              | Less contention and serialization of atomic updates                     | Applying partial updates to a private copy of the data and then updating the universal copy when done                                                                                                                              |
| Thread coarsening                         | Less redundant work, divergence, or synchronization           | Less redundant global memory traffic                                    | Assigning multiple units of parallelism to each thread to reduce the price of parallelism when it is incurred unnecessarily                                                                                                        |
