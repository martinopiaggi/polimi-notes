
# CUDA
 
CUDA (Compute Unified Device Architecture) is  an extension of C++ (and also a lot of other languages like Fortran, Java, and Python) designed for writing programs that leverage GPUs for accelerating functions.

Introduced in 2006 with NVIDIA's Tesla architecture, CUDA's success relies on understanding GPU organization. 
CUDA is composed of software (language, API, and runtime), firmware (drivers and runtime), and hardware (CUDA-enabled GPU).

CUDA provides two main APIs for GPU device management:

- the CUDA Driver
- CUDA Runtime, which are mutually exclusive. On top of these, CUDA includes libraries for executing popular algorithms and functions, enhancing productivity and performance.

Key Concepts in CUDA are:

- **Architecture Model**: Utilizes many processing cores grouped in multiprocessors with a Single Instruction, Multiple Threads (SIMT) control unit.
- **Programming Model**: Based on massive data parallelism and fine-grained parallelism, allowing code execution on varying numbers of cores without the need for recompilation.
- **Memory Model**: More explicit to the programmer, with caches not being entirely transparent, demanding a deeper understanding of memory hierarchy and access patterns.








