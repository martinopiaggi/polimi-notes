# Multi-armed bandit

- A multi-armed bandit is a tuple $\langle\mathcal{A}, \mathcal{R}\rangle$
- $\mathcal{A}$ is a known set of $m$ actions (or "arms")
- $\mathcal{R}^a(r)=\mathbb{P}[r \mid a]$ is an unknown probability distribution over rewards
- At each step $t$ the agent selects an action $a_t \in \mathcal{A}$
- The environment generates a reward $r_t \sim \mathcal{R}^{a_t}$
- The goal is to maximize cumulative reward $\sum_{\tau=1}^t r_\tau$


- The action-value is the mean reward for action $a$,
$$
Q(a)=\mathbb{E}[r \mid a]
$$
- The optimal value $V^*$ is
$$
V^*=Q\left(a^*\right)=\max _{a \in \mathcal{A}} Q(a)
$$
- The regret is the opportunity loss for one step
$$
I_t=\mathbb{E}\left[V^*-Q\left(a_t\right)\right]
$$
The total regret is the total opportunity loss
$$
L_t=\mathbb{E}\left[\sum_{\tau=1}^t V^*-Q\left(a_\tau\right)\right]
$$
- Maximise cumulative reward $\equiv$ minimise total regret



The multi-armed bandit problem is a sequential decision-making technique where there is a single state and multiple actions to choose from. The reward depends on the action taken, which can be deterministic, stochastic, or adversarial. The objective is to find the best trade-off between exploration and exploitation in order to maximize the reward over time. This can be measured by minimizing the loss or expected pseudo-regret, which represents the number of times a suboptimal arm has been chosen.


## Exploration vs exploitation

To play the optimal policy we need to reach a trade-off between:

- exploration: the choice of new unexplored actions, even at random, to increase our knowledge about the problem.
- exploitation: use only the current knowledge to make decision, following known-good paths but risking to miss some opportunities.

Reaching a balance between the two is extremely difficult and it's a core part of RL.  

At the beginning we want to lean more towards exploration to see all the possible oppurtunities and then shift more and more towards exploiting the accumulated knowledge. We will see ways of making this transition, typically we cannot remove exploration completely.


## Upper Confidence Bound (UCB)

The algorithm used for learning explores all options to find the optimal choice. It selects arms based on their upper bound value, taking into account uncertainty. After pulling an arm, the uncertainty is updated. This approach is called Upper Confidence Bound (UCB). The UCB uses the Hoeffding inequality to compute the bound and can use different bounds such as UCBV, BayesUCB, or UCB1.


Solves the exploration/exploitation dilemma using the optimism in the face of uncertainty
principle.
it uses upper confidence bounds over the expected reward estimates to avoid the
convergence on local optima arms.


Let $X_1, \ldots, X_t$ be i.i.d. random variables in $[0,1]$, and let $\bar{X}_t=\frac{1}{\tau} \sum_{\tau=1}^t X_\tau$ be the sample mean. Then
$$
\mathbb{P}\left[\mathbb{E}[X]>\bar{X}_t+u\right] \leq e^{-2 t u^2}
$$


This leads to the UCB1 algorithm

$$
a_t=\underset{a \in \mathcal{A}}{\operatorname{argmax}} Q(a)+\sqrt{\frac{2 \log t}{N_t(a)}}
$$


$$
a_t=\underset{a \in \mathcal{A}}{\operatorname{argmax}} \hat{Q}_t(a)+\hat{U}_t(a)
$$

The UCB algorithm achieves logarithmic asymptotic total regret

```python
for t in range(1, T+1):
pulled = np.argwhere(criterion == criterion.max()).reshape(-1)
reward = rew(pulled)
n_pulls[pulled] = n_pulls[pulled] + 1
exp_payoffs[pulled] = ((exp_payoffs[pulled] *
(n_pulls[pulled] - 1.0) + reward) / n_pulls[pulled])
for k in range(0, n_options)
criterion[k] = exp_payoffs[k] + np.sqrt(2 * np.log(t) / n_pulls[k])
```

The code is implementing an Upper Confidence Bound based algorithm to solve the stochastic MAB problem. It uses the criterion variable (i.e., the upper confidence bound) to select the arm (Line 2), and, after observing the reward, it updates the number of pulls of the specific arm (Line 5), the expected payoff (Line 6) and, finally, it computes the bound (Line 9). This process is repeated over a time horizon of T rounds.


## Thompson sampling

The Bayesian approach to Thompson sampling involves defining a prior distribution for each arm, sampling from these distributions at each round, pulling the arm with the largest sampled value, and updating the priors. In this example, a Bernoulli distribution is used as the prior with an initial uniform distribution. The arm is updated based on success or failure by incrementing alpha or beta values. The advantage of this method is that it theoretically achieves the best performance by matching the upper bound on regret exactly to the lower bound.


Thompson sampling implements probability matching

$$
\text { Use Bayes law to compute posterior distribution } p\left[\mathcal{R} \mid h_t\right]
$$


The main idea behind Thompson Sampling is to balance exploration and exploitation when making decisions under uncertainty. Thompson Sampling addresses this challenge by using a probabilistic approach.

Instead of trying to estimate the true underlying reward distribution for each action, it maintains a belief or posterior distribution over these distributions based on observed data.

At each time step, Thompson Sampling samples one value from each posterior distribution and selects the action with the highest sampled value as its choice for that round. By sampling from these distributions, Thompson Sampling naturally explores different actions while also exploiting those that appear promising based on current beliefs.

Over time, as more data is collected and observations are made, Thompson Sampling updates its posterior distributions accordingly. 


Thompson sampling will choose the arm ai with the largest sampled rˆ(ai)


In the context of Thompson sampling, a conjugate prior refers to a prior distribution that, when combined with observed data, results in a posterior distribution that belongs to the same family of probability distributions as the prior. This property allows for efficient and analytically tractable Bayesian updating in Thompson sampling algorithms.

For example, in the context of estimating probabilities (such as success rates or proportions), if we assume a beta distribution as our prior, then it turns out that when combining this with binomial likelihood data (which arises from Bernoulli trials), we get another beta distribution as our posterior. 



The beta distribution, parameterized by α and β, is a continuous probability distribution defined on the interval [0, 1]. It has a shape that can vary depending on the values of α and β. When both parameters are set to 1, it represents a uniform prior where all probabilities between 0 and 1 are equally likely.

In this context, each arm ai represents an action choice in some decision-making problem. The goal is to determine which arm/action yields maximum reward over time. To do this, we start with a uniform prior Beta(1, 1) for each arm ai.

As we gather information from pulling each arm and observing successes or failures, we update our belief about the underlying probability of success using Bayesian inference. This updating process incorporates new evidence into our prior beliefs.

The incremental update formula for updating the posterior distribution after pulling an arm depends on whether it resulted in success or failure:

- In case of success: fi(t + 1) = Beta(αt + 1, βt)
- In case of failure: fi(t + 1) = Beta(αt , βt + 1)

Here t denotes the number of times we have pulled that particular arm before making this update.

By using these updates iteratively as more data becomes available from pulling different arms multiple times, we obtain updated posterior distributions for each arm's underlying probability of success. These posterior distributions capture our updated beliefs about which arms are likely to yield higher rewards based on observed data.

"Optimism in the Face of Uncertainty"

The principle of optimism in the face of uncertainty is a known heuristic in MAB problems. 
Its significance goes beyond mere exploration; optimism prevents premature convergence on suboptimal solutions. It allows RL agents to continue exploring even when faced with uncertain outcomes or initial failures. This exploration can lead them towards discovering better policies and maximizing long-term rewards.

Given a MAB stochastic problem any algorithm satisfies:
$$
L_T \geq \log T \sum_{a_i \in \mathcal{A}: \Delta_i>0} \frac{\Delta_i}{K L\left(\mathcal{R}\left(a_i\right), \mathcal{R}\left(a^*\right)\right)} \quad \text { for } T \rightarrow \infty
$$
where $K L\left(\mathcal{R}\left(a_i\right), \mathcal{R}\left(a^*\right)\right)$ is the Kullback-Leibler divergence between the two Bernoulli distributions $\mathcal{R}\left(a_i\right)$ and $\mathcal{R}\left(a^*\right)$


Note that the KL divergence for Bernoulli variables with means $p$ and $q$ is:
$$
K L(p, q)=p \log \left(\frac{p}{q}\right)+(1-p) \log \left(\frac{(1-p)}{(1-q)}\right)
$$

Theorem (UCB1 Upper Bound, Auer \& Cesa-Bianchi 2002)
At finite time $T$, the expected total regret of the UCB1 algorithm applied to a stochastic MAB problem is:

$$L_T \leq 8 \log T \sum_{a_i \in \mathcal{A}: \Delta_i>0} \frac{1}{\Delta_i}+\left(1+\frac{\pi^2}{3}\right) \sum_{a_i \in \mathcal{A}: \Delta_i>0} \Delta_i$$

Theorem (Thompson Sampling Upper Bound, Kaufmann \& Munos 2012)

At time $T$, the expected total regret of Thompson Sampling algorithm applied to a stochastic MAB problem is:

$$L_T \leq O\left(\sum_{a_i \in \mathcal{A}: \Delta_i>0} \frac{\Delta_i}{K L\left(\mathcal{R}\left(a_i\right), \mathcal{R}\left(a^*\right)\right)}(\log T+\log \log T)\right)$$