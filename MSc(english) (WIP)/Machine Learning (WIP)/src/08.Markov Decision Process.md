
## Markov Decision Processes

Markov Decision Processes (MDPs) are a mathematical framework which plays a crucial role in RL. 


Markov decision processes formally describe an environment
for reinforcement learning
Where the environment is fully observable


“The future is independent of the past given the present”


MDPs provide a way to represent the agent's interaction and behavior. 

![](images/27ab5884d318e2ea38537092acbc769b.png)


We model the MDP as a tuple of six elements $\mathcal{M}:=(\mathcal{S}, \mathcal{A}, P, R, \mu, \gamma)$:

- **States**: $S$
- **Actions**: $A$ note that not all actions are possible in all the states
- **Transition model**: $P: S \times A \rightarrow \Delta(S)$ tells how the environment evolves after each action.  It's possible to rapresent it with a matrix. 
- **Reward function**: $R: S \times A \rightarrow \mathbb{R}$
- **Initial distribution** $\mu \in \Delta(S)$, we need $\operatorname{dim}(\mu)=|S|$ numbers to store it
- **Discount factor**: $\gamma \in(0,1]$

The agent's behavior is modeled by means of a policy $\pi: S \rightarrow \Delta(A)$. 
The policy can be both deterministic or probabilistic. 
Other interesting tools are: 

- $P^\pi\left(s^{\prime} \mid s\right)=\sum_{a \in \mathcal{A}} \pi(a \mid s) P\left(s^{\prime} \mid s, a\right)$ is the probability to reach state $s'$ given we are in state $s$ , so it's a function which maps state. 
- $R^\pi(s)=\sum_{a \in \mathcal{A}} \pi(a \mid s) R(s, a)$ expected reward you will ever get from state $s$ . Formula based on the expected value of the reward function. 


The **Bellman** expectation equation: 

$$\begin{aligned}
V^\pi(s)=\mathbb{E}^\pi\left[\sum_{t=0}^{+\infty} \gamma^t R\left(s_t, a_t\right) \mid s_0=s\right] & =\sum_{a \in \mathcal{A}} \pi(a \mid s)\left[R(s, a)+\gamma \sum_{s^{\prime} \in \mathcal{S}} P\left(s^{\prime} \mid s, a\right) V^\pi\left(s^{\prime}\right)\right] \\
& =R^\pi(s)+\gamma \sum_{s^{\prime} \in \mathcal{S}} P^\pi\left(s^{\prime} \mid s\right) V^\pi\left(s^{\prime}\right)
\end{aligned}$$

The value function is a mathematical tool which is used to encode the notion of performance in RL. The expected value of the sum over an infinite horizon of the reward function. Discount factor $\gamma$ regulates the weight given to future rewards. 

We can invert the matrix (computationally costly) and say: 

$$V^\pi=\left(I-\gamma P^\pi\right)^{-1} R^\pi$$

or we can simply approximate the $V^{\pi}$ in an **iterative** and **recursive** way: 

$$V^\pi=R^\pi+\gamma P^\pi V^\pi$$

The procedure of line 1 computes the closed-form solution of the state value function V
π of policy pi in the MDP with transition model P_sas, reward R_sa and
discount factor gamma. The procedure of lines 3-8 performs the iterative application of the Bellman expectation operator to compute the same value function
V
π
. The iterative procedure is stopped when a given threshold tol between consecutive approximation is reached

``` python
V1 = np.linalg.inv(np.eye(nS) - gamma * pi @ P_sas) @ (pi @ R_sa)
```

The main disadvantage of the procedure of line 1 compared to the one of lines 3-8
is a computational one, i.e., the computation of the closed-form solution might be
infeasible when the number of states/actions is large.

``` python
V_old = np.zeros(nS)
tol = 0.0001
V2 = pi @ R_sa
while np.any(np.abs(V_old - V2) > tol):
V_old = V2
V2 = pi @ (R_sa + gamma * P_sas @ V)
```


When gamma= 1 the procedure of line 1 might lead to a singular matrix (attempting to invert it), whereas the procedure of lines 3-8 might never reach the requested tolerance tol.



So MDP and a given pokicy Markov reward process. 

If you ignore the reward, there is no factor in the initial state distribution. You get a Markov process or a Markov Chain.

Markov Process (or Markov Chain)

A Markov process is a memoryless random process, i.e. a sequence
of random states S1, S2, ... with the Markov property.


Markov reward process is a Markov chain with values.

A Markov Reward Process is a tuple $\langle\mathcal{S}, \mathcal{P}, \mathcal{R}, \gamma\rangle$


Given a policy, I can indeed compute $P^{pi}$ and $R^{pi}$

The value function V^*(s) does not contain all the information to execute the optimal policy π* on a given MDP;

While the action-value function Q^*(s, a) contains all the information to execute the optimal policy $\pi^*$$ on a given MDP. 

Typically, RL algorithms  focus on estimating the optimal action value function.  So Q^* is that of P^*.


There is a unique optimal policy in an MDP;

Most Markov reward and decision processes are discounted. Why?

- Mathematically convenient to discount rewards
- Avoids infinite returns in cyclic Markov processes
- Uncertainty about the future may not be fully represented
- If the reward is financial, immediate rewards may earn more interest than delayed rewards
- Animal/human behaviour shows preference for immediate reward
- It is sometimes possible to use undiscounted Markov reward processes (i.e. $\gamma=1$ ), e.g. if all sequences terminate.


A Markov decision process (MDP) is a Markov reward process with decisions


A policy $\pi$ is a distribution over actions given states $\pi(a|s)$ 


The value function v (s) gives the long-term value of state s

immediate reward $R_{t+1}$ 



The state-value function vπ(s) of an MDP is the expected return
starting from state s, and then following policy π

The action-value function qπ(s, a) is the expected return
starting from state s, taking action a, and then following policy π


For any Markov Decision Process
- There exists an optimal policy $\pi_*$ that is better than or equal to all other policies, $\pi_* \geq \pi, \forall \pi$
- All optimal policies achieve the optimal value function, $v_{\pi_*}(s)=v_*(s)$
- All optimal policies achieve the optimal action-value function, $q_{\pi_*}(s, a)=q_*(s, a)$


Bellman Optimality Equation is non-linear
No closed form solution (in general)
Many iterative solution methods
Value Iteration
Policy Iteration
Q-learning
Sarsa




## operator 



The Bellman operator is a mathematical tool used to rewrite the expectation equation in a more concise form. It helps find the fixed point of the value function and solve problems efficiently. The optimal value function represents the best possible performance in an MDP, with an optimal policy achieving maximum utility in every state. 



The statement is saying that repeatedly applying the Bellman optimality operator will lead to convergence towards a solution. The Bellman expectation operator, denoted as capital T^pi, is just the Bellman equation applied to a generic value function. By repeatedly applying this operator, we eventually converge to the value function of policy pi.

The second statement suggests that using the Bellman solution for the Bellman expectation operator is always a good choice for computing the value function in an MDP (Markov Decision Process). This is because it leads to convergence and provides an approximate solution that is less computationally expensive than computing the exact solution using the Bellman expectation equation.






### Markovian etc 
  
- Markovian: chess, rubik's cube
- non-Markovian: poker, blackjack (i need information about past cards that have already been played)

A policy fully defines the behaviour of the agent by selecting which action to take at each step. A policy can be:

- Markovian vs history dependent
- deterministc vs stochastic
- stationary vs non-stationary


A policy defines a way of behaving (strategy). A policy, at any given point in time (state), decides which action the agent selects.
Policies can be:
- $\quad$ Markovian or History-dependent
A policy is History-dependent if it is a function also of the previous states.
A policy is Markovian if it only depends on the current state.
Deterministic or Stochastic
A policy is Deterministic if given a state the action that is taken is always the same (deterministic mapping between states and actions).
A policy is Stochastic if given a state there is a probability distribution over the actions.
- Stationary or Non-stationary
A policy is Stationary does not depend on time but only on the state.
A policy is Non-Stationary does depend on both time and states.






Markovian: Nothing prevents you to look at the real states  and previous action that you have played  to decide which action to play.  So in principle, a policy is telling you  the probability of playing action A  based on the history of what you have done in the past.
This is what is called a history dependent policy.  As the name suggests, it depends on everything  that happened in the past. A Markovian policy is a policy that depends  on the current state only.
For Markov decision processes,  Markovian policies are enough.  You don't need to take your decision based on the whole history.

In finite horizon and MDPs, non-stationary policies may be needed. If the same state is encountered at the beginning and end of a certain number of steps, it is not guaranteed that the same action will be played in both cases. It may be more beneficial to reach certain states at the beginning, while maximizing immediate reward becomes important towards the end. This distinction does not exist in infinite horizon scenarios where there is no endpoint. In an infinite horizon MDP, markovian stationary deterministic policies are sufficient. 



Stationary:  A non-stationary policy instead  is a policy which still decides the action  to play based on your current state,  but explicitly depends on time.  Meaning that if I am in state one now,  I will take an action.  If I come back to state one few moments later,  I will possibly take another action.  This is the notion of non-stationary policies. 



If your problem is not a Markov decision process,  you might need non-stationary  or even historic effect processes.  Okay.  So coming back to the answer of the exercise. 






Definition 7.6 (State-Value function). The state-value function $V^\pi(s)$ of an MDP is the expected return starting from state $s$, and then following policy $\pi$
$$
V^\pi(s)=E_\pi\left[v_t \mid s_t=s\right]
$$
For control purposes, rather than the value of each state, it is easier to consider the value of each action in each state

Definition 7.7 (Action-Value function). The action-value function $Q^\pi(s, a)$ is the expected return starting from state $s$, taking action $a$, and then following policy $\pi$
$$
Q^\pi(s, a)=E_\pi\left[v_t \mid s_t=s, a_t=a\right]
$$
It's worth mentioning that we can calculate $V^\pi(s)$ from $Q^\pi(s, a)$, but not vice versa. We have that
$$
V^\pi(s)=\sum_a \pi(a \mid s) Q^\pi(s, a)
$$




$$
V^\pi(s)=R^\pi(s)+\gamma \sum_{s^{\prime} \in S} P^\pi\left(s^{\prime} \mid s\right) V^\pi\left(s^{\prime}\right)
$$

$$
V^\pi=R^\pi+\gamma P^\pi V^\pi
$$


$$
V^\pi=\left(I-\gamma P^\pi\right)^{-1} R^\pi
$$


Definition 7.8. The Bellman operator $T^\pi$ for $V^\pi$ is defined as $T^\pi: \mathbb{R}^{|S|} \rightarrow \mathbb{R}^{|S|}$. This operator takes as an argument a value function and it returns a value function ${ }^{43}$
$$
T^\pi(V)=R^\pi+\gamma P^\pi V
$$