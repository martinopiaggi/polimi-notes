# RL Techniques

![](images/Pasted%20image%2020230824154904.png)


## RL Techniques nomenclature

There are many distinctions in the RL techniques to approach the problem:

- **Model-free/Model-based**: In RL we do not require to have the model of the environment. The samples collected during RL algorithms can be both used to estimates the model (model-based) or just to estimate the value function and not the model (model-free).
- **On/Off policy**: While the first learns the value function of the same policy used to collect samples, the second one learns the value function of a **different policy** from the one used to collect samples. **On-policy: play the policy $\pi$ whose Q-function $Q^{\pi}$ you are learning. Off-policy: play an explorative policy and learn the optimal Q-function $Q^{*}$.
- **On/Offline**: the online approaches continuously interact with the environment, updating its policy and collecting new samples while the second ones just have the data of interactions and no interaction is possible.
- **Tabular/Function Approximation**: the difference is how value function is stored: in a table or using a function (a linear approximator or neural network).


the use of continuous states and actions in reinforcement learning. It is important to note that traditional tabular methods may not be suitable for handling these types of problems. Instead, we can utilize function approximation methods, such as neural networks, to train properly for reinforcement learning tasks involving continuous variables.


### RL for Prediction

model-free prediction: estimate value function of an unknown MRP (MDP + fixed policy `Ï€`)

- Monte Carlo
- Temporal Difference

### RL for Control

Model-free control: optimize value function of an unknown MDP to learn optimal policy

- Monte Carlo Control: Monte Carlo estimation of $Q^{\pi}(s, a)$ combined with $\varepsilon$-greedy policy improvement
- SARSA: Temporal Difference $\mathrm{TD}(0)$ estimation of $Q^{\pi}(s, a)$ combined with $\varepsilon$-greedy policy improvement
- Q-learning: empirical version of Value Iteration Off-policy: play an **explorative** policy and learn the optimal Q-function Q âˆ—


## Monte Carlo 

Monte Carlo is a simple approach for estimating the value function or `Q(s,a)` (if we are predicting or controlling) directly from experience by taking the mean of the return of observed episodes. However, **it is not suitable for long episodes or infinite horizons.**

### Monte Carlo for prediction

- wait until the end of the episode
- only episodic problems
- high variance, zero bias
- Good convergence properties
- not very sensitive to initial values
- adjust prediction toward the outcome
- general approach, less efficient



First-visit Monte Carlo treats all visits equally and does not favor any particular occurrence of a state over others. It has no bias as each visit contributes equally to the estimation of the value function.


In every-visit Monte Carlo, all visits to each state within an episode are considered when estimating the value function. This approach introduces bias because it gives more weight or importance to states that are visited more frequently within an episode.



The main difference between these two methods lies in how they handle repeated visits to states within episodes.


### Monte Carlo for control

This approach is based on the generalized policy iteration but of course in RL we do not have the model. We use the same two steps, but modified.
  
- policy evaluation -> use Monte Carlo policy evaluation on the `Q(s,a)` function
- policy improvement -> greedy improvement over `Q(s,a)` Â 

We use Q because it allows to improve the policy model-free.


The problem with this approach is that the policy `Ï€'` is deterministic and since we are learning we cannot use a deterministic policy because it doesn't perform exploration. If i do not measure the value of other actions i cannot learn if they are better or not. On the other hand i want to minimize the number of times that i select a suboptimal action.


##### Îµ-greedy exploration

IDEA: never give 0 probability to any action. Â 

We modify the deterministic policy: Â 

- with probability `1 - Îµ` i choose the greedy action given by the deterministic policy
- with probabiblity `Îµ` i choose another action at random

The parameter `Îµ` regulates the amount of exploration. There is an equivalent policy improvement theorem also for Îµ-greedy policies, so we are sure that the resulting policy is always an improvement.

``` python
def eps_greedy (s, Q, eps, allowed_actions) :
if np.random.rand () <= eps:
a= % take a random action
else:
Q_s = Q[s, : ].copy()
Q_s[allowed_actions $==0]= - np.inf
a = np.argmax(Q_s)
return a
```

$$\pi^{\prime}(s)=\underset{a \in \mathcal{A}}{\operatorname{argmax}} Q(s, a)$$


![](images/Pasted%20image%2020230825121824.png)

DEFINITION: greedy in the limit of infinite exploration (GLIE) Â 

- all state-action pairs are explored infinitely many times
- the policy converges on a greedy policy (the exploration disappears in the limit of infinite sample)

  
Theorem Â 

GLIE Monte Carlo control converges to the optimal action-value function.


Greedy in the Limit with Infinite Exploration (GLIE)

- All state-action pairs are explored infinitely many times,
$$
\lim _{k \rightarrow \infty} N_k(s, a)=\infty
$$
- The policy converges on a greedy policy,
$$\lim _{k \rightarrow \infty} \pi_k(a \mid s)=\mathbf{1}\left(a=\underset{a^{\prime} \in \mathcal{A}}{\operatorname{argmax}} Q_k\left(s, a^{\prime}\right)\right)$$

When we say that the policy converges on a greedy policy in the context of "greedy in the limit with infinite exploration," it means that as an agent explores its environment infinitely, it gradually learns and improves its decision-making process. Eventually, it reaches a point where it can make optimal decisions by always choosing actions that maximize its expected cumulative reward.
This convergence to a greedy policy is generally considered desirable because it indicates that the agent has learned to exploit its knowledge effectively. It suggests that the agent has gained enough understanding of the environment and has found an optimal strategy for maximizing rewards.







$$V\left(s_t\right) \leftarrow V\left(s_t\right)+\alpha\left(v_t-V\left(s_t\right)\right) \quad$$

$${ with } \quad v_t=\sum_{l=t}^T \gamma^{l-t} r_{l+1} \quad \text { (MC return) }$$

The update rule can be broken down as follows:

1. $v_t$ represents the Monte Carlo return for state $s_t$. The Monte Carlo return is defined as the sum of discounted rewards obtained from time step $t+1$ to the end of an episode ($T$). Each reward at time step $l+1$, denoted by $r_{l+1}$, is multiplied by a discount factor $\gamma^{l-t}$. This factor discounts future rewards based on their distance from the current time step.

2. $(v_t - V(s_t))$ calculates the error between the observed return and our current estimate of state value. It quantifies how much we need to adjust our estimate to match what was actually experienced.

3. $\alpha$ (alpha) is the learning rate, which determines how much weight we give to new information compared to existing estimates. A higher alpha means more weight given to recent experiences, while a lower alpha emphasizes past experiences more.


![](images/Pasted%20image%2020230824154821.png)

MC is on-policy 



## Temporal Difference 

![](images/Pasted%20image%2020230824154836.png)

$$V\left(s_t\right) \leftarrow V\left(s_t\right)+\alpha\left(r_{t+1}+\gamma V\left(s_{t+1}\right)-V\left(s_t\right)\right)$$


- $r_{t+1}$ represents the immediate reward received after transitioning from state $s_t$ to state $s_{t+1}$.
- Finally, $(r_{t+1} + \gamma V(s_{t+1}) - V(s_t))$ calculates what's known as TD error (Temporal Difference error). It measures the difference between our current estimate of the value of being in state $s_t$, i.e., $V(s_t)$, and our updated estimate using new information about rewards obtained from transitioning from state $s_t$ to state $s_{t+1}$.


### TD for prediction

RL version of the Bellman expectation equation. TD can *bootstrap* that is it can learn from incomplete episodes. Temporal difference uses its previous estimation to update its estimation (biased but consistent). Â 

Blueprint of TD:
- Usually more efficient than MC
- learn online at every step
- can work in continuous problems
- low variance, some bias
- worse for function approximation 
- more sensitive to initial values 
- adjust prediction toward next state 
- exploits the Markov properties of the problem

### TD($\lambda$)


$$
V\left(s_t\right) \leftarrow V\left(s_t\right)+\alpha\left(v_t^\lambda-V\left(s_t\right)\right)
$$


![](images/Pasted%20image%2020230824154929.png)


Intermediate approach between TD and MC. The parameter `Î»` regulates how much we lean towards an approach or the other and the bias-variance trade-off. In MC we look at all the steps while TD (TD(0)) looks only at one step. TD(Î») looks at some steps into the future before using the approximation. Â 

- `n = 1` is the temporal difference approach (TD(0))
- `n infinite` is the Monte Carlo approach

### TD for control

So basically all temporal difference approaches use historical data for defining  the temporal difference target.  

Basically the same, we only need to make the changes that we also applied to MC.

- apply it to Q(s,a)
- use an Îµ-greedy policy improvement to update at each step Â 

##### SARSA algorithm

- policy evaluation Â 

Uses a very simple update rule to evaluate the policy : Â 

- policy improvement, Îµ-greedy policy improvement

  

SARSA converges to the optimal Q function under two assumptions:

- GLIE sequence of policies

- Robbins-Monro sequence of learning rates Â 

  $$
Q\left(s_t, a_t\right) \leftarrow Q\left(s_t, a_t\right)+\alpha\left(r_{t+1}+\gamma Q\left(s_{t+1}, \mathbf{a}_{\mathbf{t}+\mathbf{1}}\right)-Q\left(s_t, a_t\right)\right) \quad \mathbf{a}_{\mathbf{t}+\mathbf{1}} \sim \pi\left(\cdot \mid s_{t+1}\right)
$$


The SARSA algorithm iterates between: An environment step, with the transition model A policy improvement step, with the -greedy policy Ï€ An evaluation step, with the TD update of the Q function.




Low variance but biased.


SARSA algorithm:

What conditions do we need on alpha, which is the learning rate here and epsilon?   To make the algorithm converged to a desirable solution.
 
For sure we are in a non policy algorithm, so we are estimating the value function of the policy we are playing.  If we want to play the optimal policy, we need to decrease exploration.  So epsilon must be the case and this is for sure.


##### SARSA(Î»)

Same idea of TD(Î»), trade-off between Monte Carlo and SARSA (SARSA(0)), use backwards view and eligibility traces in the same way we saw in TD(Î»). The only difference is that now the eligibility traces are defined over state-action pairs `e(s,a)`.

The trade-offs are similar

- SARSA propagates information only to the last state
- MC propagates to all states
- SARSA(Î») propagates backwards but reducing the impact the more it goes back in the states (the scaling factor is given by the eligibility traces). Â 

In off policy RL, you don't need to update the exploration policy.


### Q-learning



#### Off policy learning

Why is this important:

- learn by observing someone else behaviour
- reuse experience generated from old policies
- learn about multiple policies while following one policy
  

GOAL: learn the optimal policy `Ï€ = Ï€*`. Â 

Can be seen as the RL version of value iteration, the update rule is: Â 




The only requirement is that we need to have a policy that have non zero probability to each action, but there is no constraint on the policy --> Q-learning will learn the optimal policy even if it always plays the random policy.

  

Q-learning will learn the optimal policy even if it is not playing it. This is different from SARSA because SARSA is an on-policy approach and can only learn the best Îµ-greedy policy considering also the exploration. Â 

Q-learning learns directly the best policy.





--- 












Solves a prediction problem.  Temporal difference.  Monte Carlo, both.  Both because we can use both temporal difference and Monte Carlo first and then revisit for solving both the control and the prediction problem.



Reuse the information learned from preview from past learning steps. For sure, temporal difference and this is clear because of the bootstrap procedure.  I will also say Monte Carlo because also Monte Carlo, you average the returns that you experience in successive episodes.



9.8) 

e-greedy vs greedy? You have to progressively decrease the value of epsilon. 



It is not possible to learn the optimal policy by running a different policy on an MDP. FALSE
Any  of  policy  are algorithm  uses  policy  that is not the optimal one.  For instance,  this is what is done by two. 



Because any Monte Carlo temperature difference meters reuses previously collected information.  Last very last exercise.





Robbins-monre conditions

So the learning rate alpha has to decrease.  To zero and fulfill.  That is the sum of the learning rates from one to infinity has to diverge.  While the sum of the squares of the learning rates has to converge.  So the learning rate has to go to zero, not to slow, not to fast.  Okay.  To be very from a very intuitive perspective.  A choice is one over the episode count.


Which come from stochastic approximation and guarantee that your estimate will come.


Is about a choice because the harmonic series diverges while the series of the square of one of them converges.  Okay, questions for.




How can we modify SARSA to make the algorithm work off policy?
So one possibility is to use SARSA  plus important sampling.  There is another possibility that is turning this algorithm to Q learning.


First in myopic, learn that corresponds to have a lot is low gamma values in the definition of the MDP.
 
 
Gamma equal to one should be avoided. To prevent  Prevent.  Divergence.  Off.  Do you have any idea of a class of infinite horizon, state in three.
  
  
Gamma is a part of the definition of your problem.  It is in the definition.  I have to say that practitioners tend to use gamma  as the hyperparameter of your algorithm but it's not a good practice, it's not an hyperparameter




On-policy methods learn about an optimal policy while following that same policy during training. This means they use their own actions and experiences generated by their own policy to update the Q-function. Examples of on-policy methods include SARSA and Expected SARSA.

Off-policy methods, on the other hand, learn about an optimal policy while following a different policy during training. This means they use experiences generated by a different behavior policy (which can be random or deterministic) to update the Q-function. In Q-learning, the "max" function in the update rule allows it to select actions based on its current estimate of the Q-function rather than relying on its own actions. This makes it an off-policy algorithm.


--



