# Inferenza parametrica

Dire qualcosa (FARE INFERENZA) sui parametri incogniti, usando i dati (OSSERVAZIONI CAMPIONARIE).

## Stima puntuale 

Gli stimatori puntuali forniscono, per ogni realizzazione campionaria, un solo valore come “stima” del parametro incognito $\theta$ (o di una sua funzione $k(\theta)$). Lo stimatore è quindi una v.a. (o vettore aleatorio) e la stima è un numero (o un vettore di numeri).

## Distorsione di uno stimatore

La distorsione di uno stimatore è una misura della bontà di uno stimatore puntuale, definita come la differenza tra il valore atteso dello stimatore e il parametro che si sta cercando di stimare. Uno stimatore si dice non distorto se la sua distorsione è nulla.
 $$bias = \mathbb E(\hat \theta ) -\theta$$
 
Quando uno stimatore **non** è distorto, allora si dice corretto.
  $$\mathbb E ( \hat \theta ) - \theta = 0 $$
Uno stimatore può essere anche **asintoticamente non distorto**. Cioè è distorto magari, ma il limite del valore atteso dello stimatore per n-osservazioni che tendono ad infinito è uguale al parametro.
 
### Errore quadratico medio MSE 

L'errore quadratico medio (MSE) è un'altra misura della bontà di uno stimatore, definita come la somma della varianza dello stimatore e della sua distorsione al quadrato. 
 $$MSE(\hat \theta) = \mathbb{E}[(\hat \theta - \tau(\theta))^2]$$
Ma anche (decomposizione): 
 $$MSE(\hat \theta)=Var(\hat \theta)+(bias(\hat \theta,\theta))^2$$
Un buon stimatore dovrebbe avere un MSE basso.
 
## Consistenza 

### Consistenza debole

La consistenza debole è una proprietà desiderabile di uno stimatore $\hat \theta$, che indica che lo stimatore converge al parametro che si sta cercando di stimare al crescere del numero di dati campionari.

$$\lim _ {n \rightarrow \infty} P_{\theta}(|\hat \theta - \theta|>\epsilon)\rightarrow 0$$
In poche parole uno stimatore è consistente se tende a fornire stime sempre più precise del parametro al crescere del campione.

### Consistenza in media 2

La consistenza in media 2 ci dice che lo stimatore converge in media quadratica al valore del parametro da stimare. 

$$\lim _ {n \rightarrow \infty} \mathbb E _ \theta [(\hat \theta - \theta)^2]\rightarrow 0$$

Lo stimatore consistente in media 2 è preferibile a uno stimatore solo debolmente consistente perché fornisce una stima più precisa del parametro. 
Se  la successione di stimatori è consistente in media quadratica allora è anche debolmente consistente.  

Tuttavia, la consistenza in media 2 non è sempre garantita, anche se lo stimatore è consistente in modo debole.


### Consistenza forte

La consistenza forte implica che lo stimatore converge al parametro quasi certamente, cioè con probabilità pari a 1. Questa è una proprietà ancora più forte rispetto alla consistenza debole.

$$\mathbb P (\lim _{n \rightarrow \infty} \hat \theta =\theta)=1$$

# Metodi per la stima puntuale

## Principio di sostituzione 

 Stimo il mio parametro con la 'sua versione empirica'.
 mi baso sulla legge forte dei grandi numeri, e pongo quindi la mia media empirica uguale al parametro da stimare.
 Funzione cumulata empirica, e quindi anche di sopravvivenza.
 Metodo che funziona nel mondo della fantasia. 
 
## Principio dei momenti

Determinare il parametro $\theta$ univocamente dai momenti. Cioè cerchiamo il parametro che eguaglia i momenti 'teorici' con i momenti empirici. Ci troviamo quindi un sistema. 
 $$m_k = \frac{1}{n}\sum _{i+1}^n X_i^k$$
L'idea è simile al metodo di sostituzione.
 
## MLE, Maximum likelihood estimation
 
Criterio della massima verosimiglianza. Si basa su determinare il $\theta$ che con maggior probabilità ha generato i dati osservati. 
La funzione di verosimiglianza: $$L(\theta,x)=\prod^n _i f(x_i)$$
Ma usiamo quasi sempre la log-verosimiglianza. $l(\theta,x)=ln(L(\theta,x))$
La log-verosimiglianza ha gli stessi punti di massimo di L ma ha alcuni vantaggi: i prodotti diventano somme e gli esponenti diventano moltiplicazioni. 
$$l(\theta,x)=\sum ^n _i ln(f(x_i)$$

- asintoticamente non distorta
- consistenza in media quadratica 
- asintoticamente gaussiana

#### Principio d'invarianza funzionale per gli MLE
Se $\hat\Theta$ è lo stimatore di $\theta$ allora lo stimatore di massima verosimiglianza per $\alpha = g(\theta)$ è $\hat \alpha=g(\hat \theta)$

# Stima intervallare IC

Per quantificare il grado di precisione associato alla stima puntuale di un parametro $\theta$   si ricorre alla cosiddetta stima intervallare, cioè si costruisce un intervallo di valori in cui si ha una elevata fiducia che cada il valore incognito del parametro. Gli estremi dell'intverallo sono calcolati per mezzo del campione dei dati osservati. 

## Quantità pivotale

Sia Q la quantità pivotale per la costruzione di intervalli di confidenza. Allora: $$P_{\theta}(q_1<Q<q_2)$$
dove $q_1$ e $q_2$ dipendono dal livello di confidenza $\alpha$ ma non dal parametro $\theta$.
Ci sono intervalli bilateri e unilateri. 
IC più stretto implica più precisione. 

**Intervalli di confidenza per la media $\mu$, applicato nel caso di una popolazione normale:

- varianza $\sigma$ nota (Z):
$$P(\mu \in(\bar X _n \pm z_{1 - \alpha /2}\frac{\sigma}{\sqrt n}))=1-\alpha$$
- varianza $\sigma$ incognita (T):
 $$P(\mu \in(\bar X _n \pm t_{1 - \alpha /2}\frac{S_n}{\sqrt n}))=1-\alpha$$
*NB: in caso di campione numoroso, il caso della T di Student è approssimabile da una normale*

**Intervalli di confidenza per la varianza $\sigma$ , applicato nel caso di una popolazione normale:

-  media $\mu$ nota:
$$P(\frac{S^2(n)}{X^2_{1-\alpha/2,n}}<\sigma^2<\frac{S^2(n)}{X^2_{\alpha/2,n}})=1-\alpha/2$$
- media $\mu$ incognita :
$$P(\frac{S^2(n-1)}{X^2_{1-\alpha/2,n-1}}<\sigma^2<\frac{S^2(n-1)}{X^2_{\alpha/2,n-1}})=1-\alpha/2$$

**Proporzione $p$ di una popolazione bernoulliana**

$$P(\hat p - \frac{\sqrt{\hat p (1-\hat p)}}{n}z_{1-\alpha/2} < p < \hat p + \frac{\sqrt{\hat p (1-\hat p)}}{n}z_{1-\alpha/2})=1-\alpha$$

**Parametro $\lambda$ di una popolazione poissoniana**

$$P(\bar X - \frac{\sqrt{\bar X}}{n}z_{1-\alpha/2} < p < \bar X + \frac{\sqrt{\bar X}}{n}z_{1-\alpha/2})=1- \alpha$$

**Parametro $\lambda$ di una popolazione esponenziale**

$$P(\frac{1}{2n\bar X}X^2_{\alpha/2,2n}<\lambda<\frac{1}{2n \bar X}X^2_{\alpha /2,2n})$$
