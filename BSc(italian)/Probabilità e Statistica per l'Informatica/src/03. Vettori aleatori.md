# Vettori aleatori 

Introduzione vettori di più variabili aleatorie. Nel caso di n=2 è comodo utilizzare tabelle per rappresentare i valori di probabilità.

$$(X_1 , X_2, X_3,...):\Omega \rightarrow  \mathbb{R}\times\mathbb{R}\times\mathbb{R}....$$

## Funzione densità congiunta

caso discreto:
$$f(x,y)= \sum_i \sum_j P(x=X_i,y=Y_j)$$
caso continuo:
$$f(x,y)= \int_{-\infty}^{+\infty}  \int_{-\infty}^{+\infty} P(x,y) dx dy$$

## Densità marginale

Fun Fact: si chiama così perchè (in situazioni con due vettori di dimensione $n=2$) è la probabilità che si trova 'ai margini' della tabella tipicamente utilizzata per rappresentare le probabilità.
Molto semplicemente è la probabilità di una delle variabili fissate le altre. 

Nel caso n=2:
	- caso discreto: 
$$fx_i(x) = \sum_{y} f(x_i,y)$$
	- caso continuo: $$f_{x_i}(x)=\int _{-\infty}^{+\infty}f(x,y)dy$$

## Funzione di ripartizione congiunta

$$F(x,y)=P(X\le x,Y\le y)$$

caso discreto:
$$F(x,y)= \sum_{x<X} \sum_{y<Y}f(x,y)$$

caso continuo:
$$F(x,y)= \int_{-\infty}^{X}  \int_{-\infty}^{Y} f(x,y) dx dy$$

Posso ricavare la funzione di ripartizione di x dalla funzione di ripartizione congiunta:
$$F_x(x)=\lim_{y\rightarrow \infty}F_{x,y}(x,y)$$

## Momenti 

Sia X una v.a. ass. cont. o discreta tale che $|X^k|$ ammetta valore atteso. Il numero $\mathbb{E}[X^k]$ è detto momento k-esimo. 

### Generatrice dei momenti

Funzione generatrice degli eventi, dove il momento n-esimo è la derivata ennesima della funzione $\phi$.
caso discreto:

$$\phi(X)=\sum _x e^{tx}p(x)$$

caso continuo:
$$\phi(X) = \int _{-\infty}^{+ \infty}e^{tx}f(x) dx$$

## Vettori di v.a. indipendenti

Cosa si intendi per indipendenza? 
$$f(x,y)=f_x(x)f_y(y)$$

L'indipendenza di vettori aleatori risulta comodo in diversi conti. 
Come si dimostra/controlla se c'è la dipendenza (nel pratico)?
Controlli che il prodotto delle densità marginali (ai margini della tabella) sia uguale al rispettivo elemento dentro la tabella. *lo devi fare per ognuno degli elementi, DEVE VALERE PER TUTTI*
NB: (NB forse scontato ma occhio) negli esercizi risulta particolarmente furbo controllare le caselle dove c'è lo zero ... così che si 'sgama' subito se le variabili non sono indipendenti.

## Trasformazioni vettori aleatori 

ysy: $$g(x_1, x_2, ...., x_n)=(g(x_1),g(x_2)...,g(x_n))$$
Esempio classicone: 
v.a. $S$ definita come $S=x+y$
caso discreto:
$$f(S)==\sum_x f_{x,y}(x,S -x)$$

caso discreto con v.a. indipendenti:
$$f(S)==\sum_x f_{x}(x)f_y(S -x)$$

caso continuo:
$$f(S)=\int_{-\infty}^{+\infty}f_{x,y}(x,S-x)dx$$

caso continuo con v.a. indipendenti:
$$f(S)=\int_{-\infty}^{+\infty}f_{x}(x)f_y(S-x)dx$$

NB: 'Questo procedimento' è valido per le somme  di variabili aleatorie, non confonderti con ad esempio calcolare l'integrale doppio per trovare la funzione di partizione in una 'zona' di piano.

### Max E Min di v.a. indipendenti 

Sia $max(x)=max(X_1 , ... X_n)$ e $min(x)=min(X_1 ... X_n)$ allora:

$$F_{max}(x)=\prod_{i=1}^{n}F_i(x)$$

$$F_{min}(x)=1-\prod_{i=1}^{n}S_i(x)$$

NB: se i.i.d. (indipendenti identicamente distribuite, cioè stessa legge e stessi parametri) allora: $F_{max}(x)=F_1(x)^n$ e $F_{min}(x)=S_{1}(x)^n$

### Valore atteso di funzione di vettori aleatori

Moolto molto gradito il fatto che non è necessario alcun tipo di ragionamento su funzioni o trasformazioni.
caso discreto:
$$\mathbb{E}[g(x,y)]=\sum_{(x,y)} g(x,y)f(x,y) $$
caso continuo:
$$\mathbb{E}[g(x,y)]=\int_{-\infty}^{+\infty}g(x,y)f(x,y)dxdy$$

*NB: questi esempi sono fatti con due $n=2$ .. ma ovviamente i vettori possono essere fatti di $n$ variabili aleatorie.*

#### Somma di varie funzioni notevoli
Somma di gaussiane fa gaussiane.
Somma di binomiali con p comuni sono ancora binomiali.
Somma di v.a. con distribuzione di Poisson (con parametri uguali) sono Poisson con parametri somma di parametri.

#### Covarianza

Definizione generale:
$$Cov(X,Y)=\mathbb{E}[(x-\mu_x)(y-\mu_y)]$$
Definizione che ci piace:
$$Cov(X,Y)=\mathbb{E}[XY]-\mathbb{E}[X]\mathbb{E}[Y]$$
Da notare come il caso particolare di X=Y corrisponde alla definizione di varianza. 
$$Cov(X,X)= Var(X)=\mathbb{E}[(x-\mu_x)^2]$$

La covarianza è un 'indice' che indica di quando le variabili si influenzano a vicenda, di quanto variano insieme e quindi della loro reciproca dipendenza.

Se due variabili sono **indipendenti** allora varrà sicuramente che la $Cov[X,Y]=0$ MA NON VALE VICEVERSA! Se la covarianza è uguale a zero non si può dire nulla! Sulla dipendenza delle variabili.

##### Indice di correlazione di Pearson
$$p(X,Y)=\frac{Cov[X,Y]}{\sigma _x \sigma_y}$$
l'indice di correlazione di Pearson (anche detto coefficiente di correlazione lineare) tra due variabili aleatorie è un indice che esprime un'eventuale relazione di linearità tra esse.
Ha sempre un valore compreso tra  +1  e  -1,   dove  +1 e -1   corrispondono alla perfetta correlazione lineare (positiva/negativa) e  0    corrisponde a un'assenza di correlazione lineare.

### Media campionaria/empirica

Media empirica delle variabili aleatorie $X_1 , X_2 , ...$ è la variabile aleatoria:$$\bar X=\frac{X_1 + X_2 + X_3 + ... X_n}{n}$$
Per linearità del **valore atteso** , se tutte le v.a. hanno **stessa legge** allora:
$$\mathbb{E}[\bar{X}]=\frac{1}{n}\sum_{i=1}^{n}\mathbb{E}[X_i]=\mathbb{E}[X_i]$$

### Varianza campionaria
$$S^2_n:=\frac{1}{n-1}\sum^n_{i=1}(X_i-\bar X_n)^2$$

### Deviazione standard campionaria 
$$S_n=\sqrt{S_n^2}$$

### Legge debole dei grandi numeri 
$$\lim _{n \rightarrow \infty}P(\mid { \frac{\sum X_i }{n} - \mu \mid \le \epsilon )=1}$$ $\forall \epsilon > 0$

### Legge forte dei grandi numeri 

$$P(\lim _{n \rightarrow \infty}  \frac{\sum X_i }{n} = \mu )=1$$

## Vettori e matrici

Sia $Y=[Y_1 , Y_2 , Y_3 , Y_4 ..]^T$ vettore aleatorio, allora:
$$\mathbb{E}[Y]=\mu=\begin{bmatrix} u_1 \\ u_2 \\ u_3   \\ .... \\ . ..\end{bmatrix}$$

## Matrice di varianza-covarianza 

La matrice $\Sigma$ rappresenta la variazione di ogni variabile rispetto alle altre (inclusa se stessa). È simmetrica.

## Gaussiana multivariata

Un vettore aleatorio $X=[X_1 , X_2 , ...]$ si dice gaussiano multivariato $N(\mu,\Sigma)$ se esiste una matrice A tale che $AA^T=\Sigma$ e un vettore $Z=(Z_1, ... , Z_m)^T$ iid $N(0,1)$ tale che $$X=AZ + \mu$$

E' una generalizzazione della distribuzione normale a dimensioni più elevate. La sua importanza deriva principalmente dal fatto che è spesso utilizzata per descrivere, almeno approssimativamente, un qualunque insieme di v.a a valori reali (possibilmente) correlate, ognuna delle quali è clusterizzata attorno ad un valore medio.

## Teorema Centrale del Limite

$$\lim _{n \rightarrow \infty} \frac{x - \mu}{\sqrt{\frac{\sigma^2}{n}}} \approx N(0,1)$$

quindi 

$$P(\lim _{n \rightarrow \infty} \frac{x - \mu}{\sqrt{\frac{\sigma^2}{n}}}\le \epsilon) \approx \phi(\epsilon)$$


### Correzione del continuo

La correzione di continuità è una modifica dell'intervallo di integrazione che si applica quando si approssima una distribuzione discreta con una distribuzione continua (nel nostro caso la normale).
La correzione di continuità consiste nell' ampliare / allargare di 0.5 (convenzionalmente) gli estremi dell'intervallo sul quale si integra la densità di probabilità continua usata per approssimare una distribuzione discreta.
Con tale correzione, la precisione dell'approssimazione è maggiore.
