# Variabili Aleatorie Assolutamente Continue

Level Up del caso discreto per tutti i problemi nel quale abbiamo a che fare con var. aleatorie di infiniti valori continue e non discreti (ese: temperatura, bho )

Abbiamo tutte le funzioni precedentemente riportate nel caso discreto: 

- Funzione di ripartizione

    $$F(X<x)=\int_{-\infty}^{x}f(t)dt$$

- Funzione di densità

$$F(X)' = f(x)$$


Ora funzioni di densità di probabilità continue notevoli:

## Legge uniforme del continuo

La più semplice: **uniforme** su un insieme, ovvero che attribuisce la stessa probabilità a tutti i punti appartenenti ad un dato intervallo \[a,b\] contenuto nell'insieme.
In generale per a>b si ha che $$f_x(x) =\frac{1}{a-b}$$
Mentre la sua funzione di ripartizione F(x) , cioè P(X<=x) è 
$$F(x)=x$$
## Distribuzione esponenziale
 
$$f(x) = \begin{cases} \lambda e^{-\lambda x} \space \space \space x \ge 0 \\ 0 \space \space \space \space \space \space \space \space \space \space \space x <0 \end{cases}$$

*errore comune ... non dimenticarti della funzione nel caso di x<0 , soprattutto quando fai l'integrale per trovare la funzione di ripartizione*.

## Legge di Weibull 
$$f_x(x)=\alpha \lambda x^{\alpha -1}e^{-\lambda}x^{\alpha}$$

Come la distribuzione esponenziale descrive la "durata di vita" di un fenomeno privo di memoria, così la distribuzione di Weibull può descrivere la durata di vita per un fenomeno la cui "probabilità di morire" può variare nel tempo, in funzione di $\alpha$. 
La distribuzione di Weibull con parametro $\alpha$ = 1 è una distribuzione esponenziale, la quale infatti prevede tassi di guasto costanti nel tempo. 


$\alpha$ < 1 il tasso di guasto diminuisce nel tempo (alta 'mortalità infantile') 
$\alpha$ = 1 tasso di guasto è invariante nel tempo 
$\alpha$ > 1 tasso di guasto aumenta con il tempo 

## Legge Gamma

La distribuzione Gamma è la distribuzione di probabilità della variabile aleatoria definita come la somma di variabili aleatorie indipendenti e con distribuzione esponenziale.

$$f_x(x)=\frac{\lambda ^ \alpha}{\Gamma (\alpha)}x^{\alpha -1}e^{-\lambda x}$$

con $$\Gamma = \int _0 ^\infty x^{\alpha -1}e^{-x}dx$$

## Legge Gaussiana

Legge gaussiana $N(\mu,\sigma)$:
$$f(x) = \frac{e^{-(\frac{(x - 2\mu)^"}{2\sigma^2}}}{\sqrt{2\pi\sigma^2}}$$

con $\mu$ valore atteso, mentre $\sigma ^2$ la varianza (di conseguenza $\sigma$ è la deviazione standard).
La legge Gaussiana di notevole importanza infatti come funzione di densità di probabilità prende il nome di distribuzione normale. Semplice modello per fenomeni complessi. 
nb:
- simmetrica
- $\int_{-\infty}^{+\infty}N(..,..) = 1$

La distribuzione normale gaussiana è simmetrica rispetto al valore atteso $\mu$ 

## Funzione di sopravvivenza

Utilizzata soprattutto per studiare la vita l'usura o il rodaggio d componenti, ma anche la mortalità di organismi. 
$$S(x)=1-F(X)=P(X>x)$$
Possiamo quindi descrivere comportamenti come l' **assenza di memoria** , cioè che la probabilità di sopravvivere fino a un tempo _t+s_, sapendo che tale oggetto è sopravvissuto fino al tempo _t_, è uguale alla probabibilità di un oggetto nuovo. 

**assenza di memoria** $S(t+s)|S(t)=S(s)$
**usura** $S(t+s)|S(t)<S(s)$
**rodaggio** $S(t+s)|S(t)>S(s)$

La funzione di sopravvivenza 'next level' è l'Hazard Rate o Intensità di rischio o Intensità di Guasto, indicata come $h(t)$ .
Introduciamo le seguenti relazioni:
$$e^{-H(t)}=S(t)$$
dove
$$H(t) = \int_0^t h(t)dt$$

Di conseguenza le relazioni riportate qualche riga fa le possiamo riscrivere osservando la nostra funzione $h(s)$, cioè l'*intensità di guasto*.
**assenza di memoria** $h(s)$ costante
**rodaggio** $h(s)$ decrescente 
**usura** $h(s)$ crescente 

modellizzare una misurazione di una variabile aleatoria?

## Modello scala posizione

$$X=\mu + \sigma \epsilon$$

Dove $\mu$ è la misura d'interesse , $\sigma$ è la precisione di misura e $\epsilon$ è la variabile aleatoria che indica l'errore di misura. 
La funzione di ripartizione risulta essere: $$F_{\mu,\sigma}= F_{\epsilon}(\frac{x-\mu}{\sigma})$$
## Valore atteso di una variabile ass. continua


$$E[X]=\int_{-\infty}^{+\infty} xf(x)dx$$
in analogia con (ricordiamo) il caso discreto:
$$E[x]=\sum_x x_ip_i$$

Importante proprietà del valore atteso è la linearità. 
$$E[aX +b]=aE[X]+b$$
NB: Il valore atteso di una costante, è una costante.
quindi, per il caso del modello scala posizione:
$$E[x]=\mu+\sigma E[\epsilon]$$
altra proprietà degne di nota del valore atteso:

$$se \space P(a \le x \le b) \space allora \space a \le E(x) \le b$$


## Introduzione trasformazioni variabili aleatorie

Le trasformazioni tra variabili aleatorie sono v.a. definite $y=g(x)$ , cioè funzioni di v.a.
In esercizi di questo tipo scappano sempre errorini di distrazione e di ragionamento riguardo il dominio di definizione delle funzioni. 

Le linee da seguire sono:

1) ragionare preventivamente sul dominio delle funzioni
2) spesso è utile cercare la $F_y(Y)$ e poi da lì tramite semplici uguaglianze ricondursi a $F_x(g(y)$ 
3) da $F_y(Y)$ con una bella derivata trovi $f_y(Y)$ se necessario

Finchè la trasformazione è invertibile e cioè g(x) è monotona, non c'è nessun problema, infatti le relazioni sono come te lo aspetti. Quando invece g non è invertibile bisogna fare qualche magheggio in più, ad esempio suddividi in intervalli la funzione. 
Ad ogni modo il ragionamento da applicare è lo stesso della composizione di funzioni. Come scritto precedentemente bisogna ragionare su domini e sulle relazione tra le variabili aleatorie. 
Per quanto riguarda le trasformazioni nel **caso discreto**:
si ha densità:
$$f_y(y^*_j)=\sum_{i=g(x_i)=y_j^* } f_x(x_i)$$
e ripartizione come:
$$F(Y)=F(g(x))=P(g(x)<y)=P(x<g^{-1}(y)) =F_x(g^{-1}(y))$$

Per quanto riguarda le trasformazioni nel **caso continuo**:
solita def:
$$F(Y)=F(g(x))=P(g(x)<y)=P(x<g^{-1}(y))=F_x(g^{-1}(y))$$
e se voglio calcolarmi densità:
$$f_y(y)=\frac{dF_Y(y)}{dy}=\frac{dg^{-1}(y)}{dy}f_y(g^{-1}(y))$$


##### Valore atteso per le trasformazioni


caso discreto:
$$\mathbb{E}[y]=\mathbb{E}[g(x)]=\sum g(x)f_x(x)$$
caso continuo:
$$\mathbb{E}[y]=\mathbb{E}[g(x)]=\int_\mathbb{R}g(x)f_x(x)dx$$

*NB: nessuna trasformazione di funzione .. la formula è 'come te la aspetteresti'*

##### Varianza
La varianza è definita come $$Var(x)=\sigma ^2=E((x-m)^2)$$ dove $m=E(X)$ . Si tratta quindi del valore atteso della 'distanza quadratica' tra la v.a. x e la sua media/valore atteso. 
**caso discreto**: $$Var(x)=\sum_x (x-m)^2f(x)$$
**caso continuo**:$$Var(x)=\int_{-\infty}^{+\infty}(x-m)^2f(x)dx$$

Deviazione standard: $\sigma = \sqrt{Var(x)}$
Importante proprietà dalla varianza:
$var(Ax +b) = a^2var(x)$
*NB: per dimostrare questa roba serve ricordarsi che il $\mathbb{E}[a]=a$* per ogni $a$ costante.

Può capitare di avere una certa varianza/media di una certa variabile aleatoria di un certo esperimento. Si ha la necessità di normalizzare la variabili aleatoria per evitarne variazioni causate ad esempio dal cambio  di unità di misura. 
La varianza la si normalizza con il valore atteso. 
$$\frac{\sqrt{(Var(x)}}{\mathbb{E}[x]}$$
A proposito di normalizzare, standardizzazione/normalizzazione di v.a: 
$$T:= \frac{x - \mu}{\sigma}$$ 
T è appunto la var.a. standardizzata, con $\mu$ e $\sigma$ rispettivamente la media e la deviazione standard.
Il cuore fondamentale della standardizzazione è che se T è una var. a. normalizzata allora avrà **sempre** $\mu$ = 0 e $\sigma$ = 1 


## Quantile

$z_{\alpha}$ si dice che è il quantile di $\alpha$ se $$P(X>z_a)=\alpha$$
Il quantile di 0.5 prende il nome di *mediana*.
I quantili di ordine 1/4 e 2/4 e 3/4 prendono il nome di *quartili* .
